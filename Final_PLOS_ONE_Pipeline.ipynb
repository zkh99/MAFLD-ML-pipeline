{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14386a96",
   "metadata": {},
   "source": [
    "# MAFLD Prediction in T2DM — Reproducible Pipeline (PLOS ONE Revision)\n",
    "\n",
    "**Prepared for peer review — 2025-10-17**\n",
    "\n",
    "This notebook consolidates the full analysis pipeline used in the manuscript:\n",
    "- Data cleaning and validation\n",
    "- Missingness mechanism assessment and MAR simulation\n",
    "- Imputation benchmarking across multiple methods\n",
    "- Outlier detection\n",
    "- Class balancing\n",
    "- Feature selection (including GA + Taguchi tuning)\n",
    "- Model training with stratified K-fold CV and hyperparameter search\n",
    "- Evaluation with variance estimates (repeated CV)\n",
    "- Feature importance (XGBoost, GB, LightGBM) and SHAP\n",
    "\n",
    "**Reproducibility notes**\n",
    "- Random seeds are centralized below.\n",
    "- All tuning/selection steps happen **inside** CV loops to avoid leakage.\n",
    "- Figures and tables generated here align with the revised manuscript and supplementary materials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdf7a2c",
   "metadata": {},
   "source": [
    "## 0) Setup & Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86294249",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Global configuration & seeds ----\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEEDS = {\n",
    "    \"global\": 42,\n",
    "    \"cv_seeds\": [42, 52, 62, 72, 82, 92, 102, 112, 122, 132],\n",
    "    \"impute_seeds\": [232, 123, 1, 313, 78, 121],\n",
    "}\n",
    "\n",
    "# Project paths (adjust if needed)\n",
    "DATA_DIR = Path(\"data\")       # place raw CSVs here if sharing\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Display options\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "np.random.seed(SEEDS[\"global\"])\n",
    "\n",
    "print(\"Config initialized. Seeds set. OUTPUT_DIR =\", OUTPUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbff014",
   "metadata": {},
   "source": [
    "## 1) Load and Inspect Data\n",
    "Load source data and perform basic checks. Ensure that each row corresponds to a **unique patient** (no patient appears more than once).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe2cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\4-imputed_data.csv\")\n",
    "\n",
    "# ستون آخر به عنوان برچسب\n",
    "label_col = data.columns[-1]\n",
    "y = data.iloc[:, -1].copy()\n",
    "X = data.iloc[:, :-1].copy()\n",
    "\n",
    "# مقیاس‌بندی برای LOF (خیلی مهم)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# LOF\n",
    "lof = LocalOutlierFactor()\n",
    "pred = lof.fit_predict(X_scaled)   # 1=inlier, -1=outlier\n",
    "\n",
    "# جدا کردن اینلایرها و آوتلایرها با حفظ ایندکس\n",
    "inliers_df = X.loc[pred == 1].copy()\n",
    "outliers_df = X.loc[pred != 1].copy()\n",
    "\n",
    "# برچسب را برگردان به هر کدام\n",
    "inliers_df[label_col] = y.loc[inliers_df.index]\n",
    "outliers_df[label_col] = y.loc[outliers_df.index]\n",
    "\n",
    "# ذخیره\n",
    "inliers_df.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\5-outlier_removed.csv\", index=False)\n",
    "outliers_df.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\5.5-the_outliers.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2ad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# Load your dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\5-outlier_removed.csv\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Initialize the RandomUnderSampler\n",
    "under_sampler = RandomUnderSampler(random_state=101)\n",
    "\n",
    "# Apply random undersampling\n",
    "X_resampled, y_resampled = under_sampler.fit_resample(X, y)\n",
    "\n",
    "# Combine resampled features and target into a DataFrame\n",
    "resampled_data = pd.DataFrame(X_resampled, columns=data.columns[:-1])\n",
    "resampled_data[\"fattyliver\"] = y_resampled\n",
    "\n",
    "# Save the resampled balanced dataset to a new CSV file\n",
    "resampled_data.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\6-balanced_data.csv\", index=False)\n",
    "\n",
    "print(\"Random undersampling completed and balanced dataset saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44c5fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, roc_auc_score, precision_score, roc_curve\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)   # <- درست\n",
    "pd.set_option(\"display.width\", None)\n",
    "\n",
    "pca_save_path = r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\Plain_Summary.csv\"\n",
    "plain_results_rows = []\n",
    "\n",
    "# reading data\n",
    "df = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\6-balanced_data.csv\")\n",
    "\n",
    "# X, y\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=101\n",
    ")\n",
    "\n",
    "# scale\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=200),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'SVM': SVC(probability=True),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Extra Tree': ExtraTreesClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),  # <- وارنینگ کمتر\n",
    "    'LightGBM': LGBMClassifier(verbose=-1)\n",
    "}\n",
    "\n",
    "# grids\n",
    "param_grids = {\n",
    "    'Logistic Regression': {'C': [0.1, 1, 10]},\n",
    "    'KNN': {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']},\n",
    "    'SVM': {'C': [0.1, 1, 10]},\n",
    "    'Decision Tree': {'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10]},\n",
    "    'Random Forest': {'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10]},\n",
    "    'Extra Tree': {'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10]},\n",
    "    'Gradient Boosting': {'learning_rate': [0.1, 0.01, 0.001], 'max_depth': [3, 5, 7], 'n_estimators': [100, 200, 300]},\n",
    "    'XGBoost': {'learning_rate': [0.1, 0.01, 0.001], 'max_depth': [3, 5, 7], 'n_estimators': [100, 200, 300]},\n",
    "    'LightGBM': {'learning_rate': [0.1, 0.01, 0.001], 'max_depth': [3, 5, 7], 'n_estimators': [100, 200, 300], 'num_leaves': [31, 50, 100], 'force_col_wise': [True]}\n",
    "}\n",
    "\n",
    "# train/eval\n",
    "plt.figure()\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    grid = GridSearchCV(model, param_grids[model_name], scoring=\"f1\", cv=5, n_jobs=-1)  # n_jobs=-1 برای سرعت\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    params = grid.best_params_\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    pre = precision_score(y_test, y_pred)\n",
    "    f1  = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.plot(fpr, tpr, label=model_name)\n",
    "\n",
    "    plain_results_rows.append({\n",
    "        \"Classifier\": model_name,\n",
    "        \"Best Parameters\": params,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Recall\": rec,\n",
    "        \"Precision\": pre,\n",
    "        \"F1\": f1,\n",
    "        \"AUC\": auc,\n",
    "    })\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Best Parameters: {params}\")\n",
    "    print(f\"Accuracy: {acc:.3f} | Recall: {rec:.3f} | Precision: {pre:.3f} | F1: {f1:.3f} | AUC: {auc:.3f}\")\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=1, color='black')\n",
    "plt.xlim([0, 1]); plt.ylim([0, 1.05])\n",
    "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\ROC.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plain_results_df = pd.DataFrame(plain_results_rows)\n",
    "plain_results_df.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\plain_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ad71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ورودی‌ها\n",
    "save_path = r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\Kbest_Summary.csv\"\n",
    "\n",
    "# داده را زودتر بخوان تا بتوانی feature_names را ست کنی\n",
    "df = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\6-balanced_data.csv\")\n",
    "feature_names = df.columns[:-1].tolist()\n",
    "\n",
    "# # X, y و اسپیلت\n",
    "# X = df.iloc[:, :-1].values\n",
    "# y = df.iloc[:, -1].values\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, stratify=y, random_state=101\n",
    "# )\n",
    "\n",
    "results_rows = []\n",
    "\n",
    "# Outcomes برای ذخیره‌ی منحنی دقت برحسب K\n",
    "Outcomes = {name: [] for name in Classifiers.keys()}\n",
    "\n",
    "# CV ترجیحاً Stratified برای کلاس‌بندی\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"K-best Feature Selection:\")\n",
    "\n",
    "for n_clf in Classifiers.keys():\n",
    "    print(f\"{n_clf} is going ....\")\n",
    "    num_k = []\n",
    "\n",
    "    for k in range(1, X_train.shape[1] + 1):\n",
    "        num_k.append(k)\n",
    "\n",
    "        # اسکیل داخل Pipeline تا leakage نداشته باشیم\n",
    "        pipeline = Pipeline([\n",
    "            # ('scaler', StandardScaler()),\n",
    "            ('feature_selection', SelectKBest(score_func=f_classif, k=k)),\n",
    "            ('classifier', Classifiers[n_clf])\n",
    "        ])\n",
    "\n",
    "        scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "        Outcomes[n_clf].append(np.mean(scores))\n",
    "\n",
    "    # پیدا کردن بهترین K\n",
    "    max_accuracy = max(Outcomes[n_clf])\n",
    "    max_index = Outcomes[n_clf].index(max_accuracy)\n",
    "    best_k = num_k[max_index]\n",
    "\n",
    "    # ترسیم و ذخیره‌ی نمودار\n",
    "    plt.axvline(x=best_k, color='r', linestyle='--')\n",
    "    plt.text(best_k + 0.5, max_accuracy, f'({best_k}, {max_accuracy:.2f})')\n",
    "    plt.plot(num_k, Outcomes[n_clf])\n",
    "    plt.title(n_clf)\n",
    "    plt.xlabel(\"Number of K\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.savefig(rf\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\Kbest_{n_clf}.png\", dpi=300)\n",
    "    plt.clf()\n",
    "\n",
    "    # فیت نهایی با بهترین K روی کل train\n",
    "    best_pipeline = Pipeline([\n",
    "        ('feature_selection', SelectKBest(score_func=f_classif, k=best_k)),\n",
    "        ('classifier', Classifiers[n_clf])\n",
    "    ])\n",
    "    best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # پیش‌بینی و متریک‌ها\n",
    "    y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "    # اگر مدل احتمال می‌دهد، AUC بگیر\n",
    "    if hasattr(best_pipeline.named_steps['classifier'], \"predict_proba\"):\n",
    "        y_score = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_score)\n",
    "    else:\n",
    "        auc = np.nan\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # استخراج اسامی فیچرهای انتخاب‌شده\n",
    "    kbest = best_pipeline.named_steps['feature_selection']\n",
    "    mask = kbest.get_support()\n",
    "    selected_features = [feature_names[i] for i, m in enumerate(mask) if m]\n",
    "\n",
    "    results_rows.append({\n",
    "        \"Classifier\": n_clf,\n",
    "        \"Best_K\": best_k,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Recall\": rec,\n",
    "        \"Precision\": prec,\n",
    "        \"F1\": f1,\n",
    "        \"AUC\": auc,\n",
    "        \"Selected_Features\": selected_features\n",
    "    })\n",
    "\n",
    "# ذخیره‌ی خلاصه\n",
    "pd.DataFrame(results_rows).to_csv(save_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e80eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# اگر نتایج KBest را داری:\n",
    "Kbest_df = pd.DataFrame(results_rows)\n",
    "Kbest_df.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\Kbest_Summary.csv\", index=False)\n",
    "\n",
    "# مسیر خروجی جدول\n",
    "pca_save_path = r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\PCA_Summary.csv\"\n",
    "\n",
    "# خواندن داده (اگر قبلاً در محیط هست، می‌تونی حذف کنی)\n",
    "df = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\6-balanced_data.csv\")\n",
    "feature_names = df.columns[:-1].tolist()\n",
    "\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# CV ایمن (stratified)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# دیکشنری امتیازهای CV و جمع‌کننده نتایج\n",
    "Outcomes_PCA = {name: [] for name in Classifiers.keys()}\n",
    "pca_results_rows = []\n",
    "\n",
    "print(\"PCA Feature Extraction:\")\n",
    "\n",
    "for n_clf in Classifiers.keys():\n",
    "    print(f\"{n_clf} is going ...\")\n",
    "    num_k = []\n",
    "    Outcomes_PCA[n_clf].clear()\n",
    "\n",
    "    # تعداد کامپوننت‌ها: 1 تا تعداد فیچرها\n",
    "    for k in range(1, X_train.shape[1] + 1):\n",
    "        num_k.append(k)\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "       # جلوگیری از leakage\n",
    "            ('pca', PCA(n_components=k, random_state=0)),\n",
    "            ('clf', Classifiers[n_clf])\n",
    "        ])\n",
    "\n",
    "        scores = cross_val_score(\n",
    "            pipeline, X_train, y_train, cv=kf, scoring='accuracy', n_jobs=-1\n",
    "        )\n",
    "        Outcomes_PCA[n_clf].append(np.mean(scores))\n",
    "\n",
    "    # بهترین k\n",
    "    max_accuracy = max(Outcomes_PCA[n_clf])\n",
    "    max_index = Outcomes_PCA[n_clf].index(max_accuracy)\n",
    "    best_k = num_k[max_index]\n",
    "\n",
    "    # رسم و ذخیره نمودار\n",
    "    plt.axvline(x=best_k, color='r', linestyle='--')\n",
    "    plt.text(best_k + 0.5, max_accuracy, f'({best_k}, {max_accuracy:.2f})')\n",
    "    plt.plot(num_k, Outcomes_PCA[n_clf])\n",
    "    plt.title(f\"PCA - {n_clf}\")\n",
    "    plt.xlabel(\"Number of Components\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.savefig(rf\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\PCA_{n_clf}.png\", dpi=300)\n",
    "    plt.clf()\n",
    "\n",
    "    # فیت نهایی با بهترین k روی کل train\n",
    "    best_pca_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=best_k, random_state=0)),\n",
    "        ('clf', Classifiers[n_clf])\n",
    "    ])\n",
    "    best_pca_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # پیش‌بینی روی test\n",
    "    y_pred = best_pca_pipeline.predict(X_test)\n",
    "\n",
    "    # AUC فقط اگر proba موجود است\n",
    "    if hasattr(best_pca_pipeline.named_steps['clf'], \"predict_proba\"):\n",
    "        y_score = best_pca_pipeline.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_score)\n",
    "    else:\n",
    "        auc = np.nan\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # «اهمیت» فیچرها از PCA (لودینگ‌ها وزن‌دهی‌شده با سهم واریانس)\n",
    "    pca_obj = best_pca_pipeline.named_steps['pca']\n",
    "    comps = pca_obj.components_                 # (n_components, n_features)\n",
    "    evr = pca_obj.explained_variance_ratio_     # (n_components,)\n",
    "    abs_comps = np.abs(comps)\n",
    "    weighted = abs_comps * evr[:, None]\n",
    "    feat_importance = weighted.sum(axis=0)      # (n_features,)\n",
    "\n",
    "    # انتخاب top-N فیچر به تعداد n_components منتخب (فقط برای گزارش)\n",
    "    top_idx = np.argsort(-feat_importance)[:best_k]\n",
    "    selected_features = [feature_names[i] for i in top_idx]\n",
    "\n",
    "    pca_results_rows.append({\n",
    "        \"Classifier\": n_clf,\n",
    "        \"Best_n_components\": best_k,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Recall\": rec,\n",
    "        \"Precision\": prec,\n",
    "        \"F1\": f1,\n",
    "        \"AUC\": auc,\n",
    "        \"Selected_Features\": selected_features\n",
    "    })\n",
    "\n",
    "# ذخیره خروجی\n",
    "pca_results_df = pd.DataFrame(pca_results_rows)\n",
    "pca_results_df[\"Selected_Features\"] = pca_results_df[\"Selected_Features\"].apply(lambda lst: \", \".join(lst))\n",
    "pca_results_df.to_csv(pca_save_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\n=== PCA Summary per Classifier (Test-set metrics) ===\")\n",
    "print(pca_results_df)\n",
    "print(f\"\\nSaved to: {pca_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06eb3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# اگر df اینجا تعریف نشده، بخوانش:\n",
    "# df = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\6-balanced_data.csv\")\n",
    "\n",
    "feature_names = df.columns[:-1].tolist()\n",
    "\n",
    "results_rows = []\n",
    "save_path = r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\RFECV_Summary.csv\"\n",
    "\n",
    "# X, y\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# train/test split\n",
    "\n",
    "\n",
    "# CV مناسب\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"RFECV Feature Selection (with per-fold scaling):\")\n",
    "\n",
    "# اسکیل را داخل RFECV با Pipeline انجام بده تا leakage نداشته باشیم\n",
    "rfecv_pipeline = Pipeline([\n",
    "    ('rfecv', RFECV(\n",
    "        estimator=RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "        step=1,\n",
    "        cv=kf,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "rfecv_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# دسترسی به شیء RFECV\n",
    "rfecv = rfecv_pipeline.named_steps['rfecv']\n",
    "mask = rfecv.get_support()\n",
    "selected_features = [feature_names[i] for i, m in enumerate(mask) if m]\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "# داده‌ی انتخاب‌شده (خام؛ اسکیل داخل هر پایپ‌لاین انجام می‌شود)\n",
    "X_train_sel = X_train[:, mask]\n",
    "X_test_sel  = X_test[:,  mask]\n",
    "\n",
    "# برای ذخیره‌ی امتیازهای CV\n",
    "Outcomes = {name: [] for name in Classifiers.keys()}\n",
    "\n",
    "for n_clf in Classifiers.keys():\n",
    "    print(f\"{n_clf} is going ....\")\n",
    "\n",
    "    # اسکیل داخل پایپ‌لاین هر فولد → بدون leakage\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', Classifiers[n_clf])\n",
    "    ])\n",
    "\n",
    "    # CV روی ترین (۵ فولد)\n",
    "    scores = cross_val_score(pipeline, X_train_sel, y_train, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "    Outcomes[n_clf] = scores.tolist()\n",
    "    mean_acc = float(np.mean(scores))\n",
    "\n",
    "    # نمودار امتیاز هر فولد\n",
    "    xs = list(range(1, len(scores) + 1))\n",
    "    plt.plot(xs, scores, marker='o')\n",
    "    plt.axhline(y=mean_acc, color='r', linestyle='--')\n",
    "    plt.title(f\"RFECV - {n_clf}\")\n",
    "    plt.xlabel(\"Fold\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.savefig(rf\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\RFECV_{n_clf}.png\", dpi=300)\n",
    "    plt.clf()\n",
    "\n",
    "    # فیت نهایی روی ترین و متریک‌های تست\n",
    "    pipeline.fit(X_train_sel, y_train)\n",
    "    y_pred = pipeline.predict(X_test_sel)\n",
    "\n",
    "    # AUC فقط اگر proba دارد\n",
    "    if hasattr(pipeline.named_steps['classifier'], \"predict_proba\"):\n",
    "        y_score = pipeline.predict_proba(X_test_sel)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_score)\n",
    "    else:\n",
    "        auc = np.nan\n",
    "\n",
    "    acc  = accuracy_score(y_test, y_pred)\n",
    "    rec  = recall_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    f1   = f1_score(y_test, y_pred)\n",
    "\n",
    "    results_rows.append({\n",
    "        \"Classifier\": n_clf,\n",
    "        \"Selected_Count\": int(rfecv.n_features_),\n",
    "        \"Accuracy\": acc,\n",
    "        \"Recall\": rec,\n",
    "        \"Precision\": prec,\n",
    "        \"F1\": f1,\n",
    "        \"AUC\": auc,\n",
    "        \"Selected_Features\": selected_features\n",
    "    })\n",
    "\n",
    "# ذخیره\n",
    "results_df = pd.DataFrame(results_rows)\n",
    "results_df[\"Selected_Features\"] = results_df[\"Selected_Features\"].apply(lambda lst: \", \".join(lst))\n",
    "results_df.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\n=== RFECV Summary per Classifier (Test-set metrics) ===\")\n",
    "print(results_df)\n",
    "print(f\"\\nSaved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c14f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier Imputation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\4-imputed_data.csv\")\n",
    "\n",
    "lof = LocalOutlierFactor()\n",
    "outliers = lof.fit_predict(data)\n",
    "inliers = data[outliers == 1]\n",
    "outlier = data[outliers != 1]\n",
    "\n",
    "data = pd.DataFrame(inliers, columns=data.columns)\n",
    "out = pd.DataFrame(outlier, columns=data.columns)\n",
    "data.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\5-outlier_removed.csv\" , index=False)\n",
    "out.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\5.5-the_outliers.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772fbd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_row\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, roc_auc_score, precision_score\n",
    "\n",
    "pca_save_path = r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\Plain_Summary.csv\"\n",
    "plain_results_rows = []\n",
    "#reading data\n",
    "df=pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\6-balanced_data.csv\")\n",
    "# df=pd.read_csv(r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\datasets\\Data\\7-balanced_data_without_SBP.csv\")\n",
    "\n",
    "\n",
    "\n",
    "#defining x & y\n",
    "x=df.iloc[:,:-1].values\n",
    "y=df.iloc[:,-1].values\n",
    "\n",
    "#train test spilit\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,stratify=y,random_state=101)\n",
    "\n",
    "\n",
    "#standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "#classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "# define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'SVM': SVC(probability=True),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Extra Tree': ExtraTreesClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'XGBoost': xgb.XGBClassifier(),\n",
    "    'LightGBM': LGBMClassifier(verbose=-1)\n",
    "}\n",
    "\n",
    "# define parameter grids for hyperparameter tuning\n",
    "param_grids = {\n",
    "    'Logistic Regression': {'C': [0.1, 1, 10]},\n",
    "    'KNN': {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']},\n",
    "    'SVM': {'C': [0.1, 1, 10]},\n",
    "    'Decision Tree': {'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10]},\n",
    "    'Random Forest': {'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10]},\n",
    "    'Extra Tree': {'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10]},\n",
    "    'Gradient Boosting': {'learning_rate': [0.1, 0.01, 0.001], 'max_depth': [3, 5, 7], 'n_estimators': [100, 200, 300]},\n",
    "    'XGBoost': {'learning_rate': [0.1, 0.01, 0.001], 'max_depth': [3, 5, 7], 'n_estimators': [100, 200, 300]},\n",
    "    'LightGBM': {'learning_rate': [0.1, 0.01, 0.001], 'max_depth': [3, 5, 7], 'n_estimators': [100, 200, 300], 'num_leaves': [31, 50, 100], 'force_col_wise': [True]}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# training and evaluating models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, roc_auc_score, roc_curve\n",
    "\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    param_grid = param_grids[model_name]\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring=\"f1\", cv=5)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    parameters = grid_search.best_params_\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_model.predict(x_test)\n",
    "    y_pred_prob = best_model.predict_proba(x_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    pres = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_prob[:, 1])\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:, 1])\n",
    "    plt.plot(fpr, tpr, label=model_name)\n",
    "\n",
    "    plain_results_rows.append(\n",
    "        {\n",
    "        \"Classifier\": model_name,\n",
    "        \"Best Parameters\" : parameters,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Recall\": recall,\n",
    "        \"Precision\": pres,\n",
    "        \"F1\": f1,\n",
    "        \"AUC\": auc,\n",
    "    })\n",
    "\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Best Parameters: {parameters}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"AUC: {auc}\")\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\ROC.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plain_results_df = pd.DataFrame(plain_results_rows)\n",
    "plain_results_df.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\plain_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337a7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === فقط در صورت نیاز: اگر قبلاً این‌ها را import نکردی، اضافه کن\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# اگر قبلاً تعریف نکرده‌ای:\n",
    "feature_names = df.columns[:-1].tolist()\n",
    "\n",
    "# جایی قبل از شروع حلقه‌ها (اگر نبوده):\n",
    "results_rows = []\n",
    "save_path = r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\Kbest_Summary.csv\"\n",
    "\n",
    "df=pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\6-balanced_data.csv\")\n",
    "\n",
    "#defining x & y\n",
    "x=df.iloc[:,:-1].values\n",
    "y=df.iloc[:,-1].values\n",
    "\n",
    "#train test spilit\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,stratify=y,random_state=101)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(x_train)\n",
    "X_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "Outcomes = {\n",
    "}\n",
    "\n",
    "for i in Classifiers.keys():\n",
    "    Outcomes[i] = []\n",
    "\n",
    "\n",
    "# K-fold cross-validation\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# K-best feature selection\n",
    "print(\"K-best Feature Selection:\")\n",
    "\n",
    "for n_clf in models.keys():\n",
    "    print(f\"{n_clf}is going ....\")\n",
    "    num_k = []\n",
    "    for k in range(1, x_train.shape[1] + 1):\n",
    "        print(f\"#{k}\")\n",
    "\n",
    "        num_k.append(k)\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('feature_selection', SelectKBest(score_func=f_classif, k=k)),\n",
    "            ('classifier', Classifiers[n_clf])\n",
    "        ])\n",
    "\n",
    "        scores = cross_val_score(pipeline, X_train_scaled, y_train, cv=kf, scoring='accuracy')\n",
    "        score = np.mean(scores)\n",
    "        Outcomes[n_clf].append(score)\n",
    "\n",
    "\n",
    "    max_accuracy = max(Outcomes[n_clf])\n",
    "    max_index = Outcomes[n_clf].index(max_accuracy)\n",
    "    plt.axvline(x=num_k[max_index], color='r', linestyle='--')  # Vertical line at maximum accuracy point\n",
    "    print(f'({num_k[max_index]}, {max_accuracy:.2f})')\n",
    "    plt.text(num_k[max_index] + 0.5, max_accuracy, f'({num_k[max_index]}, {max_accuracy:.2f})')\n",
    "\n",
    "    plt.plot(num_k,Outcomes[n_clf])\n",
    "    plt.title(n_clf)\n",
    "    plt.xlabel(\"Number of K\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.savefig(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\Kbest_{}.png\".format(n_clf),dpi=300)\n",
    "    plt.clf()\n",
    "\n",
    "    best_acc = max(Outcomes[n_clf])\n",
    "    best_idx = Outcomes[n_clf].index(best_acc)\n",
    "    best_k = num_k[best_idx]\n",
    "\n",
    "    # بازتعریف و فیت پایپ‌لاین با بهترین k روی دیتای train (اسکیل‌شده)\n",
    "    best_pipeline = Pipeline([\n",
    "        ('feature_selection', SelectKBest(score_func=f_classif, k=best_k)),\n",
    "        ('classifier', Classifiers[n_clf])\n",
    "    ])\n",
    "    best_pipeline.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # پیش‌بینی روی تست و محاسبه متریک‌ها\n",
    "    y_pred = best_pipeline.predict(X_test_scaled)\n",
    "    # --- پیش‌بینی روی تست ---\n",
    "    y_pred = best_pipeline.predict(X_test_scaled)\n",
    "\n",
    "    # --- فقط اگر predict_proba داشت، AUC حساب کن ---\n",
    "    if hasattr(best_pipeline.named_steps['classifier'], \"predict_proba\"):\n",
    "        y_score = best_pipeline.predict_proba(X_test_scaled)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_score)\n",
    "    else:\n",
    "        auc = np.nan  # اگر مدل proba نداشت، AUC را نذار (NaN)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_score)\n",
    "\n",
    "    # استخراج لیست اسامی فیچرهای انتخاب‌شده برای بهترین k\n",
    "    kbest = best_pipeline.named_steps['feature_selection']\n",
    "    mask = kbest.get_support()\n",
    "    selected_features = [feature_names[i] for i, m in enumerate(mask) if m]\n",
    "\n",
    "    # اضافه به سطرهای خروجی\n",
    "    results_rows.append({\n",
    "        \"Classifier\": n_clf,\n",
    "        \"Best_K\": best_k,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Recall\": rec,\n",
    "        \"Precision\": prec,\n",
    "        \"F1\": f1,\n",
    "        \"AUC\": auc,\n",
    "        \"Selected_Features\": selected_features  # به صورت لیست ذخیره می‌شود\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ecff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# 1) اگر Res در حافظه نیست از CSV بخوان (در غیر این صورت این خط را حذف یا کامنت کن)\n",
    "Res = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\sixth-121\\ResultsOfGrid.csv\")\n",
    "\n",
    "# 2) مسیر خروجی‌های R:\n",
    "mf_dir = r\"C:\\Users\\z_kho\\OneDrive\\Desktop\"\n",
    "mf_files = [f\"output{i}.csv\" for i in range(1, 100) if os.path.exists(os.path.join(mf_dir, f\"output{i}.csv\"))]\n",
    "if not mf_files:\n",
    "    print(\"هیچ فایل output*.csv برای missForest پیدا نشد.\")\n",
    "\n",
    "# 3) اگر می‌خواهی پارامترهای maxiter/ntree هم ثبت شوند:\n",
    "#    مطابق حلقه‌ی R (maxiter در بیرون، ntree در داخل)، ترتیب ترکیب‌ها این است:\n",
    "maxiter_values = [5\n",
    "                #   , 10, 20\n",
    "                  ]\n",
    "ntree_values   = [50\n",
    "\n",
    "                #   , 100, 150\n",
    "                  ]\n",
    "\n",
    "import os\n",
    "\n",
    "# مسیر خروجی‌های R\n",
    "mf_dir = r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\sixth-121\"\n",
    "\n",
    "# 3) ترکیب‌های پارامتر\n",
    "maxiter_values = [5]       # می‌توانید سایر مقادیر را اضافه کنید\n",
    "ntree_values   = [50]      # می‌توانید سایر مقادیر را اضافه کنید\n",
    "param_pairs = [(mi, nt) for mi in maxiter_values for nt in ntree_values]\n",
    "\n",
    "# ساخت mf_files بر اساس تعداد ترکیب‌ها\n",
    "mf_files = []\n",
    "for idx in range(1, len(param_pairs)+1):\n",
    "    fname = f\"output{idx}.csv\"\n",
    "    path = os.path.join(mf_dir, fname)\n",
    "    if os.path.exists(path):\n",
    "        mf_files.append(fname)\n",
    "    else:\n",
    "        print(f\"هشدار: فایل {fname} وجود ندارد!\")\n",
    "\n",
    "print(\"mf_files:\", mf_files)\n",
    "param_pairs = [(mi, nt) for mi in maxiter_values for nt in ntree_values]  # [(5,50),(5,100),(5,150),(10,50),...]\n",
    "print(param_pairs)\n",
    "# 4) وزن‌دهی بر اساس سهم مفقودی هر ستون (مثل قبل)\n",
    "scale_weight = {}\n",
    "sum_miss = int(dff.isna().sum().sum())\n",
    "if sum_miss == 0:\n",
    "    print(\"هشدار: در dff هیچ NaN نیست (sum_miss == 0). وزن‌دهی ممکن است معنی‌دار نباشد.\")\n",
    "for cls in df_test.columns:\n",
    "    we = int(dff[cls].isna().sum()) if cls in dff.columns else 0\n",
    "    scale_weight[cls] = (we / sum_miss) if sum_miss > 0 else 0.0\n",
    "\n",
    "# 5) فقط ستون‌هایی را ارزیابی کن که واقعاً در df (dff) NaN داشتند و در df_test هم وجود دارند\n",
    "continues_eval = [c for c in continues if (c in missings) ]\n",
    "binary_eval    = [c for c in ['Retino','CAD','CVA','Smoking'] if (c in missings) ]\n",
    "\n",
    "labels, labels2, params_col, con_values, bin_values, details = [], [], [], [], [], []\n",
    "\n",
    "print(mf_files)\n",
    "for idx, fname in enumerate(mf_files, start=1):\n",
    "    path = os.path.join(mf_dir, fname)\n",
    "    print(f\"Evaluating missForest file: {fname}\")\n",
    "\n",
    "    # 5-1) بارگذاری و هم‌راستاسازی\n",
    "    df_imputed = pd.read_csv(path)\n",
    "    # هم‌نام و هم‌ترتیب با dff:\n",
    "    df_imputed = df_imputed[dff.columns]\n",
    "    df_imputed.index = dff.index\n",
    "\n",
    "    # 5-2) ارزیابی (از همان evaluate_imputation خودت)\n",
    "    eval_dict = evaluate_imputation(df_imputed, df_test, missings)  # همان تابع قبلی‌ات\n",
    "\n",
    "    # 5-3) امتیاز وزن‌دار پیوسته/باینری\n",
    "    # پیوسته:\n",
    "    cont_score, w_sum_c = 0.0, 0.0\n",
    "    for c in continues_eval:\n",
    "        if c in eval_dict and \"R2\" in eval_dict[c]:\n",
    "            cont_score += eval_dict[c][\"R2\"] * scale_weight.get(c, 0.0)\n",
    "            w_sum_c   += scale_weight.get(c, 0.0)\n",
    "    cont_score = (cont_score / w_sum_c) if w_sum_c > 0 else np.nan\n",
    "\n",
    "    # باینری:\n",
    "    bin_score, w_sum_b = 0.0, 0.0\n",
    "    for b in binary_eval:\n",
    "        if b in eval_dict and \"Accuracy\" in eval_dict[b]:\n",
    "            bin_score += eval_dict[b][\"Accuracy\"] * scale_weight.get(b, 0.0)\n",
    "            w_sum_b   += scale_weight.get(b, 0.0)\n",
    "    bin_score = (bin_score / w_sum_b) if w_sum_b > 0 else np.nan\n",
    "\n",
    "    # 5-4) برچسب‌ها و پارامترها\n",
    "    lab = f\"MissForest_{idx}\"\n",
    "    labels.append(lab)\n",
    "    labels2.append(lab)  # برای سازگاری با ساختار Res\n",
    "    if idx <= len(param_pairs):\n",
    "        params_col.append({\"maxiter\": param_pairs[idx-1][0], \"ntree\": param_pairs[idx-1][1]})\n",
    "    else:\n",
    "        params_col.append({})  # اگر فایل‌ها بیشتر از 9 بودند\n",
    "\n",
    "    con_values.append(cont_score)\n",
    "    bin_values.append(bin_score)\n",
    "    details.append(eval_dict)\n",
    "\n",
    "# 6) ساخت DataFrameِ missForest و ادغام با Res\n",
    "Res_mf = pd.DataFrame({\n",
    "    \"Labels\":   labels,\n",
    "    \"Labels2\":  labels2,\n",
    "    \"Parameters\": params_col,\n",
    "    \"Continues\": con_values,\n",
    "    \"Binary\":    bin_values,\n",
    "    \"Details\":   details\n",
    "})\n",
    "\n",
    "# اگر Res قبلاً ساخته شده:\n",
    "try:\n",
    "    Res_combined = pd.concat([Res, Res_mf], ignore_index=True)\n",
    "except NameError:\n",
    "    # اگر Res در حافظه نبود، فقط missForest را داریم\n",
    "    Res_combined = Res_mf.copy()\n",
    "\n",
    "# ذخیره نسخه‌ی ادغام‌شده\n",
    "Res_combined.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\sixth-121\\ResultsOfGrid_ALL.csv\", index=False)\n",
    "print(\"Saved:\", r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\sixth-121\\ResultsOfGrid_ALL.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5911097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('module://matplotlib_inline.backend_inline')  # نمایش داخل VS Code/Jupyter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# خواندن کل نتایج\n",
    "res_all = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\sixth-121\\ResultsOfGrid_ALL.csv\")\n",
    "\n",
    "# ستون Model از روی Labels ساخته می‌شه (قبل از \"_\" هرچی باشه = نام مدل)\n",
    "res_all[\"Model\"] = res_all[\"Labels\"].str.split(\"_\").str[0]\n",
    "\n",
    "# انتخاب بهترین ردیف برای هر مدل بر اساس Continues\n",
    "best_per_model = res_all.loc[res_all.groupby(\"Model\")[\"Continues\"].idxmax()].reset_index(drop=True)\n",
    "\n",
    "# ذخیره جدول بهترین‌ها\n",
    "best_per_model.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\sixth-121\\BestPerModel.csv\", index=False)\n",
    "print(best_per_model[[\"Model\",\"Labels\",\"Parameters\",\"Continues\",\"Binary\"]])\n",
    "\n",
    "# ---- نمودار مقایسه ----\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(best_per_model[\"Model\"], best_per_model[\"Continues\"])\n",
    "plt.title(\"Best Continues (Weighted R²) per Model\")\n",
    "plt.ylabel(\"Weighted R²\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\best_contunues.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(best_per_model[\"Model\"], best_per_model[\"Binary\"])\n",
    "plt.title(\"Best Binary (Weighted Accuracy) per Model\")\n",
    "plt.ylabel(\"Weighted Accuracy\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\sixth-121\\best_binary.png\", dpi=200)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7455f496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\datasets\\Data\\3-my_null_data_40_del.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088707c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_name = [\"LogReg\", \"KNN\", \"DT\", \"SVM\", \"RF\", \"ET\", \"XGB\", \"AdaBoost\"]\n",
    "classifiers = {\n",
    "    \"LogReg\":LogisticRegression(),\n",
    "    \"KNN\":KNeighborsClassifier(),\n",
    "    \"DT\":DecisionTreeClassifier(),\n",
    "    \"SVM\":SVC(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"ET\": ExtraTreesClassifier(),\n",
    "    \"XGB\": xgb.XGBClassifier(objective='binary:logistic'),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "}\n",
    "\n",
    "# ---------- Import values\n",
    "prob_per = 0.90\n",
    "rand_per = 0.10\n",
    "\n",
    "def set_classifier(col):\n",
    "    The_col = Gridi[Gridi[\"Column\"] == col]\n",
    "    Maxi = np.argmax(The_col[\"F1\"])\n",
    "    The_row = The_col.iloc[Maxi]\n",
    "\n",
    "    clf = classifiers[The_row[\"Classifier\"]]\n",
    "    param = eval(The_row[\"Parameters\"])\n",
    "    clf.set_params(**param)\n",
    "    return clf\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\datasets\\Data\\3-my_null_data_40_del.csv\")\n",
    "data = data.iloc[:,:-1]\n",
    "\n",
    "#Making missing numbers and stuff\n",
    "missing_dataframe = pd.DataFrame(columns=[\"Variable\", \"Missing_number\" ,\"Percentage\"])\n",
    "data_records = data.shape[0]\n",
    "for i, col in enumerate(data.columns):\n",
    "\n",
    "    missing = data[col].isnull().sum()\n",
    "    missing_dataframe.loc[i, \"Variable\"] = col\n",
    "    missing_dataframe.loc[i, \"Missing_number\"] = missing\n",
    "    missing_dataframe.loc[i, \"Percentage\"] = round((missing/data_records)*100,2)\n",
    "\n",
    "missing_dataframe = missing_dataframe.sort_values(by= \"Percentage\" , ascending=False)\n",
    "missing_dataframe.reset_index(inplace=True , drop=True)\n",
    "missing_columns = missing_dataframe[\"Variable\"][missing_dataframe[\"Missing_number\"]>100]\n",
    "\n",
    "print(missing_dataframe)\n",
    "# Preparing test and train df\n",
    "\n",
    "df = data.copy()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train , df_test = train_test_split(df , train_size=0.5 , random_state=232)\n",
    "\n",
    "\n",
    "df_test = df_test.dropna()\n",
    "df_test[\"row\"] = range(1, len(df_test) + 1)\n",
    "df_test_original = df_test.copy()\n",
    "\n",
    "\n",
    "new_col = [\"Column\",\"Accuracy\" , \"Precision\" , \"Recall\" , \"F1\"]\n",
    "Log_res = pd.DataFrame(columns=new_col)\n",
    "\n",
    "for colmn in missing_columns:\n",
    "\n",
    "    clf = set_classifier(colmn)\n",
    "    y = np.where(df_train[colmn].isna(), 1, 0)\n",
    "    selected = [i for i in df_train.columns if i != colmn]\n",
    "    x = df_train[selected].fillna(df_train[selected].median())\n",
    "\n",
    "    clf.fit(x,y)\n",
    "    prob = clf.predict_proba(df_test[selected])\n",
    "\n",
    "    df_test[\"{}_prob\".format(colmn)] = prob[0][1]\n",
    "    df_test = df_test.sort_values(by=\"{}_prob\".format(colmn), ascending=False).reset_index(drop=True)\n",
    "    missing_percent = int(missing_dataframe[\"Percentage\"][missing_dataframe[\"Variable\"] == colmn].iloc[0]) / 100\n",
    "    num_rows = int((missing_percent * df_test.shape[0])*prob_per) + 1\n",
    "    num_rows_rand = int((missing_percent * df_test.shape[0]) * rand_per) + 1\n",
    "\n",
    "    df_test[\"{}_missing\".format(colmn)] = df_test[colmn]\n",
    "    df_test.loc[:num_rows , \"{}_missing\".format(colmn)] = np.nan\n",
    "    random_indices = np.random.choice(df_test[df_test[\"{}_missing\".format(colmn)] != np.nan].index, size=num_rows_rand, replace=False)\n",
    "    df_test.loc[random_indices, \"{}_missing\".format(colmn) ] = np.nan\n",
    "\n",
    "    df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Delete all '_prob' columns\n",
    "prob_columns = [col for col in df_test.columns if col.endswith('_prob')]\n",
    "df_test.drop(columns=prob_columns, inplace=True)\n",
    "\n",
    "# Replace columns with their corresponding '_missing' columns\n",
    "for col in df_test.columns:\n",
    "    if col.endswith('_missing'):\n",
    "        original_col = col.replace('_missing', '')\n",
    "        if original_col in df_test.columns:\n",
    "            df_test[original_col] = df_test[col]\n",
    "\n",
    "# Drop the '_missing' columns after replacement\n",
    "missing_columns = [col for col in df_test.columns if col.endswith('_missing')]\n",
    "df_test.drop(columns=missing_columns, inplace=True)\n",
    "\n",
    "df_test= df_test.sort_values(by=\"row\")\n",
    "\n",
    "df_test = df_test.drop(\"row\" , axis=1)\n",
    "df_test_original = df_test_original.drop(\"row\" , axis=1)\n",
    "\n",
    "df_test.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\sixth-121\\test2.csv\", index=False)\n",
    "df_test_original.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\sixth-121\\test_original2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448aa854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_row\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, roc_auc_score, precision_score\n",
    "import shap\n",
    "\n",
    "#reading data\n",
    "df=pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\6-balanced_data.csv\")\n",
    "\n",
    "\n",
    "#defining x & y\n",
    "x=df.iloc[:,:-1].values\n",
    "y=df.iloc[:,-1].values\n",
    "\n",
    "#train test spilit\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,stratify=y,random_state=101)\n",
    "\n",
    "\n",
    "# #feature selection\n",
    "# from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# selector = SelectKBest(f_classif, k=15)  # Select top 10 features based on F-score\n",
    "# x_train_selected = selector.fit_transform(x_train, y_train)\n",
    "# x_test_selected = selector.transform(x_test)\n",
    "\n",
    "#standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test =scaler.transform(x_test)\n",
    "\n",
    "x_train = pd.DataFrame(x_train , columns= df.columns[:-1])\n",
    "x_test = pd.DataFrame(x_test , columns= df.columns[:-1])\n",
    "\n",
    "#classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "chosen = [\n",
    "        # [0, 1, 4, 6, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 23, 24, 28, 29, 30],\n",
    "        #   [1, 4, 7, 11, 12, 22, 24, 28],\n",
    "        #   [0, 1, 6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 25, 29],\n",
    "        #   [0, 1, 2, 3, 6, 7, 11, 12, 13, 17, 18, 22, 24, 26, 27, 29, 30],\n",
    "        #   [0, 1, 2, 5, 6, 7, 11, 12, 13, 14, 17, 18, 20, 21, 24, 27, 29, 30],\n",
    "          list(range(31))\n",
    "          # ,[0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 21, 23, 24, 25, 27, 28, 29, 30],\n",
    "        # list(range(31))\n",
    "    ]\n",
    "\n",
    "\n",
    "models = {\n",
    "    # 'Logistic Regression': LogisticRegression(C=1),\n",
    "    # 'KNN': KNeighborsClassifier(n_neighbors=5 , weights='distance'),\n",
    "    # 'SVM': SVC(probability=True , C=0.1 , kernel='linear' ),\n",
    "    # 'Decision Tree': DecisionTreeClassifier(max_depth=5 , min_samples_split=5),\n",
    "    # 'Extra Tree': ExtraTreesClassifier(max_depth=None, min_samples_split=5, n_estimators=200),\n",
    "#    'Gradient Boosting': GradientBoostingClassifier(learning_rate=0.1, max_depth=5, n_estimators=300)\n",
    "    'XGBoost': XGBClassifier(n_estimators=300, max_depth=5, learning_rate=0.1, subsample=1.0, colsample_bytree=1.0, eval_metric='logloss', use_label_encoder=False, tree_method='hist', n_jobs=-1, random_state=0),\n",
    "\n",
    "    #'LightGBM': LGBMClassifier(force_col_wise=True,learning_rate= 0.1, max_depth= 7, n_estimators=200, num_leaves=100)\n",
    "}\n",
    "\n",
    "f_name = [df.columns[i] for i in chosen[0]]\n",
    "print(f_name)\n",
    "f_name = [\n",
    "\"DDM: 3.5%\",\n",
    "\"PLT: 13.3%\",\n",
    "\"Retino: 0.4%\",\n",
    "\"Sex: 1.2%\",\n",
    "\"Height: 2.8%\",\n",
    "\"Weight: 4.4%\",\n",
    "\"Waist: 2.4%\",\n",
    "\"DBP: 1.6%\",\n",
    "\"CRP: 13.6%\",\n",
    "\"FBS: 2.2%\",\n",
    "\"LDL: 2.8%\",\n",
    "\"Cr: 1.7%\",\n",
    "\"UA: 2.8%\",\n",
    "\"ALT: 36.3%\",\n",
    "\"ALKP: 3.3%\",\n",
    "\"CVA: 0.7%\",\n",
    "\"HOMA: 2.9%\",\n",
    "\"BMI: 4.2%\",\n",
    "]\n",
    "\n",
    "model = models.values()\n",
    "names = list(models.keys())\n",
    "\n",
    "for i, clf in enumerate(model) :\n",
    "\n",
    "    x_train_selected = x_train.iloc[:,chosen[i]].values\n",
    "    x_test_selected = x_test.iloc[:, chosen[i]].values\n",
    "\n",
    "\n",
    "    clf.fit(x_train_selected, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = clf.predict(x_test_selected)\n",
    "    y_pred_prob=clf.predict_proba(x_test_selected)\n",
    "\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    pres = precision_score(y_test, y_pred)\n",
    "    auc=roc_auc_score(y_test,y_pred_prob[:,1])\n",
    "\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Model: {names[i]}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Presicion: {pres}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"AUC: {auc}\")\n",
    "    print(\"---------------------------\")\n",
    "    # SHAP values\n",
    "\n",
    "    plt.rcParams[\"font.family\"] = \"Calibri\"\n",
    "\n",
    "    explainer = shap.Explainer(clf, x_train_selected)\n",
    "    shap_values = explainer(x_train_selected, check_additivity=False)\n",
    "\n",
    "    # SHAP summary plot\n",
    "    fig, ax = plt.subplots()\n",
    "    shap.summary_plot(shap_values, x_train_selected, feature_names=f_name, show=False)\n",
    "\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "    # Define your own color gradient with start and end colors\n",
    "    start_color = \"#0F9ED5\"  # Blue\n",
    "    end_color = \"#d70f47\"  # Red\n",
    "\n",
    "    # Create a colormap from the two colors\n",
    "    custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [start_color, end_color])\n",
    "\n",
    "    for fc in plt.gcf().get_children():\n",
    "        for fcc in fc.get_children():\n",
    "            if hasattr(fcc, \"set_cmap\"):\n",
    "                fcc.set_cmap(custom_cmap)\n",
    "\n",
    "    plt.savefig(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\SHAP.png\",dpi=600)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6c056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== requirements: pandas, numpy, scikit-learn, xgboost, lightgbm =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ----------------------------\n",
    "# 0) داده‌ها\n",
    "# ----------------------------\n",
    "# >>> این دو خط را مطابق داده‌های خودت تنظیم کن <<<\n",
    "# df: شامل همه ستون‌های ورودی + ستون target\n",
    "# target_col: نام ستون هدف\n",
    "# df = ...  # دیتافریم خودت\n",
    "target_col = \"fattyliver\"\n",
    "df = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\Clean Outputs\\2-imputed_data.csv\")\n",
    "X = df.drop(columns=[target_col]).copy()\n",
    "y = df[target_col].values\n",
    "feature_names = X.columns.tolist()\n",
    "X_np = X.values\n",
    "n_features = X_np.shape[1]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- اسکیل کردن فقط برای انتخاب فیچر ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_np)\n",
    "\n",
    "# 2) Gradient Boosting + KBest (K=22)\n",
    "K_GB = 22\n",
    "sel_gb = SelectKBest(score_func=f_classif, k=K_GB)\n",
    "X_sel_gb = sel_gb.fit_transform(X_scaled, y)  # اینجا از X_scaled استفاده کن\n",
    "mask_gb = sel_gb.get_support(indices=True)\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) XGBoost (بدون فیچر سلکشن) — بهترین مدل نهایی\n",
    "# ----------------------------\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    "    importance_type='gain'  # برای شفافیت\n",
    ")\n",
    "xgb.fit(X_np, y)\n",
    "imp_xgb = xgb.feature_importances_.astype(float)  # طول = n_features\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Gradient Boosting + KBest (K=22)\n",
    "#    نکته: فقط روی 22 فیچر برتر فیت می‌کنیم و ایمپورتنس را\n",
    "#    به وکتور n_features نگاشت می‌کنیم (بقیه = 0)\n",
    "# ----------------------------\n",
    "K_GB = 22  # طبق گفته‌ی تو\n",
    "sel_gb = SelectKBest(score_func=f_classif, k=K_GB)\n",
    "X_sel_gb = sel_gb.fit_transform(X_np, y)\n",
    "mask_gb = sel_gb.get_support(indices=True)  # ایندکس‌های انتخاب‌شده\n",
    "\n",
    "gb = GradientBoostingClassifier(\n",
    "    learning_rate=0.1, max_depth=5, n_estimators=300, random_state=0\n",
    ")\n",
    "gb.fit(X_sel_gb, y)\n",
    "imp_gb_selected = gb.feature_importances_.astype(float)  # طول = K_GB\n",
    "\n",
    "# نگاشت به طول کامل\n",
    "imp_gb = np.zeros(n_features, dtype=float)\n",
    "imp_gb[mask_gb] = imp_gb_selected\n",
    "\n",
    "# ----------------------------\n",
    "# 3) LightGBM با 18 فیچر انتخابی مشخص‌شده\n",
    "#    ایندکس‌هایی که خودت دادی:\n",
    "# ----------------------------\n",
    "lgb_indices = [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 17, 20, 24, 25, 26, 29]\n",
    "X_sel_lgb = X_np[:, lgb_indices]\n",
    "\n",
    "lgb = LGBMClassifier(\n",
    "    force_col_wise=True,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=7,\n",
    "    n_estimators=100,\n",
    "    verbose=-1,\n",
    "    random_state=0\n",
    ")\n",
    "lgb.fit(X_sel_lgb, y)\n",
    "imp_lgb_selected = lgb.feature_importances_.astype(float)  # طول = 18\n",
    "\n",
    "# نگاشت به طول کامل\n",
    "imp_lgb = np.zeros(n_features, dtype=float)\n",
    "for idx_local, idx_global in enumerate(lgb_indices):\n",
    "    imp_lgb[idx_global] = float(imp_lgb_selected[idx_local])\n",
    "\n",
    "# ----------------------------\n",
    "# 4) نرمال‌سازی اختیاری (مثلاً مجموع هر مدل = 1) — اگر خواستی کامنت را بردار\n",
    "# ----------------------------\n",
    "def normalize(v):\n",
    "    s = v.sum()\n",
    "    return v / s if s > 0 else v\n",
    "\n",
    "imp_xgb = normalize(imp_xgb)\n",
    "imp_gb  = normalize(imp_gb)\n",
    "imp_lgb = normalize(imp_lgb)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) ساخت CSV خروجی\n",
    "#    اگر ستونی در مدلی انتخاب نشده، مقدارش 0 است (الان همینطور شده)\n",
    "# ----------------------------\n",
    "out_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"XGBoost_without\": imp_xgb,\n",
    "    \"GB_KBest\": imp_gb,\n",
    "    \"LightGBM_selected\": imp_lgb,\n",
    "})\n",
    "\n",
    "# مرتب‌سازی اختیاری بر اساس XGBoost\n",
    "out_df = out_df.sort_values(\"XGBoost_without\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# ذخیره\n",
    "out_df.to_csv(\"feature_importance_comparison.csv\", index=False)\n",
    "\n",
    "print(\"Saved: feature_importance_comparison.csv\")\n",
    "out_df.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbe01e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== requirements: pandas, numpy, scikit-learn, xgboost, lightgbm =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ----------------------------\n",
    "# 0) داده‌ها\n",
    "# ----------------------------\n",
    "target_col = \"fattyliver\"\n",
    "df = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\Clean Outputs\\2-imputed_data.csv\")\n",
    "\n",
    "X_df = df.drop(columns=[target_col]).copy()\n",
    "y = df[target_col].values\n",
    "feature_names = X_df.columns.tolist()\n",
    "\n",
    "X_np = X_df.values\n",
    "n_features = X_np.shape[1]\n",
    "\n",
    "# ----------------------------\n",
    "# اسکیل فقط برای انتخاب فیچر (KBest)\n",
    "# ----------------------------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_np)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) XGBoost (بدون فیچر سلکشن) — بهترین مدل نهایی\n",
    "# ----------------------------\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    "    importance_type='gain'\n",
    ")\n",
    "xgb.fit(X_np, y)\n",
    "imp_xgb = xgb.feature_importances_.astype(float)  # طول = n_features\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Gradient Boosting + KBest (K=22)\n",
    "#    سلکتور را با X_scaled فیت کن، ولی مدل را روی X_np[:, mask] آموزش بده\n",
    "# ----------------------------\n",
    "K_GB = 22\n",
    "sel_gb = SelectKBest(score_func=f_classif, k=K_GB)\n",
    "sel_gb.fit(X_scaled, y)\n",
    "\n",
    "mask_gb = sel_gb.get_support(indices=True)  # ایندکس‌های انتخاب‌شده\n",
    "X_sel_gb = X_np[:, mask_gb]                 # داده‌ی خام (اسکیل لازم نیست برای درخت)\n",
    "\n",
    "gb = GradientBoostingClassifier(\n",
    "    learning_rate=0.1, max_depth=5, n_estimators=300, random_state=0\n",
    ")\n",
    "gb.fit(X_sel_gb, y)\n",
    "imp_gb_selected = gb.feature_importances_.astype(float)  # طول = K_GB\n",
    "\n",
    "# نگاشت به طول کامل\n",
    "imp_gb = np.zeros(n_features, dtype=float)\n",
    "imp_gb[mask_gb] = imp_gb_selected\n",
    "\n",
    "# ----------------------------\n",
    "# 3) LightGBM با 18 فیچر انتخابی مشخص‌شده\n",
    "# ----------------------------\n",
    "lgb_indices = [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 17, 20, 24, 25, 26, 29]\n",
    "X_sel_lgb = X_np[:, lgb_indices]\n",
    "\n",
    "lgb = LGBMClassifier(\n",
    "    force_col_wise=True,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=7,\n",
    "    n_estimators=100,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1,\n",
    "    random_state=0\n",
    ")\n",
    "lgb.fit(X_sel_lgb, y)\n",
    "imp_lgb_selected = lgb.feature_importances_.astype(float)  # طول = 18\n",
    "\n",
    "# نگاشت به طول کامل\n",
    "imp_lgb = np.zeros(n_features, dtype=float)\n",
    "for idx_local, idx_global in enumerate(lgb_indices):\n",
    "    imp_lgb[idx_global] = float(imp_lgb_selected[idx_local])\n",
    "\n",
    "# ----------------------------\n",
    "# 4) نرمال‌سازی اختیاری (جمع هر ستون = 1)\n",
    "# ----------------------------\n",
    "def normalize(v):\n",
    "    s = v.sum()\n",
    "    return v / s if s > 0 else v\n",
    "\n",
    "imp_xgb = normalize(imp_xgb)\n",
    "imp_gb  = normalize(imp_gb)\n",
    "imp_lgb = normalize(imp_lgb)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) ساخت CSV خروجی\n",
    "# ----------------------------\n",
    "out_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"XGBoost_without\": imp_xgb,\n",
    "    \"GB_KBest\": imp_gb,\n",
    "    \"LightGBM_selected\": imp_lgb,\n",
    "})\n",
    "\n",
    "# مرتب‌سازی اختیاری بر اساس XGBoost\n",
    "out_df = out_df.sort_values(\"XGBoost_without\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "out_df.to_csv(\"feature_importance_comparison.csv\", index=False)\n",
    "print(\"Saved: feature_importance_comparison.csv\")\n",
    "print(out_df.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b123794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import heapq\n",
    "from sklearn.metrics import  f1_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "AbbClassifers = [\"SVM\"\n",
    "    # \"LG\", \"SVM\", \"DT\" ,\n",
    "    #  \"ET\" ,\n",
    "    #  \"GB\" , \"XGB\" , \"LGB\"\n",
    "    ]\n",
    "\n",
    "CL = [\n",
    "\n",
    "    # LogisticRegression(C=1, solver=\"liblinear\", max_iter=1000, random_state=0),\n",
    "    SVC(C=1, probability=True, random_state=0)\n",
    "    # ,\n",
    "    # # DecisionTreeClassifier(max_depth=5, min_samples_split=5, random_state=0),\n",
    "    # # ExtraTreesClassifier(\n",
    "    # #         n_estimators=100, max_depth=None, min_samples_split=2, n_jobs=-1, random_state=0\n",
    "    # #     ),\n",
    "    # GradientBoostingClassifier(\n",
    "    #         learning_rate=0.1, max_depth=5, n_estimators=100, random_state=0\n",
    "    #     ),\n",
    "    # XGBClassifier(\n",
    "    #         n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "    #         subsample=1.0, colsample_bytree=1.0,\n",
    "    #         eval_metric=\"logloss\", use_label_encoder=False,\n",
    "    #         tree_method=\"hist\", n_jobs=-1, random_state=0\n",
    "    #     ),\n",
    "    # LGBMClassifier(\n",
    "    #         n_estimators=100, learning_rate=0.1, max_depth=7, num_leaves=31,\n",
    "    #         force_col_wise=True, random_state=0, n_jobs=-1\n",
    "    #     )\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "df=pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\6-balanced_data.csv\")\n",
    "\n",
    "\n",
    "#defining x & y\n",
    "x=df.iloc[:,:-1].values\n",
    "y=df.iloc[:,-1].values\n",
    "\n",
    "#train test spilit\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,stratify=y,random_state=101)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(x_train)\n",
    "X_test = scaler.transform(x_test)\n",
    "\n",
    "ro = 0\n",
    "row = 0\n",
    "\n",
    "for n_classifier in range(len(AbbClassifers)):\n",
    "\n",
    "    Results = pd.DataFrame(\n",
    "        columns=[\"row\", \"gen\", \"train\", \"validation\", \"random_state\", \"classifier\", \"Best\", \"Best Res\", \"test\"])\n",
    "    clf = CL[n_classifier]\n",
    "    name = AbbClassifers[n_classifier]\n",
    "    print(\"========================\")\n",
    "    print(\"Classifier: \", name)\n",
    "\n",
    "\n",
    "    # The first time check results of recall without feature selection\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    scores = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=3, n_jobs=1)\n",
    "    acc = scores.mean()\n",
    "    Results.loc[row, \"validation\"] = acc\n",
    "    print(\"First croos validation Recall without feature selection: \", acc)\n",
    "    Results.loc[row, \"Validation\"] = acc\n",
    "\n",
    "\n",
    "    first_y_pred = clf.predict(X_test)\n",
    "    first_acc = accuracy_score(y_test, first_y_pred)\n",
    "    print(\"First test Recall without feature selection: \", first_acc)\n",
    "    Results.loc[row, \"test\"] = first_acc\n",
    "\n",
    "\n",
    "\n",
    "    # Row in Result DataFrame\n",
    "    row += 1\n",
    "\n",
    "    # Defining number of features\n",
    "    num_features = np.shape(x)[1]\n",
    "    Best_rec = []\n",
    "    Best_fea = []\n",
    "\n",
    "    # --- Input area\n",
    "    # Hyperparamters\n",
    "    num_population = 200\n",
    "    num_gens = 50\n",
    "\n",
    "    mutation_rate = 0.2\n",
    "    elitism_rate = 0.1\n",
    "\n",
    "    least_num = 4\n",
    "    most_length = num_features - 1\n",
    "\n",
    "    remained = int((1 - elitism_rate) * num_population)\n",
    "    elitism_n = num_population - remained\n",
    "\n",
    "\n",
    "    # Rollet Selection:\n",
    "    # -- Get's the fitness function of all the chromosomes in popularion\n",
    "    # -- Selects one chr based on random seclection but with probabilites\n",
    "\n",
    "    def roulette_selection(fitness_values):\n",
    "        total_fitness = sum(fitness_values)\n",
    "        probabilities = [fitness / total_fitness for fitness in fitness_values]\n",
    "        selected_index = np.random.choice(len(fitness_values), p=probabilities)\n",
    "        return selected_index\n",
    "\n",
    "\n",
    "    # Translates the binary chromosemes [0,1,0,1] to a list of selected featues [1,3]\n",
    "    def translate(chromosome):\n",
    "        return [i for i, gene in enumerate(chromosome) if gene == 1]\n",
    "\n",
    "\n",
    "    #Creats a random first generation\n",
    "    def first_pop(num_features, model, X_train, y_train):\n",
    "\n",
    "        first_pop = []\n",
    "\n",
    "        # Generating K Best\n",
    "        while len(first_pop)<num_population:\n",
    "            new_pop=[random.randint(0,1) for _ in range(num_features)]\n",
    "            first_pop.append(new_pop)\n",
    "        fitness_values = evaluate(first_pop)\n",
    "\n",
    "        return first_pop, fitness_values\n",
    "\n",
    "\n",
    "    # Get's the features and returens the fitness function values\n",
    "\n",
    "    fitness_cache = {}\n",
    "\n",
    "    def evaluate(population):\n",
    "        Fitness_values = []\n",
    "        for chromosome in population:\n",
    "            feats = tuple(i for i,g in enumerate(chromosome) if g == 1)\n",
    "            if len(feats) < 4:   # حداقل تعداد ویژگی\n",
    "                Fitness_values.append(0.0)\n",
    "                continue\n",
    "            if feats in fitness_cache:\n",
    "                Fitness_values.append(fitness_cache[feats])\n",
    "                continue\n",
    "            score = cross_val_score(clf, X_train[:, feats], y_train,\n",
    "                                    scoring=\"accuracy\", cv=3, n_jobs=-1).mean()\n",
    "            fitness_cache[feats] = score\n",
    "            Fitness_values.append(score)\n",
    "        return Fitness_values\n",
    "\n",
    "\n",
    "\n",
    "    # Crossover\n",
    "    def crossover(p1, p2):\n",
    "        cross_point = random.randint(1, num_features)\n",
    "        c1 = p1[:cross_point] + p2[cross_point:]\n",
    "        c2 = p2[:cross_point] + p1[cross_point:]\n",
    "        return c1, c2\n",
    "\n",
    "\n",
    "    # Mutation\n",
    "    def mutate(individual):\n",
    "\n",
    "        mut_point = random.randint(1, num_features)\n",
    "\n",
    "        if random.randint(0, 1) == 1:\n",
    "            pop = [random.choice([0, 1]) for _ in range(num_features - mut_point)]\n",
    "            mutated_individual = individual[:mut_point] + pop[:]\n",
    "        else:\n",
    "            pop = [random.choice([0, 1]) for _ in range(mut_point)]\n",
    "            mutated_individual = pop[:] + individual[mut_point:]\n",
    "\n",
    "        return mutated_individual\n",
    "\n",
    "\n",
    "    # Genetic algorithm\n",
    "    def gen(population, rec_values):\n",
    "        new_population = []\n",
    "\n",
    "        # Generate new individuals\n",
    "        while len(new_population) < num_population:\n",
    "            selected_parents = [roulette_selection(rec_values) for _ in range(2)]\n",
    "            par1, par2 = population[selected_parents[0]], population[selected_parents[1]]\n",
    "\n",
    "            if random.random() < mutation_rate:\n",
    "                new1, new2 = mutate(par1), mutate(par2)\n",
    "                new_population.extend([new1, new2])\n",
    "            else:\n",
    "                ch1, ch2 = crossover(par1, par2)\n",
    "                new_population.extend([ch1, ch2])\n",
    "\n",
    "        # Evaluate fitness of new individuals\n",
    "        new_rec_values = evaluate(new_population)\n",
    "\n",
    "        # Combine populations and fitness values\n",
    "        population_pool = population + new_population\n",
    "        rec_values += new_rec_values\n",
    "\n",
    "        # Select elite individuals\n",
    "        elite_indices = heapq.nlargest(elitism_n, range(len(population)), key=lambda index: rec_values[index])\n",
    "        elite_population = [population[index] for index in elite_indices]\n",
    "        elite_rec_values = [rec_values[index] for index in elite_indices]\n",
    "\n",
    "        # Remove elite individuals from the population\n",
    "        for index in sorted(elite_indices, reverse=True):\n",
    "            del population_pool[index]\n",
    "            del rec_values[index]\n",
    "\n",
    "        # Select remaining individuals using roulette selection\n",
    "        selected_indices = [roulette_selection(rec_values) for _ in range(num_population - elitism_n)]\n",
    "        selected_population = [population_pool[index] for index in selected_indices]\n",
    "        selected_rec_values = [rec_values[index] for index in selected_indices]\n",
    "\n",
    "        # Update population and fitness values\n",
    "        population = elite_population + selected_population\n",
    "        rec_values = elite_rec_values + selected_rec_values\n",
    "\n",
    "        # Record best fitness and corresponding features\n",
    "        best_index = max(range(len(population)), key=lambda index: rec_values[index])\n",
    "        Best_rec.append(rec_values[best_index])\n",
    "        Best_fea.append(translate(population[best_index]))\n",
    "\n",
    "        return population, rec_values\n",
    "\n",
    "\n",
    "    def check_last_five_lists(list_of_lists):\n",
    "        if len(list_of_lists) < 15:\n",
    "            return False  # Not enough lists to compare\n",
    "\n",
    "        last_five_lists = list_of_lists[-15:]\n",
    "        # Check if all elements are the same\n",
    "        if all(lst == last_five_lists[0] for lst in last_five_lists):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    # Now running\n",
    "    generation = 0\n",
    "\n",
    "    for g in range(num_gens):\n",
    "        print(g)\n",
    "        if g == 0:\n",
    "            pop_u, rec_values = first_pop(df.shape[1] - 1, LogisticRegression(), X_train, y_train)\n",
    "\n",
    "        else:\n",
    "            pop_u, rec_values = gen(pop_u, rec_values)\n",
    "\n",
    "            clf.fit(X_train[:, Best_fea[-1]], y_train)\n",
    "            y_predic = clf.predict(X_train[:, Best_fea[-1]])\n",
    "            acc = accuracy_score(y_train, y_predic)\n",
    "\n",
    "            Results.loc[row, \"train\"] = acc\n",
    "            print(\"train: \", acc)\n",
    "\n",
    "            scores = cross_val_score(clf, X_train[:,  Best_fea[-1]], y_train, scoring='accuracy', cv=3)\n",
    "            acc = scores.mean()\n",
    "            Results.loc[row, \"validation\"] = acc\n",
    "            print(\"validation:\", acc)\n",
    "\n",
    "            y_predic = clf.predict(X_test[:, Best_fea[-1]])\n",
    "            acc = accuracy_score(y_test, y_predic)\n",
    "\n",
    "            Results.loc[row, \"test\"] = acc\n",
    "            print(\"test:\", acc)\n",
    "            print(\"***************\")\n",
    "\n",
    "            Results.loc[row, \"classifier\"] = type(clf).__name__\n",
    "            Results.loc[row, \"Best\"] = str(Best_fea[-1])\n",
    "            Results.loc[row, \"Best Res\"] = Best_rec[-1]\n",
    "            row += 1\n",
    "        generation += 1\n",
    "        if check_last_five_lists(Best_fea):\n",
    "            break\n",
    "\n",
    "    Results.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\GA\\Results{}.csv\".format(name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b7d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, mean_absolute_error\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from itertools import product\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\test.csv\")\n",
    "df_test = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\test_original.csv\")\n",
    "\n",
    "\n",
    "binary = [ 'Retino', 'htn', 'sex',  'CAD', 'CVA', 'Smoking']\n",
    "for bin in binary:\n",
    "    df[bin] = np.round(df[bin])\n",
    "    df_test[bin] = np.round(df_test[bin])\n",
    "\n",
    "dff = df.copy()\n",
    "dfff = df.copy()\n",
    "\n",
    "missings = [i for i in df.columns if df[i].isna().sum() > 0]\n",
    "continues = ['PLT', 'hip', 'CRP', 'VitD', 'insulin', 'UA', 'ast', 'alt', 'alkp', 'homa']\n",
    "\n",
    "\n",
    "\n",
    "param_grid_svr = {'C': [0.1, 1, 10], 'epsilon': [0.1, 0.2, 0.3]}\n",
    "param_grid_rf = {'n_estimators': [50, 100, 200], \"max_depth\" :[3,5,7]}\n",
    "param_grid_gbr = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "param_grid_etr = {'n_estimators': [50, 100, 200], \"max_depth\" :[3,5,7]}\n",
    "param_grid_abr = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1]}\n",
    "param_grid_lgbm = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "param_grid_xgboost = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "\n",
    "params = [param_grid_svr,param_grid_rf,  param_grid_gbr, param_grid_etr, param_grid_abr, param_grid_lgbm, param_grid_xgboost]\n",
    "\n",
    "\n",
    "estimators = {\n",
    "    'SVR': IterativeImputer(estimator=SVR(), random_state=101, max_iter=10, initial_strategy='mean'),\n",
    "    'RandomForest5': IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=101, initial_strategy='mean'),\n",
    "    'GradientBoosting': IterativeImputer(estimator=GradientBoostingRegressor(), max_iter=10, random_state=101, initial_strategy='mean'),\n",
    "    'ExtraTrees': IterativeImputer(estimator=ExtraTreesRegressor(), max_iter=10, random_state=101, initial_strategy='mean'),\n",
    "    'AdaBoost': IterativeImputer(estimator=AdaBoostRegressor(), max_iter=10, random_state=101, initial_strategy='mean'),\n",
    "    'LightGBM': IterativeImputer(estimator=LGBMRegressor(), max_iter=10, random_state=101, initial_strategy='mean'),\n",
    "    'XGBoost': IterativeImputer(estimator=XGBRegressor(), max_iter=10, random_state=101, initial_strategy='mean'),\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "\n",
    "# Function to evaluate the imputation results\n",
    "def evaluate_imputation(df_imputed, df_true, missings):\n",
    "    evaluations = {}\n",
    "    for col in missings:\n",
    "        missing_indices = dff[col].isna()\n",
    "        y_true = df_true.loc[missing_indices, col].values\n",
    "        y_pred = df_imputed.loc[missing_indices, col].values\n",
    "\n",
    "        if col in continues:\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            MABR = mean_absolute_error (y_true, y_pred)\n",
    "            evaluations[col] = {'MSE': mse, 'R2': r2, 'MABR':MABR}\n",
    "        else:\n",
    "            y_pred = np.round(y_pred).astype(int)\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "            evaluations[col] = {'Accuracy': acc}\n",
    "    return evaluations\n",
    "\n",
    "combos={}\n",
    "\n",
    "for j, (name, estimator) in enumerate(estimators.items()):\n",
    "    combinations = product(*params[j].values())\n",
    "\n",
    "    for i ,comb in enumerate(combinations):\n",
    "        print(f\"Imputing with {name} _ {i}...\")\n",
    "        param_combo = dict(zip(params[j].keys(), comb))\n",
    "        estimator.estimator.set_params(**param_combo)\n",
    "        combos[f\"{name}_{i}\"] = param_combo\n",
    "\n",
    "        if callable(estimator):\n",
    "            df_imputed = estimator(dfff.copy())\n",
    "        else:\n",
    "            df_imputed = estimator.fit_transform(dfff.copy())\n",
    "        df_imputed = pd.DataFrame(df_imputed, columns=dfff.columns)\n",
    "        results[f\"{name}_{i}\"] = evaluate_imputation(df_imputed, df_test, missings)\n",
    "\n",
    "\n",
    "print(results)\n",
    "\n",
    "scale_weight = {}\n",
    "sum_miss = np.sum(dff.isna().sum(), axis=0)\n",
    "\n",
    "for cls in df_test.columns:\n",
    "    we = dfff[cls].isna().sum()\n",
    "    scale_weight[cls] = we / sum_miss\n",
    "\n",
    "\n",
    "labels =[]\n",
    "con_values = []\n",
    "binary_values = []\n",
    "\n",
    "continues = ['PLT','hip', 'CRP', 'VitD', 'insulin', 'UA', 'ast', 'alt', 'alkp', 'homa']\n",
    "binary = ['Retino','CAD', 'CVA', 'Smoking']\n",
    "for est in results.keys():\n",
    "    labels.append(est)\n",
    "    continues_score = 0\n",
    "    binary_score = 0\n",
    "\n",
    "    scale_weight_con = 0\n",
    "    scale_weight_con_list = []\n",
    "    for con in continues:\n",
    "        continues_score = continues_score + results[est][con][\"R2\"] * scale_weight[con]\n",
    "        scale_weight_con = scale_weight_con + scale_weight[con]\n",
    "        scale_weight_con_list.append(scale_weight[con])\n",
    "    con_values.append(continues_score /scale_weight_con )\n",
    "\n",
    "    scale_weight_bin = 0\n",
    "    scale_weight_bin_list = []\n",
    "    for bin in binary:\n",
    "        binary_score = binary_score+ results[est][bin][\"Accuracy\"] * scale_weight[bin]\n",
    "        scale_weight_bin = scale_weight_bin + scale_weight[bin]\n",
    "        scale_weight_bin_list.append(scale_weight[bin])\n",
    "    binary_values.append(binary_score /scale_weight_bin )\n",
    "\n",
    "\n",
    "#\n",
    "# plt.bar(labels,con_values)\n",
    "# plt.title(\"continues\")\n",
    "# plt.show()\n",
    "#\n",
    "# plt.bar(labels,binary_values)\n",
    "# plt.title(\"binary\")\n",
    "# plt.show()\n",
    "\n",
    "Res = pd.DataFrame(columns=[\"Labels\" ,\"Labels2\", \"Parameters\" , \"Continues\" , \"Binary\",\"Details\"])\n",
    "Res[\"Labels\"] = labels\n",
    "Res[\"Labels2\"] = combos.keys()\n",
    "Res[\"Parameters\"] = combos.values()\n",
    "Res[\"Continues\"] = con_values\n",
    "Res[\"Binary\"] = binary_values\n",
    "Res[\"Details\"] = results.values()\n",
    "\n",
    "\n",
    "Wi_con = pd.DataFrame(columns = [\"Con\",\"Con_weight\",\"Bin\",\"Bin_weight\"])\n",
    "Wi_bin = pd.DataFrame(columns = [\"Bin\",\"Bin_weight\"])\n",
    "Wi_con[\"Con\"] = continues\n",
    "Wi_con[\"Con_weight\"] = scale_weight_con_list\n",
    "\n",
    "Wi_bin[\"Bin\"] = binary\n",
    "Wi_bin[\"Bin_weight\"] = scale_weight_bin_list\n",
    "\n",
    "\n",
    "\n",
    "Res.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\ResultsOfGrid.csv\")\n",
    "Wi_con.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\WeightsCon.csv\")\n",
    "Wi_bin.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\WeightsBin.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee9c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, mean_absolute_error, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from itertools import product\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "df = pd.read_csv(r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\python codes\\1-Missing Data\\reivse\\60-40\\test 60-40.csv\")\n",
    "df_test = pd.read_csv(r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\python codes\\1-Missing Data\\reivse\\60-40\\test_original 60-40.csv\")\n",
    "\n",
    "binary = ['Retino', 'htn', 'sex', 'CAD', 'CVA', 'Smoking']\n",
    "for bin in binary:\n",
    "    df[bin] = np.round(df[bin])\n",
    "    df_test[bin] = np.round(df_test[bin])\n",
    "\n",
    "dff = df.copy()\n",
    "dfff = df.copy()\n",
    "\n",
    "missings = [i for i in df.columns if df[i].isna().sum() > 0]\n",
    "continues = ['PLT', 'hip', 'CRP', 'VitD', 'insulin', 'UA', 'ast', 'alt', 'alkp', 'homa']\n",
    "reg_param_grid = {'fit_intercept': [True, False], 'copy_X': [True, False], 'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]}\n",
    "dtree_param_grid = {'max_depth': [3, 5, 8], 'min_samples_split': [2, 5, 10]}\n",
    "svm_param_grid = {'C': [0.1, 1, 10]}\n",
    "rf_param_grid = {'n_estimators': [100, 200, 300], 'max_depth': [3, 5, 8]}\n",
    "et_param_grid = {'n_estimators': [100, 200, 300], 'max_depth': [3, 5, 8]}\n",
    "xgb_param_grid = {'max_depth': [3, 5, 8], 'learning_rate': [0.1, 0.05, 0.01]}\n",
    "ada_param_grid = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1]}\n",
    "knn_param_grid = {'n_neighbors':[5,7,10,15]}\n",
    "\n",
    "\n",
    "\n",
    "params = [knn_param_grid]\n",
    "\n",
    "estimators = {\n",
    "    \"KNN\" : KNNImputer()\n",
    "}\n",
    "\n",
    "# Function to evaluate the imputation results\n",
    "def evaluate_imputation(df_imputed, df_true, missings):\n",
    "    evaluations = {}\n",
    "    for col in missings:\n",
    "        missing_indices = dff[col].isna()\n",
    "        y_true = df_true.loc[missing_indices, col].values\n",
    "        y_pred = df_imputed.loc[missing_indices, col].values\n",
    "\n",
    "        if col in continues:\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            MABR = mean_absolute_error(y_true, y_pred)\n",
    "            evaluations[col] = {'MSE': mse, 'R2': r2, 'MABR': MABR}\n",
    "        else:\n",
    "            y_pred = np.clip(y_pred, 0, 1)  # Clip values to [0, 1]\n",
    "            y_pred = np.round(y_pred).astype(int)\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "            F1 = f1_score(y_true, y_pred , zero_division = 0)\n",
    "            evaluations[col] = {'Accuracy': acc , \"F1\": F1}\n",
    "    return evaluations\n",
    "\n",
    "# Process each estimator and save results to CSV\n",
    "for j, (name, estimator) in enumerate(estimators.items()):\n",
    "    combinations = product(*params[j].values())\n",
    "    results = {}\n",
    "    combos = {}\n",
    "\n",
    "    for i, comb in enumerate(combinations):\n",
    "        print(f\"Imputing with {name} _ {i}...\")\n",
    "        param_combo = dict(zip(params[j].keys(), comb))\n",
    "        estimator.set_params(**param_combo)\n",
    "        combos[f\"{name}_{i}\"] = param_combo\n",
    "\n",
    "        df_imputed = estimator.fit_transform(dfff.copy())\n",
    "        df_imputed = pd.DataFrame(df_imputed, columns=dfff.columns)\n",
    "        results[f\"{name}_{i}\"] = evaluate_imputation(df_imputed, df_test, missings)\n",
    "\n",
    "    scale_weight = {}\n",
    "    sum_miss = np.sum(dff.isna().sum(), axis=0)\n",
    "\n",
    "    for cls in df_test.columns:\n",
    "        we = dfff[cls].isna().sum()\n",
    "        scale_weight[cls] = we / sum_miss\n",
    "\n",
    "    labels = []\n",
    "    con_values = []\n",
    "    binary_values = []\n",
    "\n",
    "    continues = ['PLT', 'hip', 'CRP', 'VitD', 'insulin', 'UA', 'ast', 'alt', 'alkp', 'homa']\n",
    "    binary = ['Retino', 'CAD', 'CVA', 'Smoking']\n",
    "    for est in results.keys():\n",
    "        labels.append(est)\n",
    "        continues_score = 0\n",
    "        binary_score = 0\n",
    "\n",
    "        scale_weight_con = 0\n",
    "        scale_weight_con_list = []\n",
    "        for con in continues:\n",
    "            continues_score += results[est][con][\"R2\"] * scale_weight[con]\n",
    "            scale_weight_con += scale_weight[con]\n",
    "            scale_weight_con_list.append(scale_weight[con])\n",
    "        con_values.append(continues_score / scale_weight_con)\n",
    "\n",
    "        scale_weight_bin = 0\n",
    "        scale_weight_bin_list = []\n",
    "        for bin in binary:\n",
    "            binary_score += results[est][bin][\"Accuracy\"] * scale_weight[bin]\n",
    "            scale_weight_bin += scale_weight[bin]\n",
    "            scale_weight_bin_list.append(scale_weight[bin])\n",
    "        binary_values.append(binary_score / scale_weight_bin)\n",
    "\n",
    "    Res = pd.DataFrame(columns=[\"Labels\", \"Labels2\", \"Parameters\", \"Continues\", \"Binary\", \"Details\"])\n",
    "    Res[\"Labels\"] = labels\n",
    "    Res[\"Labels2\"] = combos.keys()\n",
    "    Res[\"Parameters\"] = combos.values()\n",
    "    Res[\"Continues\"] = con_values\n",
    "    Res[\"Binary\"] = binary_values\n",
    "    Res[\"Details\"] = results.values()\n",
    "\n",
    "    Wi_con = pd.DataFrame(columns=[\"Con\", \"Con_weight\"])\n",
    "    Wi_bin = pd.DataFrame(columns=[\"Bin\", \"Bin_weight\"])\n",
    "    Wi_con[\"Con\"] = continues\n",
    "    Wi_con[\"Con_weight\"] = scale_weight_con_list\n",
    "\n",
    "    Wi_bin[\"Bin\"] = binary\n",
    "    Wi_bin[\"Bin_weight\"] = scale_weight_bin_list\n",
    "\n",
    "    Res.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\ResultsOfGrid_KNN.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11aff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\datasets\\Data\\3-my_null_data_40_del.csv\")\n",
    "imputer =  IterativeImputer(estimator=ExtraTreesRegressor(n_estimators = 300, max_depth = 8), max_iter=10, random_state=101, initial_strategy='mean')\n",
    "\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "imputed_data = pd.DataFrame(imputed_data , columns=data.columns)\n",
    "binary = ['Retino', 'htn', 'sex', 'CAD', 'CVA', 'Smoking']\n",
    "for col in binary:\n",
    "    imputed_data[col] = np.round(imputed_data[col])\n",
    "imputed_data.to_csv(r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\datasets\\Data\\4-imputed_data.csv\" , index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13705f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data =  pd.read_csv(r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\python codes\\1-Missing Data\\reivse\\90-10\\ResultsOfGrid_XGBoost.csv\")\n",
    "\n",
    "estimators = [\n",
    "    'Ridge' ,\n",
    "    'DecisionTree',\n",
    "    'SVR',\n",
    "    'RandomForest',\n",
    "    'ExtraTrees',\n",
    "#    'XGBoost',\n",
    "    'AdaBoost',\n",
    "    # \"missforest\",\n",
    "    # \"KNN\"\n",
    "]\n",
    "\n",
    "for i in estimators:\n",
    "    new_data = pd.read_csv(r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\python codes\\1-Missing Data\\reivse\\90-10\\ResultsOfGrid_{}.csv\".format(i))\n",
    "    data = pd.concat((data , new_data))\n",
    "\n",
    "data.to_csv(r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\python codes\\1-Missing Data\\reivse\\90-10\\AllToGether.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, mean_absolute_error, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from itertools import product\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "df = pd.read_csv(r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\python codes\\1-Missing Data\\reivse\\60-40\\test 60-40.csv\")\n",
    "df_test = pd.read_csv(r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\python codes\\1-Missing Data\\reivse\\60-40\\test_original 60-40.csv\")\n",
    "\n",
    "binary = ['Retino', 'htn', 'sex', 'CAD', 'CVA', 'Smoking']\n",
    "for bin in binary:\n",
    "    df[bin] = np.round(df[bin])\n",
    "    df_test[bin] = np.round(df_test[bin])\n",
    "\n",
    "dff = df.copy()\n",
    "dfff = df.copy()\n",
    "\n",
    "missings = [i for i in df.columns if df[i].isna().sum() > 0]\n",
    "continues = ['PLT', 'hip', 'CRP', 'VitD', 'insulin', 'UA', 'ast', 'alt', 'alkp', 'homa']\n",
    "reg_param_grid = {'fit_intercept': [True, False], 'copy_X': [True, False], 'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]}\n",
    "dtree_param_grid = {'max_depth': [3, 5, 8], 'min_samples_split': [2, 5, 10]}\n",
    "svm_param_grid = {'C': [0.1, 1, 10]}\n",
    "rf_param_grid = {'n_estimators': [100, 200, 300], 'max_depth': [3, 5, 8]}\n",
    "et_param_grid = {'n_estimators': [100, 200, 300], 'max_depth': [3, 5, 8]}\n",
    "xgb_param_grid = {'max_depth': [3, 5, 8], 'learning_rate': [0.1, 0.05, 0.01]}\n",
    "ada_param_grid = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1]}\n",
    "knn_param_grid = {'n_neighbors':[5,7,10,15]}\n",
    "\n",
    "\n",
    "\n",
    "params = [knn_param_grid]\n",
    "\n",
    "estimators = {\n",
    "    \"missforest\" : KNNImputer()\n",
    "\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "# Function to evaluate the imputation results\n",
    "def evaluate_imputation(df_imputed, df_true, missings):\n",
    "    evaluations = {}\n",
    "    for col in missings:\n",
    "        missing_indices = dff[col].isna()\n",
    "        y_true = df_true.loc[missing_indices, col].values\n",
    "        y_pred = df_imputed.loc[missing_indices, col].values\n",
    "\n",
    "        if col in continues:\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            MABR = mean_absolute_error(y_true, y_pred)\n",
    "            evaluations[col] = {'MSE': mse, 'R2': r2, 'MABR': MABR}\n",
    "        else:\n",
    "            y_pred = np.clip(y_pred, 0, 1)  # Clip values to [0, 1]\n",
    "            y_pred = np.round(y_pred).astype(int)\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "            F1 = f1_score(y_true, y_pred , zero_division = 0)\n",
    "            evaluations[col] = {'Accuracy': acc , \"F1\": F1}\n",
    "    return evaluations\n",
    "\n",
    "# Process each estimator and save results to CSV\n",
    "for j, (name, estimator) in enumerate(estimators.items()):\n",
    "    combinations = product(*params[j].values())\n",
    "    results = {}\n",
    "    combos = {}\n",
    "\n",
    "    for i in range(1,10):\n",
    "        print(f\"Imputing with {name} _ {i}...\")\n",
    "\n",
    "        df_imputed = pd.read_csv(r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\python codes\\1-Missing Data\\reivse\\60-40\\missforest 60-40\\output{}.csv\".format(i))\n",
    "        results[f\"{name}_{i}\"] = evaluate_imputation(df_imputed, df_test, missings)\n",
    "\n",
    "    scale_weight = {}\n",
    "    sum_miss = np.sum(dff.isna().sum(), axis=0)\n",
    "\n",
    "    for cls in df_test.columns:\n",
    "        we = dfff[cls].isna().sum()\n",
    "        scale_weight[cls] = we / sum_miss\n",
    "\n",
    "    labels = []\n",
    "    con_values = []\n",
    "    binary_values = []\n",
    "\n",
    "    continues = ['PLT', 'hip', 'CRP', 'VitD', 'insulin', 'UA', 'ast', 'alt', 'alkp', 'homa']\n",
    "    binary = ['Retino', 'CAD', 'CVA', 'Smoking']\n",
    "    for est in results.keys():\n",
    "        labels.append(est)\n",
    "        continues_score = 0\n",
    "        binary_score = 0\n",
    "\n",
    "        scale_weight_con = 0\n",
    "        scale_weight_con_list = []\n",
    "        for con in continues:\n",
    "            continues_score += results[est][con][\"R2\"] * scale_weight[con]\n",
    "            scale_weight_con += scale_weight[con]\n",
    "            scale_weight_con_list.append(scale_weight[con])\n",
    "        con_values.append(continues_score / scale_weight_con)\n",
    "\n",
    "        scale_weight_bin = 0\n",
    "        scale_weight_bin_list = []\n",
    "        for bin in binary:\n",
    "            binary_score += results[est][bin][\"Accuracy\"] * scale_weight[bin]\n",
    "            scale_weight_bin += scale_weight[bin]\n",
    "            scale_weight_bin_list.append(scale_weight[bin])\n",
    "        binary_values.append(binary_score / scale_weight_bin)\n",
    "\n",
    "    Res = pd.DataFrame(columns=[\"Labels\", \"Labels2\", \"Parameters\", \"Continues\", \"Binary\", \"Details\"])\n",
    "    Res[\"Labels\"] = labels\n",
    "    # Res[\"Labels2\"] = combos.keys()\n",
    "    # Res[\"Parameters\"] = combos.values()\n",
    "    Res[\"Continues\"] = con_values\n",
    "    Res[\"Binary\"] = binary_values\n",
    "    Res[\"Details\"] = results.values()\n",
    "\n",
    "    Wi_con = pd.DataFrame(columns=[\"Con\", \"Con_weight\"])\n",
    "    Wi_bin = pd.DataFrame(columns=[\"Bin\", \"Bin_weight\"])\n",
    "    Wi_con[\"Con\"] = continues\n",
    "    Wi_con[\"Con_weight\"] = scale_weight_con_list\n",
    "\n",
    "    Wi_bin[\"Bin\"] = binary\n",
    "    Wi_bin[\"Bin_weight\"] = scale_weight_bin_list\n",
    "\n",
    "    Res.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\ResultsOfGrid_missforest.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e634ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Reproducible CV with CIs (No Feature Selection) ====\n",
    "# Requirements: scikit-learn, numpy, pandas, xgboost, lightgbm\n",
    "# If needed: pip install scikit-learn xgboost lightgbm pandas numpy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# ------------------------------\n",
    "# 1) تنظیمات پایه\n",
    "# ------------------------------\n",
    "TARGET_COL = \"fattyliver\"    # <<--- نام ستون هدفت رو اینجا بگذار (۰/۱)\n",
    "N_SPLITS = 5            # K-fold\n",
    "SEEDS: List[int] = [42, 52, 62, 72, 82, 92, 102, 112, 122, 132]  # 10 تکرار با بذرهای مختلف\n",
    "SCORING_AVG = \"binary\"  # برای precision/recall/f1 روی کلاس مثبت (label=1)\n",
    "POS_LABEL = 1\n",
    "\n",
    "# ------------------------------\n",
    "# 2) آماده‌سازی مدل‌ها با هایپرپارامترهای اعلام‌شده\n",
    "#    (اسکیلر فقط برای مدل‌های حساس به مقیاس درون Pipeline)\n",
    "# ------------------------------\n",
    "def make_models() -> Dict[str, Pipeline]:\n",
    "    models = {}\n",
    "\n",
    "    # Logistic Regression {'C': 1}\n",
    "    models[\"Logistic Regression\"] = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"clf\", LogisticRegression(C=1, solver=\"liblinear\", max_iter=1000, random_state=0))\n",
    "    ])\n",
    "\n",
    "    # KNN {'n_neighbors': 7, 'weights': 'distance'}\n",
    "    models[\"KNN\"] = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"clf\", KNeighborsClassifier(n_neighbors=7, weights=\"distance\"))\n",
    "    ])\n",
    "\n",
    "    # SVM {'C': 1}\n",
    "    models[\"SVM\"] = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"clf\", SVC(C=1, probability=True, random_state=0))\n",
    "    ])\n",
    "\n",
    "    # Decision Tree {'max_depth': 5, 'min_samples_split': 5}\n",
    "    models[\"Decision Tree\"] = Pipeline([\n",
    "        (\"clf\", DecisionTreeClassifier(max_depth=5, min_samples_split=5, random_state=0))\n",
    "    ])\n",
    "\n",
    "    # Random Forest {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 300}\n",
    "    models[\"Random Forest\"] = Pipeline([\n",
    "        (\"clf\", RandomForestClassifier(\n",
    "            n_estimators=300, max_depth=None, min_samples_split=10, n_jobs=-1, random_state=0\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Extra Trees {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 300}\n",
    "    models[\"Extra Tree\"] = Pipeline([\n",
    "        (\"clf\", ExtraTreesClassifier(\n",
    "            n_estimators=300, max_depth=None, min_samples_split=2, n_jobs=-1, random_state=0\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Gradient Boosting {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300}\n",
    "    models[\"Gradient Boosting\"] = Pipeline([\n",
    "        (\"clf\", GradientBoostingClassifier(\n",
    "            learning_rate=0.1, max_depth=5, n_estimators=300, random_state=0\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # XGBoost {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300}\n",
    "    models[\"XGBoost\"] = Pipeline([\n",
    "        (\"clf\", XGBClassifier(\n",
    "            n_estimators=300, max_depth=5, learning_rate=0.1,\n",
    "            subsample=1.0, colsample_bytree=1.0,\n",
    "            eval_metric=\"logloss\", use_label_encoder=False,\n",
    "            tree_method=\"hist\", n_jobs=-1, random_state=0\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # LightGBM {'force_col_wise': True, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'num_leaves': 31}\n",
    "    models[\"LightGBM\"] = Pipeline([\n",
    "        (\"clf\", LGBMClassifier(\n",
    "            n_estimators=100, learning_rate=0.1, max_depth=7, num_leaves=31,\n",
    "            force_col_wise=True, random_state=0, n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    return models\n",
    "\n",
    "# ------------------------------\n",
    "# 3) توابع کمکی برای CI\n",
    "# ------------------------------\n",
    "def mean_sd_ci(x: np.ndarray, alpha: float = 0.05) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    برمی‌گرداند: mean, sd, ci_low, ci_high برای سطح اطمینان ۱-آلفا\n",
    "    از تقریب نرمال/تی با sd/√n استفاده می‌کند.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    n = len(x)\n",
    "    mean = x.mean()\n",
    "    sd = x.std(ddof=1) if n > 1 else 0.0\n",
    "    # برای نمونه‌های نسبتاً بزرگ، 1.96 خوبه؛ اگر خواستی دقیق‌تر: t-quantile\n",
    "    z = 1.96\n",
    "    se = sd / np.sqrt(max(n, 1))\n",
    "    return mean, sd, mean - z * se, mean + z * se\n",
    "\n",
    "# ------------------------------\n",
    "# 4) حلقه CV تکراری با بذرهای مختلف\n",
    "# ------------------------------\n",
    "def evaluate_models_repeated_cv(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = TARGET_COL,\n",
    "    seeds: List[int] = SEEDS,\n",
    "    n_splits: int = N_SPLITS\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    X = df.drop(columns=[target_col]).values\n",
    "    y = df[target_col].values\n",
    "\n",
    "    models = make_models()\n",
    "\n",
    "    rows_per_fold = []   # نتایج هر فولد/تکرار\n",
    "    summary_rows = []    # خلاصه نهایی\n",
    "\n",
    "    for model_name, pipe in models.items():\n",
    "        print(\"Now Runing:\",model_name)\n",
    "        # برای هر مدل، با هر seed یک StratifiedKFold تازه می‌سازیم\n",
    "        per_fold_scores = {\n",
    "            \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": [], \"roc_auc\": []\n",
    "        }\n",
    "\n",
    "        for seed in seeds:\n",
    "            skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "            for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X, y), start=1):\n",
    "                X_train, X_test = X[train_idx], X[test_idx]\n",
    "                y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "                pipe.fit(X_train, y_train)\n",
    "\n",
    "                # پیش‌بینی احتمال برای ROC-AUC (در صورت نبود predict_proba از decision_function استفاده می‌کنیم)\n",
    "                if hasattr(pipe[-1], \"predict_proba\"):\n",
    "                    y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "                elif hasattr(pipe[-1], \"decision_function\"):\n",
    "                    # decision_function را به [0,1] تبدیل می‌کنیم\n",
    "                    df_raw = pipe.decision_function(X_test)\n",
    "                    # نگاشت مین-مکس\n",
    "                    df_min, df_max = df_raw.min(), df_raw.max()\n",
    "                    if df_max > df_min:\n",
    "                        y_proba = (df_raw - df_min) / (df_max - df_min)\n",
    "                    else:\n",
    "                        y_proba = np.zeros_like(df_raw, dtype=float)\n",
    "                else:\n",
    "                    # fallback (به ندرت)\n",
    "                    y_proba = pipe.predict(X_test).astype(float)\n",
    "\n",
    "                y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                prec = precision_score(y_test, y_pred, average=SCORING_AVG, zero_division=0, pos_label=POS_LABEL)\n",
    "                rec = recall_score(y_test, y_pred, average=SCORING_AVG, zero_division=0, pos_label=POS_LABEL)\n",
    "                f1 = f1_score(y_test, y_pred, average=SCORING_AVG, zero_division=0, pos_label=POS_LABEL)\n",
    "                try:\n",
    "                    auc = roc_auc_score(y_test, y_proba)\n",
    "                except ValueError:\n",
    "                    # اگر فقط یک کلاس در y_test باشد\n",
    "                    auc = np.nan\n",
    "\n",
    "                per_fold_scores[\"accuracy\"].append(acc)\n",
    "                per_fold_scores[\"precision\"].append(prec)\n",
    "                per_fold_scores[\"recall\"].append(rec)\n",
    "                per_fold_scores[\"f1\"].append(f1)\n",
    "                per_fold_scores[\"roc_auc\"].append(auc)\n",
    "\n",
    "                rows_per_fold.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"seed\": seed,\n",
    "                    \"fold\": fold_idx,\n",
    "                    \"accuracy\": acc,\n",
    "                    \"precision\": prec,\n",
    "                    \"recall\": rec,\n",
    "                    \"f1\": f1,\n",
    "                    \"roc_auc\": auc\n",
    "                })\n",
    "\n",
    "        # خلاصه آماری با CI برای هر متریک\n",
    "        for metric in [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]:\n",
    "            arr = np.array(per_fold_scores[metric], dtype=float)\n",
    "            # حذف nan برای roc_auc در صورت لزوم\n",
    "            arr = arr[~np.isnan(arr)]\n",
    "            mean, sd, lo, hi = mean_sd_ci(arr) if len(arr) > 0 else (np.nan, np.nan, np.nan, np.nan)\n",
    "            summary_rows.append({\n",
    "                \"model\": model_name,\n",
    "                \"metric\": metric,\n",
    "                \"mean\": mean,\n",
    "                \"std\": sd,\n",
    "                \"ci95_low\": lo,\n",
    "                \"ci95_high\": hi,\n",
    "                \"n_folds\": len(arr)\n",
    "            })\n",
    "\n",
    "    per_fold_df = pd.DataFrame(rows_per_fold)\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "    return per_fold_df, summary_df\n",
    "\n",
    "# ------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5) مثالِ اجرا\n",
    "# ------------------------------\n",
    "# فرض: df را از قبل ساخته‌ای و ستون TARGET_COL را دارد\n",
    "df = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\6-balanced_data.csv\")  # نمونه\n",
    "\n",
    "per_fold_df, summary_df = evaluate_models_repeated_cv(df, target_col=TARGET_COL)\n",
    "\n",
    "# # ذخیره برای گزارش به داورها / ضمیمه‌ها\n",
    "per_fold_df.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\cv_per_fold_no_fs.csv\", index=False)\n",
    "summary_df.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\cv_summary_no_fs.csv\", index=False)\n",
    "\n",
    "# # نمایش خلاصه مرتب‌شده بر اساس AUC\n",
    "print(\n",
    "    summary_df[summary_df[\"metric\"] == \"roc_auc\"]\n",
    "    .sort_values(\"mean\", ascending=False)\n",
    "    .assign(mean_sd=lambda d: d[\"mean\"].round(3).astype(str) + \" ± \" + d[\"std\"].round(3).astype(str),\n",
    "            ci95=lambda d: \"[\" + d[\"ci95_low\"].round(3).astype(str) + \", \" + d[\"ci95_high\"].round(3).astype(str) + \"]\")\n",
    "    [[\"model\",\"mean_sd\",\"ci95\",\"n_folds\"]]\n",
    "    .to_string(index=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b27ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Repeated CV + 95% CI with SelectKBest (per-model K) ====\n",
    "# Requirements: scikit-learn, numpy, pandas, xgboost, lightgbm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ------------------------------\n",
    "# تنظیمات\n",
    "# ------------------------------\n",
    "TARGET_COL = \"fattyliver\"   # اگر نمی‌خواهی به نام تکیه کنی، پایین از iloc استفاده کن\n",
    "N_SPLITS = 5\n",
    "SEEDS = [42, 52, 62, 72, 82, 92, 102, 112, 122, 132]  # 10 تکرار\n",
    "SCORING_AVG = \"binary\"\n",
    "POS_LABEL = 1\n",
    "\n",
    "# Kهای منتخب برای هر مدل (طبق چیزی که دادی)\n",
    "# K_MAP: Dict[str, int] = {\n",
    "#     \"Logistic Regression\": 28,\n",
    "#     \"KNN\": 6,\n",
    "#     \"SVM\": 28,\n",
    "#     \"Decision Tree\": 5,\n",
    "#     \"Random Forest\": 31,\n",
    "#     \"Extra Tree\": 22,\n",
    "#     \"Gradient Boosting\": 25,\n",
    "#     \"XGBoost\": 22,\n",
    "#     \"LightGBM\": 21,\n",
    "# }\n",
    "\n",
    "#For with SMOTE\n",
    "K_MAP: Dict[str, int] = {\n",
    "    \"Logistic Regression\": 26,\n",
    "    \"KNN\": 12,\n",
    "    \"SVM\": 18,\n",
    "    \"Decision Tree\": 8,\n",
    "    \"Random Forest\": 22,\n",
    "    \"Extra Tree\": 24,\n",
    "    \"Gradient Boosting\": 22,\n",
    "    \"XGBoost\": 31,\n",
    "    \"LightGBM\": 30,\n",
    "}\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# ساخت مدل‌ها + KBest داخل Pipeline\n",
    "# ------------------------------\n",
    "def make_models_kbest() -> Dict[str, Pipeline]:\n",
    "    # امتیازدهی با MI؛ برای تصادفی‌بودن MI، random_state می‌گذاریم تا تکرارپذیر باشد\n",
    "    selector = lambda k: SelectKBest(score_func=lambda X, y: mutual_info_classif(\n",
    "        X, y, random_state=0, discrete_features=\"auto\"\n",
    "    ), k=k)\n",
    "\n",
    "    models = {}\n",
    "\n",
    "    # 1) Logistic Regression  {'C': 1}\n",
    "    models[\"Logistic Regression\"] = Pipeline([\n",
    "        (\"kbest\", selector(K_MAP[\"Logistic Regression\"])),\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"clf\", LogisticRegression(C=1, solver=\"liblinear\", max_iter=1000, random_state=0)),\n",
    "    ])\n",
    "\n",
    "    # 2) KNN  {'n_neighbors': 7, 'weights': 'distance'}\n",
    "    models[\"KNN\"] = Pipeline([\n",
    "        (\"kbest\", selector(K_MAP[\"KNN\"])),\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"clf\", KNeighborsClassifier(n_neighbors=7, weights=\"distance\")),\n",
    "    ])\n",
    "\n",
    "    # 3) SVM  {'C': 1}\n",
    "    models[\"SVM\"] = Pipeline([\n",
    "        (\"kbest\", selector(K_MAP[\"SVM\"])),\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"clf\", SVC(C=1, probability=True, random_state=0)),\n",
    "    ])\n",
    "\n",
    "    # 4) Decision Tree  {'max_depth': 5, 'min_samples_split': 5}\n",
    "    models[\"Decision Tree\"] = Pipeline([\n",
    "        (\"kbest\", selector(K_MAP[\"Decision Tree\"])),\n",
    "        (\"clf\", DecisionTreeClassifier(max_depth=5, min_samples_split=5, random_state=0)),\n",
    "    ])\n",
    "\n",
    "    # 5) Random Forest  {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 300}\n",
    "    models[\"Random Forest\"] = Pipeline([\n",
    "        (\"kbest\", selector(K_MAP[\"Random Forest\"])),\n",
    "        (\"clf\", RandomForestClassifier(\n",
    "            n_estimators=300, max_depth=None, min_samples_split=10, n_jobs=-1, random_state=0\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "    # 6) Extra Tree  {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 300}\n",
    "    models[\"Extra Tree\"] = Pipeline([\n",
    "        (\"kbest\", selector(K_MAP[\"Extra Tree\"])),\n",
    "        (\"clf\", ExtraTreesClassifier(\n",
    "            n_estimators=300, max_depth=None, min_samples_split=2, n_jobs=-1, random_state=0\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "    # 7) Gradient Boosting  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300}\n",
    "    models[\"Gradient Boosting\"] = Pipeline([\n",
    "        (\"kbest\", selector(K_MAP[\"Gradient Boosting\"])),\n",
    "        (\"clf\", GradientBoostingClassifier(\n",
    "            learning_rate=0.1, max_depth=5, n_estimators=300, random_state=0\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "    # 8) XGBoost  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300}\n",
    "    models[\"XGBoost\"] = Pipeline([\n",
    "        (\"kbest\", selector(K_MAP[\"XGBoost\"])),\n",
    "        (\"clf\", XGBClassifier(\n",
    "            n_estimators=300, max_depth=5, learning_rate=0.1,\n",
    "            subsample=1.0, colsample_bytree=1.0,\n",
    "            eval_metric=\"logloss\", use_label_encoder=False,\n",
    "            tree_method=\"hist\", n_jobs=-1, random_state=0\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "    # 9) LightGBM  {'force_col_wise': True, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'num_leaves': 31}\n",
    "    models[\"LightGBM\"] = Pipeline([\n",
    "        (\"kbest\", selector(K_MAP[\"LightGBM\"])),\n",
    "        (\"clf\", LGBMClassifier(\n",
    "            n_estimators=100, learning_rate=0.1, max_depth=7, num_leaves=31,\n",
    "            force_col_wise=True, random_state=0, n_jobs=-1\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "    return models\n",
    "\n",
    "# ------------------------------\n",
    "# محاسبه میانگین/انحراف معیار/CI95\n",
    "# ------------------------------\n",
    "def mean_sd_ci(x: np.ndarray, alpha: float = 0.05) -> Tuple[float, float, float, float]:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    n = len(x)\n",
    "    mean = x.mean()\n",
    "    sd = x.std(ddof=1) if n > 1 else 0.0\n",
    "    z = 1.96\n",
    "    se = sd / np.sqrt(max(n, 1))\n",
    "    return mean, sd, mean - z * se, mean + z * se\n",
    "\n",
    "# ------------------------------\n",
    "# موتور ارزیابی (تکرارهای CV با seedهای مختلف)\n",
    "# ------------------------------\n",
    "def evaluate_models_repeated_cv_kbest(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = TARGET_COL,\n",
    "    seeds: List[int] = SEEDS,\n",
    "    n_splits: int = N_SPLITS\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    # اگر نمی‌خواهی روی نام ستون تکیه کنی:\n",
    "    # X = df.iloc[:, :-1].values\n",
    "    # y = df.iloc[:, -1].values\n",
    "    X = df.drop(columns=[target_col]).values\n",
    "    y = df[target_col].values\n",
    "\n",
    "    models = make_models_kbest()\n",
    "\n",
    "    rows_per_fold = []\n",
    "    summary_rows = []\n",
    "\n",
    "    for model_name, pipe in models.items():\n",
    "        per_fold_scores = {m: [] for m in [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]}\n",
    "\n",
    "        for seed in seeds:\n",
    "            skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "            for fold_idx, (tr, te) in enumerate(skf.split(X, y), start=1):\n",
    "                X_train, X_test = X[tr], X[te]\n",
    "                y_train, y_test = y[tr], y[te]\n",
    "\n",
    "                pipe.fit(X_train, y_train)\n",
    "\n",
    "                # احتمال برای AUC\n",
    "                if hasattr(pipe[-1], \"predict_proba\"):\n",
    "                    y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "                elif hasattr(pipe[-1], \"decision_function\"):\n",
    "                    df_raw = pipe.decision_function(X_test)\n",
    "                    mn, mx = df_raw.min(), df_raw.max()\n",
    "                    y_proba = (df_raw - mn) / (mx - mn) if mx > mn else np.zeros_like(df_raw, dtype=float)\n",
    "                else:\n",
    "                    y_proba = pipe.predict(X_test).astype(float)\n",
    "\n",
    "                y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                prec = precision_score(y_test, y_pred, average=SCORING_AVG, zero_division=0, pos_label=POS_LABEL)\n",
    "                rec = recall_score(y_test, y_pred, average=SCORING_AVG, zero_division=0, pos_label=POS_LABEL)\n",
    "                f1 = f1_score(y_test, y_pred, average=SCORING_AVG, zero_division=0, pos_label=POS_LABEL)\n",
    "                try:\n",
    "                    auc = roc_auc_score(y_test, y_proba)\n",
    "                except ValueError:\n",
    "                    auc = np.nan\n",
    "\n",
    "                per_fold_scores[\"accuracy\"].append(acc)\n",
    "                per_fold_scores[\"precision\"].append(prec)\n",
    "                per_fold_scores[\"recall\"].append(rec)\n",
    "                per_fold_scores[\"f1\"].append(f1)\n",
    "                per_fold_scores[\"roc_auc\"].append(auc)\n",
    "\n",
    "                rows_per_fold.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"seed\": seed,\n",
    "                    \"fold\": fold_idx,\n",
    "                    \"accuracy\": acc,\n",
    "                    \"precision\": prec,\n",
    "                    \"recall\": rec,\n",
    "                    \"f1\": f1,\n",
    "                    \"roc_auc\": auc,\n",
    "                    \"kbest_k\": K_MAP[model_name]\n",
    "                })\n",
    "\n",
    "        for metric in [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]:\n",
    "            arr = np.array(per_fold_scores[metric], dtype=float)\n",
    "            arr = arr[~np.isnan(arr)]\n",
    "            mean, sd, lo, hi = mean_sd_ci(arr) if len(arr) > 0 else (np.nan, np.nan, np.nan, np.nan)\n",
    "            summary_rows.append({\n",
    "                \"model\": model_name,\n",
    "                \"metric\": metric,\n",
    "                \"mean\": mean,\n",
    "                \"std\": sd,\n",
    "                \"ci95_low\": lo,\n",
    "                \"ci95_high\": hi,\n",
    "                \"n_folds\": len(arr),\n",
    "                \"kbest_k\": K_MAP[model_name]\n",
    "            })\n",
    "\n",
    "    per_fold_df = pd.DataFrame(rows_per_fold)\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    return per_fold_df, summary_df\n",
    "\n",
    "# ------------------------------\n",
    "# مثال اجرا\n",
    "# ------------------------------\n",
    "df = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\6-balanced_data.csv\")\n",
    "\n",
    "\n",
    "per_fold_df, summary_df = evaluate_models_repeated_cv_kbest(df, target_col=TARGET_COL)\n",
    "per_fold_df.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\cv_per_fold_kbest.csv\", index=False)\n",
    "summary_df.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\cv_summary_kbest.csv\", index=False)\n",
    "\n",
    "print(\n",
    "    summary_df[summary_df[\"metric\"] == \"roc_auc\"]\n",
    "    .sort_values(\"mean\", ascending=False)\n",
    "    .assign(mean_sd=lambda d: d[\"mean\"].round(3).astype(str) + \" ± \" + d[\"std\"].round(3).astype(str),\n",
    "            ci95=lambda d: \"[\" + d[\"ci95_low\"].round(3).astype(str) + \", \" + d[\"ci95_high\"].round(3).astype(str) + \"]\")\n",
    "    [[\"model\",\"kbest_k\",\"mean_sd\",\"ci95\",\"n_folds\"]]\n",
    "    .to_string(index=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31fe2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Repeated CV + 95% CI with PCA (per-model n_components) ====\n",
    "# Requirements: scikit-learn, numpy, pandas, xgboost, lightgbm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ------------------------------\n",
    "# تنظیمات\n",
    "# ------------------------------\n",
    "TARGET_COL = \"fattyliver\"   # اگر نمی‌خواهی روی نام تکیه کنی، پایین از iloc استفاده کن\n",
    "N_SPLITS = 5\n",
    "SEEDS: List[int] = [42, 52, 62, 72, 82, 92, 102, 112, 122, 132]  # 10 تکرار\n",
    "SCORING_AVG = \"binary\"\n",
    "POS_LABEL = 1\n",
    "\n",
    "# بهترین تعداد مؤلفه‌های PCA برای هر مدل\n",
    "# PCA_MAP: Dict[str, int] = {\n",
    "#     \"Logistic Regression\": 30,\n",
    "#     \"KNN\": 15,\n",
    "#     \"SVM\": 30,\n",
    "#     \"Decision Tree\": 14,\n",
    "#     \"Random Forest\": 28,\n",
    "#     \"Extra Tree\": 28,\n",
    "#     \"Gradient Boosting\": 30,\n",
    "#     \"XGBoost\": 25,\n",
    "#     \"LightGBM\": 30,\n",
    "# }\n",
    "\n",
    "#PCA with SMOTE\n",
    "PCA_MAP: Dict[str, int] = {\n",
    "    \"Logistic Regression\": 29,\n",
    "    \"KNN\": 17,\n",
    "    \"SVM\": 30,\n",
    "    \"Decision Tree\": 20,\n",
    "    \"Random Forest\": 17,\n",
    "    \"Extra Tree\": 29,\n",
    "    \"Gradient Boosting\": 30,\n",
    "    \"XGBoost\": 30,\n",
    "    \"LightGBM\": 30,\n",
    "}\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# ساخت مدل‌ها + PCA داخل Pipeline\n",
    "# ------------------------------\n",
    "def make_models_pca() -> Dict[str, Pipeline]:\n",
    "    # PCA بعد از StandardScaler برای همه مدل‌ها (یکسان‌سازی مقیاس قبل از تجزیه)\n",
    "    # whiten=False چون معمولاً برای طبقه‌بندی لازم نیست و نویز اضافه می‌کند\n",
    "    def block(nc: int):\n",
    "        return [(\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "                (\"pca\", PCA(n_components=nc, svd_solver=\"auto\", random_state=0))]\n",
    "\n",
    "    models = {}\n",
    "\n",
    "    # 1) Logistic Regression  {'C': 1}\n",
    "    models[\"Logistic Regression\"] = Pipeline(block(PCA_MAP[\"Logistic Regression\"]) + [\n",
    "        (\"clf\", LogisticRegression(C=1, solver=\"liblinear\", max_iter=1000, random_state=0))\n",
    "    ])\n",
    "\n",
    "    # 2) KNN  {'n_neighbors': 7, 'weights': 'distance'}\n",
    "    models[\"KNN\"] = Pipeline(block(PCA_MAP[\"KNN\"]) + [\n",
    "        (\"clf\", KNeighborsClassifier(n_neighbors=7, weights=\"distance\"))\n",
    "    ])\n",
    "\n",
    "    # 3) SVM  {'C': 1}\n",
    "    models[\"SVM\"] = Pipeline(block(PCA_MAP[\"SVM\"]) + [\n",
    "        (\"clf\", SVC(C=1, probability=True, random_state=0))\n",
    "    ])\n",
    "\n",
    "    # 4) Decision Tree  {'max_depth': 5, 'min_samples_split': 5}\n",
    "    models[\"Decision Tree\"] = Pipeline(block(PCA_MAP[\"Decision Tree\"]) + [\n",
    "        (\"clf\", DecisionTreeClassifier(max_depth=5, min_samples_split=5, random_state=0))\n",
    "    ])\n",
    "\n",
    "    # 5) Random Forest  {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 300}\n",
    "    models[\"Random Forest\"] = Pipeline(block(PCA_MAP[\"Random Forest\"]) + [\n",
    "        (\"clf\", RandomForestClassifier(\n",
    "            n_estimators=300, max_depth=None, min_samples_split=10, n_jobs=-1, random_state=0\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # 6) Extra Tree  {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 300}\n",
    "    models[\"Extra Tree\"] = Pipeline(block(PCA_MAP[\"Extra Tree\"]) + [\n",
    "        (\"clf\", ExtraTreesClassifier(\n",
    "            n_estimators=300, max_depth=None, min_samples_split=2, n_jobs=-1, random_state=0\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # 7) Gradient Boosting  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300}\n",
    "    models[\"Gradient Boosting\"] = Pipeline(block(PCA_MAP[\"Gradient Boosting\"]) + [\n",
    "        (\"clf\", GradientBoostingClassifier(\n",
    "            learning_rate=0.1, max_depth=5, n_estimators=300, random_state=0\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # 8) XGBoost  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300}\n",
    "    models[\"XGBoost\"] = Pipeline(block(PCA_MAP[\"XGBoost\"]) + [\n",
    "        (\"clf\", XGBClassifier(\n",
    "            n_estimators=300, max_depth=5, learning_rate=0.1,\n",
    "            subsample=1.0, colsample_bytree=1.0,\n",
    "            eval_metric=\"logloss\", use_label_encoder=False,\n",
    "            tree_method=\"hist\", n_jobs=-1, random_state=0\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # 9) LightGBM  {'force_col_wise': True, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'num_leaves': 31}\n",
    "    models[\"LightGBM\"] = Pipeline(block(PCA_MAP[\"LightGBM\"]) + [\n",
    "        (\"clf\", LGBMClassifier(\n",
    "            n_estimators=100, learning_rate=0.1, max_depth=7, num_leaves=31,\n",
    "            force_col_wise=True, random_state=0, n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    return models\n",
    "\n",
    "# ------------------------------\n",
    "# محاسبه میانگین/SD/CI95\n",
    "# ------------------------------\n",
    "def mean_sd_ci(x: np.ndarray, alpha: float = 0.05) -> Tuple[float, float, float, float]:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    n = len(x)\n",
    "    mean = x.mean()\n",
    "    sd = x.std(ddof=1) if n > 1 else 0.0\n",
    "    z = 1.96\n",
    "    se = sd / np.sqrt(max(n, 1))\n",
    "    return mean, sd, mean - z * se, mean + z * se\n",
    "\n",
    "# ------------------------------\n",
    "# موتور ارزیابی (تکرارهای CV با seedهای مختلف)\n",
    "# ------------------------------\n",
    "def evaluate_models_repeated_cv_pca(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = TARGET_COL,\n",
    "    seeds: List[int] = SEEDS,\n",
    "    n_splits: int = N_SPLITS\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    # اگر می‌خواهی همیشه ستون آخر هدف باشد:\n",
    "    # X = df.iloc[:, :-1].values\n",
    "    # y = df.iloc[:, -1].values\n",
    "    X = df.drop(columns=[target_col]).values\n",
    "    y = df[target_col].values\n",
    "\n",
    "    models = make_models_pca()\n",
    "\n",
    "    rows_per_fold = []\n",
    "    summary_rows = []\n",
    "\n",
    "    for model_name, pipe in models.items():\n",
    "        per_fold_scores = {m: [] for m in [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]}\n",
    "\n",
    "        for seed in seeds:\n",
    "            skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "            for fold_idx, (tr, te) in enumerate(skf.split(X, y), start=1):\n",
    "                X_train, X_test = X[tr], X[te]\n",
    "                y_train, y_test = y[tr], y[te]\n",
    "\n",
    "                pipe.fit(X_train, y_train)\n",
    "\n",
    "                # احتمال برای AUC\n",
    "                if hasattr(pipe[-1], \"predict_proba\"):\n",
    "                    y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "                elif hasattr(pipe[-1], \"decision_function\"):\n",
    "                    df_raw = pipe.decision_function(X_test)\n",
    "                    mn, mx = df_raw.min(), df_raw.max()\n",
    "                    y_proba = (df_raw - mn) / (mx - mn) if mx > mn else np.zeros_like(df_raw, dtype=float)\n",
    "                else:\n",
    "                    y_proba = pipe.predict(X_test).astype(float)\n",
    "\n",
    "                y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                prec = precision_score(y_test, y_pred, average=SCORING_AVG, zero_division=0, pos_label=POS_LABEL)\n",
    "                rec = recall_score(y_test, y_pred, average=SCORING_AVG, zero_division=0, pos_label=POS_LABEL)\n",
    "                f1 = f1_score(y_test, y_pred, average=SCORING_AVG, zero_division=0, pos_label=POS_LABEL)\n",
    "                try:\n",
    "                    auc = roc_auc_score(y_test, y_proba)\n",
    "                except ValueError:\n",
    "                    auc = np.nan\n",
    "\n",
    "                per_fold_scores[\"accuracy\"].append(acc)\n",
    "                per_fold_scores[\"precision\"].append(prec)\n",
    "                per_fold_scores[\"recall\"].append(rec)\n",
    "                per_fold_scores[\"f1\"].append(f1)\n",
    "                per_fold_scores[\"roc_auc\"].append(auc)\n",
    "\n",
    "                rows_per_fold.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"seed\": seed,\n",
    "                    \"fold\": fold_idx,\n",
    "                    \"accuracy\": acc,\n",
    "                    \"precision\": prec,\n",
    "                    \"recall\": rec,\n",
    "                    \"f1\": f1,\n",
    "                    \"roc_auc\": auc,\n",
    "                    \"pca_n_components\": PCA_MAP[model_name]\n",
    "                })\n",
    "\n",
    "        for metric in [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]:\n",
    "            arr = np.array(per_fold_scores[metric], dtype=float)\n",
    "            arr = arr[~np.isnan(arr)]\n",
    "            mean, sd, lo, hi = mean_sd_ci(arr) if len(arr) > 0 else (np.nan, np.nan, np.nan, np.nan)\n",
    "            summary_rows.append({\n",
    "                \"model\": model_name,\n",
    "                \"metric\": metric,\n",
    "                \"mean\": mean,\n",
    "                \"std\": sd,\n",
    "                \"ci95_low\": lo,\n",
    "                \"ci95_high\": hi,\n",
    "                \"n_folds\": len(arr),\n",
    "                \"pca_n_components\": PCA_MAP[model_name]\n",
    "            })\n",
    "\n",
    "    per_fold_df = pd.DataFrame(rows_per_fold)\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    return per_fold_df, summary_df\n",
    "\n",
    "# ------------------------------\n",
    "# # مثال اجرا\n",
    "# ------------------------------\n",
    "df = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\6-balanced_data.csv\")\n",
    "\n",
    "per_fold_df, summary_df = evaluate_models_repeated_cv_pca(df, target_col=TARGET_COL)\n",
    "per_fold_df.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\cv_per_fold_pca.csv\", index=False)\n",
    "summary_df.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\cv_summary_pca.csv\", index=False)\n",
    "\n",
    "# بررسی سریع AUCها:\n",
    "print(\n",
    "    summary_df[summary_df[\"metric\"] == \"roc_auc\"]\n",
    "    .sort_values(\"mean\", ascending=False)\n",
    "    .assign(mean_sd=lambda d: d[\"mean\"].round(3).astype(str) + \" ± \" + d[\"std\"].round(3).astype(str),\n",
    "            ci95=lambda d: \"[\" + d[\"ci95_low\"].round(3).astype(str) + \", \" + d[\"ci95_high\"].round(3).astype(str) + \"]\")\n",
    "    [[\"model\",\"pca_n_components\",\"mean_sd\",\"ci95\",\"n_folds\"]]\n",
    "    .to_string(index=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f307b071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "# ===== تنظیمات =====\n",
    "N_SPLITS = 5\n",
    "SEEDS = [0, 42, 101, 202, 303]\n",
    "TARGET_COL = \"fattyliver\"  # یا آخرین ستون با iloc[:, -1]\n",
    "\n",
    "# ===== مدل‌ها با هایپرپارامترهای انتخاب‌شده =====\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(C=1, max_iter=500, solver=\"liblinear\"),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=7, weights=\"distance\"),\n",
    "    \"SVM\": SVC(C=1, probability=True),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5, min_samples_split=5, random_state=0),\n",
    "    \"Random Forest\": RandomForestClassifier(max_depth=None, min_samples_split=10, n_estimators=300, random_state=0),\n",
    "    \"Extra Tree\": ExtraTreesClassifier(max_depth=None, min_samples_split=2, n_estimators=300, random_state=0),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(learning_rate=0.1, max_depth=5, n_estimators=300, random_state=0),\n",
    "    \"XGBoost\": XGBClassifier(learning_rate=0.1, max_depth=5, n_estimators=300, use_label_encoder=False, eval_metric=\"logloss\", random_state=0),\n",
    "    \"LightGBM\": LGBMClassifier(force_col_wise=True, learning_rate=0.1, max_depth=7, n_estimators=100, num_leaves=31, random_state=0),\n",
    "}\n",
    "\n",
    "def evaluate_models_rfecv(df: pd.DataFrame):\n",
    "    X = df.drop(columns=[TARGET_COL]).values\n",
    "    y = df[TARGET_COL].values\n",
    "\n",
    "    results_rows = []\n",
    "    for model_name, model in models.items():\n",
    "        all_metrics = []\n",
    "        for seed in SEEDS:\n",
    "            skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=seed)\n",
    "            fold_metrics = []\n",
    "            for train_idx, test_idx in skf.split(X, y):\n",
    "                X_train, X_test = X[train_idx], X[test_idx]\n",
    "                y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "                pipe = Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"clf\", model)\n",
    "                ])\n",
    "\n",
    "                pipe.fit(X_train, y_train)\n",
    "                y_pred = pipe.predict(X_test)\n",
    "                y_prob = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                fold_metrics.append([\n",
    "                    accuracy_score(y_test, y_pred),\n",
    "                    recall_score(y_test, y_pred),\n",
    "                    precision_score(y_test, y_pred),\n",
    "                    f1_score(y_test, y_pred),\n",
    "                    roc_auc_score(y_test, y_prob),\n",
    "                ])\n",
    "            all_metrics.extend(fold_metrics)\n",
    "\n",
    "        all_metrics = np.array(all_metrics)\n",
    "        results_rows.append({\n",
    "            \"Classifier\": model_name,\n",
    "            # \"Selected_Features_RFECV\": 30,  # چون برای همه ۳۰ ویژگی انتخاب شد\n",
    "            \"Selected_Features_RFECV\": 11,  # چون برای همه ۳۰ ویژگی انتخاب شد with SMOTE\n",
    "            \"Accuracy_mean\": np.mean(all_metrics[:, 0]),\n",
    "            \"Accuracy_std\": np.std(all_metrics[:, 0]),\n",
    "            \"Recall_mean\": np.mean(all_metrics[:, 1]),\n",
    "            \"Recall_std\": np.std(all_metrics[:, 1]),\n",
    "            \"Precision_mean\": np.mean(all_metrics[:, 2]),\n",
    "            \"Precision_std\": np.std(all_metrics[:, 2]),\n",
    "            \"F1_mean\": np.mean(all_metrics[:, 3]),\n",
    "            \"F1_std\": np.std(all_metrics[:, 3]),\n",
    "            \"AUC_mean\": np.mean(all_metrics[:, 4]),\n",
    "            \"AUC_std\": np.std(all_metrics[:, 4]),\n",
    "        })\n",
    "    return pd.DataFrame(results_rows)\n",
    "\n",
    "# ===== اجرا =====\n",
    "df = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\6-balanced_data.csv\")\n",
    "results_rfecv = evaluate_models_rfecv(df)\n",
    "results_rfecv.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoostRFECV_Summary.csv\", index=False)\n",
    "print(results_rfecv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Repeated CV + 95% CI with GA-selected features (per-model subsets) ====\n",
    "# pip install scikit-learn xgboost lightgbm pandas numpy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ------------------------------\n",
    "# تنظیمات کلی\n",
    "# ------------------------------\n",
    "TARGET_COL = \"fattyliver\"   # اگر نمی‌خوای به نام تکیه کنی، پایین گزینه ستون آخر هم هست\n",
    "N_SPLITS = 5\n",
    "SEEDS = [42, 52, 62, 72, 82, 92, 102, 112, 122, 132]   # 10 تکرار\n",
    "POS_LABEL = 1\n",
    "SCORING_AVG = \"binary\"\n",
    "\n",
    "# ------------------------------\n",
    "# نگاشت GA: ایندکس‌های ویژگی منتخب برای هر مدل (طبق داده‌ای که دادی)\n",
    "# ایندکس‌ها نسبت به «X = df.drop(target)» هستن (از 0 شروع).\n",
    "# ------------------------------\n",
    "GA_MAP: Dict[str, List[int]] = {\n",
    "    \"LightGBM\":            [0, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 17, 20, 24, 25, 26, 29],\n",
    "    \"Gradient Boosting\":   [0, 1, 4, 7, 8, 11, 12, 16, 17, 18, 20, 22, 24, 25],\n",
    "    \"Logistic Regression\": [0, 1, 2, 6, 7, 9, 10, 11, 12, 13, 16, 17, 18, 19, 24, 25, 29],\n",
    "    \"Extra Tree\":          [0, 1, 7, 11, 12, 14, 15, 16, 24, 27, 30],\n",
    "    \"Decision Tree\":       [0, 1, 9, 12, 16, 17, 21, 22, 23, 24, 25, 26, 27, 28, 30],\n",
    "    \"XGBoost\":             [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 16, 17, 19, 20, 21, 24, 25, 26, 27, 28, 29, 30],\n",
    "    \"KNN\":                 [4, 7, 9, 11, 12, 24, 25, 28, 30],\n",
    "    \"SVM\" : [0, 1, 2, 3, 4, 7, 8, 10, 11, 16, 17, 20, 23, 24, 28, 29, 30]\n",
    "}\n",
    "\n",
    "# نام‌ها را با کلیدهای مدل‌ها هماهنگ می‌کنیم\n",
    "MODEL_KEYS = [\n",
    "    \"Logistic Regression\", \"KNN\", \"SVM\", \"Decision Tree\",\n",
    "    \"Random Forest\", \"Extra Tree\", \"Gradient Boosting\", \"XGBoost\", \"LightGBM\"\n",
    "]\n",
    "\n",
    "# ------------------------------\n",
    "# سازنده‌ی مدل‌ها (با n_jobs=-1 و SVM سریع)\n",
    "# ------------------------------\n",
    "def build_models() -> Dict[str, object]:\n",
    "    return {\n",
    "        \"Logistic Regression\": LogisticRegression(C=1, solver=\"liblinear\", max_iter=1000, random_state=0),\n",
    "        \"KNN\": KNeighborsClassifier(n_neighbors=7, weights=\"distance\", n_jobs=-1),\n",
    "        \"SVM\": SVC(C=1, probability=False, random_state=0),  # AUC از decision_function\n",
    "        \"Decision Tree\": DecisionTreeClassifier(max_depth=5, min_samples_split=5, random_state=0),\n",
    "        \"Random Forest\": RandomForestClassifier(\n",
    "            n_estimators=300, max_depth=None, min_samples_split=10, n_jobs=-1, random_state=0\n",
    "        ),\n",
    "        \"Extra Tree\": ExtraTreesClassifier(\n",
    "            n_estimators=300, max_depth=None, min_samples_split=2, n_jobs=-1, random_state=0\n",
    "        ),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "            learning_rate=0.1, max_depth=5, n_estimators=300, random_state=0\n",
    "        ),\n",
    "        \"XGBoost\": XGBClassifier(\n",
    "            n_estimators=300, max_depth=5, learning_rate=0.1,\n",
    "            subsample=1.0, colsample_bytree=1.0,\n",
    "            eval_metric=\"logloss\", use_label_encoder=False,\n",
    "            tree_method=\"hist\", n_jobs=-1, random_state=0\n",
    "        ),\n",
    "        \"LightGBM\": LGBMClassifier(\n",
    "            n_estimators=100, learning_rate=0.1, max_depth=7, num_leaves=31,\n",
    "            force_col_wise=True, n_jobs=-1, random_state=0\n",
    "        ),\n",
    "    }\n",
    "\n",
    "# ------------------------------\n",
    "# کمک‌تابع CI\n",
    "# ------------------------------\n",
    "def mean_sd_ci(x: np.ndarray) -> Tuple[float, float, float, float]:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x[~np.isnan(x)]\n",
    "    n = len(x)\n",
    "    if n == 0:\n",
    "        return (np.nan, np.nan, np.nan, np.nan)\n",
    "    mean = x.mean()\n",
    "    sd = x.std(ddof=1) if n > 1 else 0.0\n",
    "    z = 1.96\n",
    "    se = sd / np.sqrt(max(n, 1))\n",
    "    return mean, sd, mean - z * se, mean + z * se\n",
    "\n",
    "# ------------------------------\n",
    "# ارزیابی با GA feature subsets\n",
    "# ------------------------------\n",
    "def evaluate_models_repeated_cv_ga(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = TARGET_COL,\n",
    "    seeds: List[int] = SEEDS,\n",
    "    n_splits: int = N_SPLITS\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    # اگر ترجیح می‌دی همیشه ستون آخر هدف باشه:\n",
    "    # feat_df = df.iloc[:, :-1].copy(); y = df.iloc[:, -1].to_numpy()\n",
    "    feat_df = df.drop(columns=[target_col]).copy()\n",
    "    y = df[target_col].to_numpy()\n",
    "    # (اختیاری) به float32 برای سرعت/مموری کمتر:\n",
    "    for c in feat_df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(feat_df[c]):\n",
    "            feat_df[c] = feat_df[c].astype(np.float32)\n",
    "\n",
    "    models = build_models()\n",
    "    all_rows, summary_rows = [], []\n",
    "\n",
    "    n_features_total = feat_df.shape[1]\n",
    "    feature_indices_all = list(range(n_features_total))\n",
    "\n",
    "    for model_name in MODEL_KEYS:\n",
    "        model = models[model_name]\n",
    "        # ایندکس‌های GA برای این مدل (اگر نداشتی، همه‌ی فیچرها)\n",
    "        ga_idx = GA_MAP.get(model_name, feature_indices_all)\n",
    "\n",
    "        # انتخاب‌گر ستونی بر اساس ایندکس‌ها (داخل هر فولد، بدون لیکیج)\n",
    "        selector = ColumnTransformer(\n",
    "            transformers=[(\"sel\", \"passthrough\", ga_idx)],\n",
    "            remainder=\"drop\", verbose_feature_names_out=False\n",
    "        )\n",
    "\n",
    "        # آیا نیاز به اسکیلر داریم؟\n",
    "        needs_scaler = model_name in {\"Logistic Regression\", \"KNN\", \"SVM\"}\n",
    "        steps = [(\"select\", selector)]\n",
    "        if needs_scaler:\n",
    "            steps.append((\"scaler\", StandardScaler(with_mean=True, with_std=True)))\n",
    "\n",
    "        steps.append((\"clf\", model))\n",
    "        pipe = Pipeline(steps=steps)\n",
    "\n",
    "        per_fold = {m: [] for m in [\"accuracy\",\"precision\",\"recall\",\"f1\",\"roc_auc\"]}\n",
    "\n",
    "        for seed in seeds:\n",
    "            skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "            for fold_idx, (tr, te) in enumerate(skf.split(feat_df, y), start=1):\n",
    "                X_train = feat_df.iloc[tr]\n",
    "                X_test  = feat_df.iloc[te]\n",
    "                y_train, y_test = y[tr], y[te]\n",
    "\n",
    "                pipe.fit(X_train, y_train)\n",
    "\n",
    "                # امتیاز برای AUC\n",
    "                clf = pipe[-1]\n",
    "                if hasattr(clf, \"predict_proba\"):\n",
    "                    y_score = pipe.predict_proba(X_test)[:, 1]\n",
    "                elif hasattr(clf, \"decision_function\"):\n",
    "                    y_score = pipe.decision_function(X_test)\n",
    "                else:\n",
    "                    y_score = pipe.predict(X_test).astype(float)\n",
    "\n",
    "                y_pred = (y_score >= 0.5).astype(int)\n",
    "\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                prec = precision_score(y_test, y_pred, average=SCORING_AVG, zero_division=0, pos_label=POS_LABEL)\n",
    "                rec = recall_score(y_test, y_pred, average=SCORING_AVG, zero_division=0, pos_label=POS_LABEL)\n",
    "                f1 = f1_score(y_test, y_pred, average=SCORING_AVG, zero_division=0, pos_label=POS_LABEL)\n",
    "                try:\n",
    "                    auc = roc_auc_score(y_test, y_score)\n",
    "                except ValueError:\n",
    "                    auc = np.nan\n",
    "\n",
    "                per_fold[\"accuracy\"].append(acc)\n",
    "                per_fold[\"precision\"].append(prec)\n",
    "                per_fold[\"recall\"].append(rec)\n",
    "                per_fold[\"f1\"].append(f1)\n",
    "                per_fold[\"roc_auc\"].append(auc)\n",
    "\n",
    "                all_rows.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"seed\": seed,\n",
    "                    \"fold\": fold_idx,\n",
    "                    \"accuracy\": acc,\n",
    "                    \"precision\": prec,\n",
    "                    \"recall\": rec,\n",
    "                    \"f1\": f1,\n",
    "                    \"roc_auc\": auc,\n",
    "                    \"ga_n_features\": len(ga_idx),\n",
    "                    \"ga_indices\": ga_idx\n",
    "                })\n",
    "\n",
    "        # خلاصه با CI95\n",
    "        for metric in [\"accuracy\",\"precision\",\"recall\",\"f1\",\"roc_auc\"]:\n",
    "            mean, sd, lo, hi = mean_sd_ci(np.array(per_fold[metric], dtype=float))\n",
    "            summary_rows.append({\n",
    "                \"model\": model_name,\n",
    "                \"metric\": metric,\n",
    "                \"mean\": mean,\n",
    "                \"std\": sd,\n",
    "                \"ci95_low\": lo,\n",
    "                \"ci95_high\": hi,\n",
    "                \"n_folds\": len(per_fold[metric]),\n",
    "                \"ga_n_features\": len(ga_idx),\n",
    "                \"ga_indices\": ga_idx\n",
    "            })\n",
    "\n",
    "    per_fold_df = pd.DataFrame(all_rows)\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    return per_fold_df, summary_df\n",
    "\n",
    "# ------------------------------\n",
    "# مثال اجرا\n",
    "# ------------------------------\n",
    "# df = pd.read_csv(\"your_clean_data.csv\")\n",
    "# اگر برچسبت رشته‌ای است، به 0/1 نگاشت کن:\n",
    "# df[\"fatty liver\"] = df[\"fatty liver\"].map({\"Non-MAFLD\":0, \"MAFLD\":1}).astype(int)\n",
    "\n",
    "# per_fold_df, summary_df = evaluate_models_repeated_cv_ga(df, target_col=TARGET_COL)\n",
    "# per_fold_df.to_csv(\"cv_per_fold_ga.csv\", index=False)\n",
    "# summary_df.to_csv(\"cv_summary_ga.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # مثال اجرا\n",
    "# ------------------------------\n",
    "df = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\6-balanced_data.csv\")\n",
    "\n",
    "per_fold_df, summary_df = evaluate_models_repeated_cv_ga(df, target_col=TARGET_COL)\n",
    "per_fold_df.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\cv_per_fold_Ga.csv\", index=False)\n",
    "summary_df.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\cv_summary_Ga.csv\", index=False)\n",
    "\n",
    "# # بررسی سریع AUCها:\n",
    "# print(\n",
    "#     summary_df[summary_df[\"metric\"] == \"roc_auc\"]\n",
    "#     .sort_values(\"mean\", ascending=False)\n",
    "#     .assign(mean_sd=lambda d: d[\"mean\"].round(3).astype(str) + \" ± \" + d[\"std\"].round(3).astype(str),\n",
    "#             ci95=lambda d: \"[\" + d[\"ci95_low\"].round(3).astype(str) + \", \" + d[\"ci95_high\"].round(3).astype(str) + \"]\")\n",
    "#     [[\"model\",\"pca_n_components\",\"mean_sd\",\"ci95\",\"n_folds\"]]\n",
    "#     .to_string(index=False)\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# نمایش سریع AUCها\n",
    "print(\n",
    "     summary_df[summary_df[\"metric\"]==\"roc_auc\"]\n",
    "     .sort_values(\"mean\", ascending=False)\n",
    "     .assign(mean_sd=lambda d: d[\"mean\"].round(3).astype(str)+\" ± \"+d[\"std\"].round(3).astype(str),\n",
    "             ci95=lambda d: \"[\"+d[\"ci95_low\"].round(3).astype(str)+\", \"+d[\"ci95_high\"].round(3).astype(str)+\"]\")\n",
    "     [[\"model\",\"ga_n_features\",\"mean_sd\",\"ci95\",\"n_folds\"]]\n",
    "     .to_string(index=False)\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d3a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ---------- تنظیمات ----------\n",
    "TARGET_COL = \"fattyliver\"   # یا نام ستون هدفت\n",
    "N_SPLITS = 5\n",
    "SEEDS: List[int] = [0, 42, 101, 202, 303]  # می‌تونی مثل سایر اسکریپت‌ها 10 تایی هم بذاری\n",
    "POS_LABEL = 1\n",
    "\n",
    "# ---------- مدل‌ها (هایپرپارامتر مثل قبل + سرعت بهتر) ----------\n",
    "def make_models() -> Dict[str, object]:\n",
    "    return {\n",
    "        \"Logistic Regression\": LogisticRegression(C=1, solver=\"liblinear\", max_iter=1000, random_state=0),\n",
    "        \"KNN\": KNeighborsClassifier(n_neighbors=7, weights=\"distance\", n_jobs=-1),\n",
    "        \"SVM\": SVC(C=1, probability=False, random_state=0),  # AUC از decision_function\n",
    "        \"Decision Tree\": DecisionTreeClassifier(max_depth=5, min_samples_split=5, random_state=0),\n",
    "        \"Random Forest\": RandomForestClassifier(\n",
    "            n_estimators=300, max_depth=None, min_samples_split=10, n_jobs=-1, random_state=0\n",
    "        ),\n",
    "        \"Extra Tree\": ExtraTreesClassifier(\n",
    "            n_estimators=300, max_depth=None, min_samples_split=2, n_jobs=-1, random_state=0\n",
    "        ),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "            learning_rate=0.1, max_depth=5, n_estimators=300, random_state=0\n",
    "        ),\n",
    "        \"XGBoost\": XGBClassifier(\n",
    "            n_estimators=300, max_depth=5, learning_rate=0.1,\n",
    "            subsample=1.0, colsample_bytree=1.0,\n",
    "            eval_metric=\"logloss\", use_label_encoder=False,\n",
    "            tree_method=\"hist\", n_jobs=-1, random_state=0, verbosity=0\n",
    "        ),\n",
    "        \"LightGBM\": LGBMClassifier(\n",
    "            n_estimators=100, learning_rate=0.1, max_depth=7, num_leaves=31,\n",
    "            force_col_wise=True, n_jobs=-1, random_state=0, verbose=-1\n",
    "        ),\n",
    "    }\n",
    "\n",
    "# ---------- کمکی: میانگین/انحراف معیار/CI95 ----------\n",
    "def mean_sd_ci(x: np.ndarray) -> Tuple[float, float, float, float]:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x[~np.isnan(x)]\n",
    "    n = len(x)\n",
    "    if n == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    mean = x.mean()\n",
    "    sd = x.std(ddof=1) if n > 1 else 0.0\n",
    "    z = 1.96\n",
    "    se = sd / np.sqrt(n)\n",
    "    return mean, sd, mean - z*se, mean + z*se\n",
    "\n",
    "# ---------- ارزیابی RFECV (ثابت: 30 فیچر انتخاب‌شده) ----------\n",
    "def evaluate_models_repeated_cv_rfecv(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = TARGET_COL,\n",
    "    seeds: List[int] = SEEDS,\n",
    "    n_splits: int = N_SPLITS\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # اگر می‌خوای همیشه ستون آخر هدف باشه:\n",
    "    # Xall = df.iloc[:, :-1].copy(); y = df.iloc[:, -1].to_numpy()\n",
    "    Xall = df.drop(columns=[target_col]).copy()\n",
    "    y = df[target_col].to_numpy()\n",
    "\n",
    "    # بهینه‌سازی سبک\n",
    "    for c in Xall.columns:\n",
    "        if pd.api.types.is_numeric_dtype(Xall[c]):\n",
    "            Xall[c] = Xall[c].astype(np.float32)\n",
    "\n",
    "    models = make_models()\n",
    "\n",
    "    per_fold_rows = []\n",
    "    summary_rows = []\n",
    "    SELECTED_COUNT = 30  # طبق نتیجه RFECV شما\n",
    "\n",
    "    for model_name, clf in models.items():\n",
    "        # اسکیل برای مدل‌های حساس\n",
    "        needs_scaler = model_name in {\"Logistic Regression\", \"KNN\", \"SVM\"}\n",
    "        steps = []\n",
    "        if needs_scaler:\n",
    "            steps.append((\"scaler\", StandardScaler()))\n",
    "        steps.append((\"clf\", clf))\n",
    "        pipe = Pipeline(steps=steps)\n",
    "\n",
    "        scores_collect = {m: [] for m in [\"accuracy\",\"precision\",\"recall\",\"f1\",\"roc_auc\"]}\n",
    "\n",
    "        for seed in seeds:\n",
    "            skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "            for fold_idx, (tr, te) in enumerate(skf.split(Xall, y), start=1):\n",
    "                X_train, X_test = Xall.iloc[tr].values, Xall.iloc[te].values\n",
    "                y_train, y_test = y[tr], y[te]\n",
    "\n",
    "                pipe.fit(X_train, y_train)\n",
    "\n",
    "                # امتیاز/نمره برای AUC و برچسب\n",
    "                clf_final = pipe[-1]\n",
    "                if hasattr(clf_final, \"predict_proba\"):\n",
    "                    y_score = pipe.predict_proba(X_test)[:, 1]\n",
    "                    y_pred = (y_score >= 0.5).astype(int)\n",
    "                elif hasattr(clf_final, \"decision_function\"):\n",
    "                    y_score = pipe.decision_function(X_test)\n",
    "                    y_pred = (y_score >= 0).astype(int)  # آستانه صفر برای decision_function\n",
    "                else:\n",
    "                    y_score = pipe.predict(X_test).astype(float)\n",
    "                    y_pred = (y_score >= 0.5).astype(int)\n",
    "\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                prec = precision_score(y_test, y_pred, average=\"binary\", zero_division=0, pos_label=POS_LABEL)\n",
    "                rec = recall_score(y_test, y_pred, average=\"binary\",  zero_division=0, pos_label=POS_LABEL)\n",
    "                f1 = f1_score(y_test, y_pred, average=\"binary\", zero_division=0, pos_label=POS_LABEL)\n",
    "                try:\n",
    "                    auc = roc_auc_score(y_test, y_score)\n",
    "                except ValueError:\n",
    "                    auc = np.nan\n",
    "\n",
    "                scores_collect[\"accuracy\"].append(acc)\n",
    "                scores_collect[\"precision\"].append(prec)\n",
    "                scores_collect[\"recall\"].append(rec)\n",
    "                scores_collect[\"f1\"].append(f1)\n",
    "                scores_collect[\"roc_auc\"].append(auc)\n",
    "\n",
    "                per_fold_rows.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"seed\": seed,\n",
    "                    \"fold\": fold_idx,\n",
    "                    \"accuracy\": acc,\n",
    "                    \"precision\": prec,\n",
    "                    \"recall\": rec,\n",
    "                    \"f1\": f1,\n",
    "                    \"roc_auc\": auc,\n",
    "                    \"kbest_k\": SELECTED_COUNT  # برای سازگاری نام ستون\n",
    "                })\n",
    "\n",
    "        # خلاصه به قالب واحد\n",
    "        for metric in [\"accuracy\",\"precision\",\"recall\",\"f1\",\"roc_auc\"]:\n",
    "            mean, sd, lo, hi = mean_sd_ci(np.array(scores_collect[metric], dtype=float))\n",
    "            summary_rows.append({\n",
    "                \"model\": model_name,\n",
    "                \"metric\": metric,\n",
    "                \"mean\": mean,\n",
    "                \"std\": sd,\n",
    "                \"ci95_low\": lo,\n",
    "                \"ci95_high\": hi,\n",
    "                \"n_folds\": len(scores_collect[metric]),\n",
    "                \"kbest_k\": SELECTED_COUNT  # برای سازگاری نام ستون\n",
    "            })\n",
    "\n",
    "    per_fold_df = pd.DataFrame(per_fold_rows)\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    return per_fold_df, summary_df\n",
    "\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\6-balanced_data.csv\")\n",
    "per_fold_df, summary_df = evaluate_models_repeated_cv_rfecv(df, target_col=TARGET_COL)\n",
    "summary_df.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\RFECV2_Summary.csv\", index=False)\n",
    "per_fold_df.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\per fold 2RFECV_Summary.csv\", index=False)\n",
    "print(summary_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e3789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# آدرس فایل\n",
    "file_path = r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\datasets\\Data\\1-my_null_data.csv\"\n",
    "\n",
    "# خوندن فایل CSV\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# محاسبه درصد null ها برای هر ستون و مرتب‌سازی نزولی\n",
    "null_percent = df.isnull().mean() * 100\n",
    "null_percent_sorted = null_percent.sort_values(ascending=False)\n",
    "\n",
    "print(null_percent_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c828639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# مثال: یک DataFrame نمونه\n",
    "df = pd.read_csv(r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\datasets\\Data\\1-my_null_data.csv\")\n",
    "# 1) درصد نال به ازای هر ستون (مرتب شده، با فرمت درصد)\n",
    "def percent_missing_by_column(df, sort_desc=True, format_percent=True):\n",
    "    s = df.isnull().mean() * 100  # mean() روی boolean -> نسبت Trueها\n",
    "    if sort_desc:\n",
    "        s = s.sort_values(ascending=False)\n",
    "    if format_percent:\n",
    "        return s.map(lambda x: f\"{x:.2f}%\")\n",
    "    return s\n",
    "\n",
    "# 2) درصد نال کلی (در کل دیتافریم)\n",
    "def percent_missing_overall(df):\n",
    "    total_cells = df.size\n",
    "    total_missing = df.isnull().sum().sum()\n",
    "    return (total_missing / total_cells) * 100\n",
    "\n",
    "# 3) درصد نال هر ردیف (می‌تونی آستانه برای فیلتر گذاری استفاده کنی)\n",
    "def percent_missing_by_row(df, format_percent=True):\n",
    "    s = df.isnull().mean(axis=1) * 100\n",
    "    if format_percent:\n",
    "        return s.map(lambda x: f\"{x:.2f}%\")\n",
    "    return s\n",
    "\n",
    "# 4) خلاصه‌ی کامل (ستون‌ها + کلی)\n",
    "def missing_summary(df):\n",
    "    col_pct = df.isnull().mean() * 100\n",
    "    col_count = df.isnull().sum()\n",
    "    overall_pct = percent_missing_overall(df)\n",
    "    summary = pd.DataFrame({\n",
    "        'missing_count': col_count,\n",
    "        'missing_pct': col_pct.map(lambda x: round(x, 2))\n",
    "    }).sort_values('missing_pct', ascending=False)\n",
    "    return summary, round(overall_pct, 2)\n",
    "\n",
    "# استفاده و نمایش\n",
    "print(\"درصد نال هر ستون:\")\n",
    "print(percent_missing_by_column(df))\n",
    "\n",
    "print(\"\\nدرصد نال کلی دیتافریم:\")\n",
    "print(f\"{percent_missing_overall(df):.2f}%\")\n",
    "\n",
    "print(\"\\nدرصد نال هر ردیف:\")\n",
    "print(percent_missing_by_row(df))\n",
    "\n",
    "print(\"\\nخلاصه کامل:\")\n",
    "summary_df, overall = missing_summary(df)\n",
    "print(summary_df)\n",
    "print(f\"\\nدرصد نال کلی: {overall:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e122d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# --------------------------\n",
    "# Helpers\n",
    "# --------------------------\n",
    "def _find_col(df: pd.DataFrame, name: str) -> str:\n",
    "    \"\"\"Find a column by name case-insensitively; returns actual column name or raises.\"\"\"\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    if name.lower() in lower_map:\n",
    "        return lower_map[name.lower()]\n",
    "    # Try common variants\n",
    "    variants = [name.replace(\"_\",\"\"), name.replace(\" \",\"\"), name.upper(), name.lower(), name.capitalize()]\n",
    "    for v in variants:\n",
    "        if v.lower() in lower_map:\n",
    "            return lower_map[v.lower()]\n",
    "    raise KeyError(f\"Column '{name}' not found (case-insensitive). Available: {list(df.columns)[:10]} ...\")\n",
    "\n",
    "def _row_missing_pct(df: pd.DataFrame) -> pd.Series:\n",
    "    return df.isna().mean(axis=1)\n",
    "\n",
    "def _col_missing_pct(df: pd.DataFrame) -> pd.Series:\n",
    "    return df.isna().mean(axis=0)\n",
    "\n",
    "def _coerce_binary(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Coerce a likely-binary series to {0,1} where possible.\"\"\"\n",
    "    s = series.copy()\n",
    "    # Map common string values\n",
    "    mapping = {\n",
    "        'yes':1, 'no':0, 'y':1, 'n':0, 'true':1, 'false':0, 'male':1, 'female':0,\n",
    "        'm':1, 'f':0\n",
    "    }\n",
    "    if s.dtype == object:\n",
    "        s = s.str.strip().str.lower().map(mapping).astype('float64')\n",
    "    # If still not numeric, try casting\n",
    "    if not np.issubdtype(s.dtype, np.number):\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "    # Clip to 0/1 if takes only two unique numeric values\n",
    "    uniq = pd.unique(s.dropna())\n",
    "    if len(uniq) == 2:\n",
    "        # Map min->0, max->1\n",
    "        mn, mx = float(np.nanmin(uniq)), float(np.nanmax(uniq))\n",
    "        s = s.map({mn:0.0, mx:1.0})\n",
    "    return s\n",
    "\n",
    "def _summarize_cohort(df: pd.DataFrame,\n",
    "                      key_cont_cols: List[str],\n",
    "                      target_col: str,\n",
    "                      sex_col: str = None) -> Dict:\n",
    "    \"\"\"Return basic cohort characteristics.\"\"\"\n",
    "    out = {}\n",
    "    out['n_rows'] = int(df.shape[0])\n",
    "    out['n_cols'] = int(df.shape[1])\n",
    "    out['overall_missing_pct'] = float(df.isna().mean().mean()*100)\n",
    "\n",
    "    # Continuous summaries\n",
    "    cont_summary = {}\n",
    "    for col in key_cont_cols:\n",
    "        if col not in df.columns:  # skip if missing\n",
    "            continue\n",
    "        s = pd.to_numeric(df[col], errors='coerce')\n",
    "        cont_summary[col] = {\n",
    "            'mean': float(np.nanmean(s)),\n",
    "            'median': float(np.nanmedian(s)),\n",
    "            'missing_pct': float(s.isna().mean()*100)\n",
    "        }\n",
    "    out['continuous'] = cont_summary\n",
    "\n",
    "    # Target prevalence\n",
    "    tgt = _coerce_binary(df[target_col])\n",
    "    out['mafld_prevalence_pct'] = float(np.nanmean(tgt)*100)\n",
    "\n",
    "    # Sex proportion (optional)\n",
    "    if sex_col and sex_col in df.columns:\n",
    "        sex = _coerce_binary(df[sex_col])\n",
    "        out['sex_prop_male_pct'] = float(np.nanmean(sex)*100)\n",
    "\n",
    "    return out\n",
    "\n",
    "def _smd(x: pd.Series, y: pd.Series) -> float:\n",
    "    \"\"\"Standardized mean difference for continuous variables (Hedges not applied).\"\"\"\n",
    "    x = pd.to_numeric(x, errors='coerce')\n",
    "    y = pd.to_numeric(y, errors='coerce')\n",
    "    mx, my = np.nanmean(x), np.nanmean(y)\n",
    "    sx, sy = np.nanstd(x, ddof=1), np.nanstd(y, ddof=1)\n",
    "    # Pooled SD\n",
    "    sp = np.sqrt(((sx**2) + (sy**2))/2.0)\n",
    "    if sp == 0 or np.isnan(sp):\n",
    "        return np.nan\n",
    "    return float((mx - my)/sp)\n",
    "\n",
    "# --------------------------\n",
    "# Main cleaning & compare\n",
    "# --------------------------\n",
    "def clean_with_thresholds(df: pd.DataFrame,\n",
    "                          feature_missing_thresh: float,\n",
    "                          row_missing_thresh: float) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Remove columns with missing_pct > feature_missing_thresh\n",
    "    and rows with missing_pct > row_missing_thresh. Returns cleaned df and a small log.\n",
    "    Thresholds are in fractions (e.g., 0.45 means 45%).\n",
    "    \"\"\"\n",
    "    col_miss = _col_missing_pct(df)\n",
    "    keep_cols = col_miss[col_miss <= feature_missing_thresh].index.tolist()\n",
    "    removed_cols = col_miss[col_miss > feature_missing_thresh].sort_values(ascending=False)\n",
    "\n",
    "    df2 = df[keep_cols].copy()\n",
    "    row_miss = _row_missing_pct(df2)\n",
    "    keep_rows = row_miss[row_miss <= row_missing_thresh].index\n",
    "    removed_rows = row_miss[row_miss > row_missing_thresh].sort_values(ascending=False)\n",
    "\n",
    "    df_clean = df2.loc[keep_rows].copy()\n",
    "\n",
    "    log = {\n",
    "        'removed_cols': removed_cols.to_dict(),\n",
    "        'removed_rows_count': int(removed_rows.shape[0]),\n",
    "        'kept_rows_count': int(df_clean.shape[0]),\n",
    "        'kept_cols_count': int(df_clean.shape[1]),\n",
    "    }\n",
    "    return df_clean, log\n",
    "\n",
    "def compare_thresholds(df: pd.DataFrame,\n",
    "                       thresholds: List[Tuple[float, float]] = [(0.30, 0.50),\n",
    "                                                                (0.45, 0.50),\n",
    "                                                                (0.60, 0.50)],\n",
    "                       key_cont_candidates: List[str] = ['age','BMI','ALT','AST','PLT','CRP'],\n",
    "                       target_name: str = 'fattyliver',\n",
    "                       sex_name: str = 'sex') -> Dict:\n",
    "    \"\"\"\n",
    "    Run cleaning for multiple thresholds and summarize cohorts.\n",
    "    Returns dict with:\n",
    "      - 'scenarios': per-threshold summaries\n",
    "      - 'smd_vs_045': SMDs of key continuous vars vs the 0.45/0.50 scenario\n",
    "    \"\"\"\n",
    "    # Resolve essential columns case-insensitively\n",
    "    target_col = _find_col(df, target_name)\n",
    "    sex_col = None\n",
    "    try:\n",
    "        sex_col = _find_col(df, sex_name)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Resolve the set of key continuous columns that actually exist\n",
    "    resolved_keys = []\n",
    "    for nm in key_cont_candidates:\n",
    "        try:\n",
    "            resolved_keys.append(_find_col(df, nm))\n",
    "        except KeyError:\n",
    "            pass\n",
    "    # De-duplicate while preserving order\n",
    "    key_cont_cols = list(dict.fromkeys(resolved_keys))\n",
    "\n",
    "    scenarios = {}\n",
    "    cleaned_dfs = {}\n",
    "\n",
    "    for f_thr, r_thr in thresholds:\n",
    "        label = f\"feat_{int(f_thr*100)}_row_{int(r_thr*100)}\"\n",
    "        df_clean, log = clean_with_thresholds(df, f_thr, r_thr)\n",
    "        summ = _summarize_cohort(df_clean, key_cont_cols, target_col, sex_col)\n",
    "        summ['thresholds'] = {'feature_missing': f_thr, 'row_missing': r_thr}\n",
    "        summ['removed_cols_top5'] = dict(sorted(log['removed_cols'].items(),\n",
    "                                                key=lambda x: x[1],\n",
    "                                                reverse=True)[:5])\n",
    "        summ['removed_rows_count'] = log['removed_rows_count']\n",
    "        scenarios[label] = summ\n",
    "        cleaned_dfs[label] = df_clean\n",
    "\n",
    "    # Compute SMDs for key continuous variables vs 0.45/0.50 as baseline (if available)\n",
    "    base_key = \"feat_45_row_50\"\n",
    "    smd_table = {}\n",
    "    if base_key in cleaned_dfs:\n",
    "        base = cleaned_dfs[base_key]\n",
    "        for label, dfi in cleaned_dfs.items():\n",
    "            if label == base_key:\n",
    "                continue\n",
    "            smds = {}\n",
    "            for col in key_cont_cols:\n",
    "                if col in base.columns and col in dfi.columns:\n",
    "                    smds[col] = _smd(base[col], dfi[col])\n",
    "            smd_table[label] = smds\n",
    "\n",
    "    return {'scenarios': scenarios, 'smd_vs_045': smd_table}\n",
    "\n",
    "# --------------------------\n",
    "# Example usage\n",
    "# --------------------------\n",
    "df = pd.read_csv(r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\datasets\\Data\\0-Original Data.csv\")\n",
    "# اجرای مقایسه با thresholdهای مختلف\n",
    "result = compare_thresholds(df)\n",
    "\n",
    "# خلاصه ویژگی‌های هر سناریو (چند متغیر مهم و تعداد ردیف/ستون و شیوع MAFLD)\n",
    "rows = []\n",
    "for label, summ in result['scenarios'].items():\n",
    "    rows.append({\n",
    "        'scenario': label,\n",
    "        'n_rows': summ['n_rows'],\n",
    "        'n_cols': summ['n_cols'],\n",
    "        'overall_missing_%': round(summ['overall_missing_pct'], 2),\n",
    "        'MAFLD_prev_%': round(summ['mafld_prevalence_pct'], 2),\n",
    "        'Age_mean': round(summ['continuous'].get('age',{}).get('mean', np.nan), 2),\n",
    "        'BMI_mean': round(summ['continuous'].get('BMI',{}).get('mean', np.nan), 2),\n",
    "        'ALT_mean': round(summ['continuous'].get('alt',{}).get('mean', np.nan), 2),\n",
    "        'AST_mean': round(summ['continuous'].get('ast',{}).get('mean', np.nan), 2),\n",
    "        'PLT_mean': round(summ['continuous'].get('PLT',{}).get('mean', np.nan), 2),\n",
    "        'CRP_mean': round(summ['continuous'].get('CRP',{}).get('mean', np.nan), 2),\n",
    "    })\n",
    "summary_df = pd.DataFrame(rows)\n",
    "print(summary_df)\n",
    "\n",
    "# تفاوت‌ها (SMD) نسبت به سناریوی اصلی (45%/50%)\n",
    "smd_df = pd.DataFrame(result['smd_vs_045']).T\n",
    "print(\"\\nSMDs vs 45%/50% baseline:\")\n",
    "print(smd_df.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef84481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# --------------------------\n",
    "# Helpers\n",
    "# --------------------------\n",
    "def _find_col(df: pd.DataFrame, name: str) -> str:\n",
    "    \"\"\"Find a column by name case-insensitively; returns actual column name or raises.\"\"\"\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    if name.lower() in lower_map:\n",
    "        return lower_map[name.lower()]\n",
    "    # Try a few variants\n",
    "    variants = [name.replace(\"_\",\"\"), name.replace(\" \",\"\"), name.upper(), name.lower(), name.capitalize()]\n",
    "    for v in variants:\n",
    "        if v.lower() in lower_map:\n",
    "            return lower_map[v.lower()]\n",
    "    raise KeyError(f\"Column '{name}' not found (case-insensitive). Available: {list(df.columns)[:10]} ...\")\n",
    "\n",
    "def _row_missing_pct(df: pd.DataFrame) -> pd.Series:\n",
    "    return df.isna().mean(axis=1)\n",
    "\n",
    "def _col_missing_pct(df: pd.DataFrame) -> pd.Series:\n",
    "    return df.isna().mean(axis=0)\n",
    "\n",
    "def _coerce_binary(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Coerce a likely-binary series to {0,1} where possible.\"\"\"\n",
    "    s = series.copy()\n",
    "    mapping = {'yes':1, 'no':0, 'y':1, 'n':0, 'true':1, 'false':0, 'male':1, 'female':0, 'm':1, 'f':0}\n",
    "    if s.dtype == object:\n",
    "        s = s.astype(str).str.strip().str.lower().map(mapping).astype('float64')\n",
    "    if not np.issubdtype(s.dtype, np.number):\n",
    "        s = pd.to_numeric(s, errors='coerce')\n",
    "    uniq = pd.unique(s.dropna())\n",
    "    if len(uniq) == 2:\n",
    "        mn, mx = float(np.nanmin(uniq)), float(np.nanmax(uniq))\n",
    "        s = s.map({mn:0.0, mx:1.0})\n",
    "    return s\n",
    "\n",
    "def _summarize_cohort(df: pd.DataFrame,\n",
    "                      key_cont_cols: List[str],\n",
    "                      target_col: str,\n",
    "                      sex_col: str = None) -> Dict:\n",
    "    \"\"\"Return basic cohort characteristics.\"\"\"\n",
    "    out = {}\n",
    "    out['n_rows'] = int(df.shape[0])\n",
    "    out['n_cols'] = int(df.shape[1])\n",
    "    out['overall_missing_pct'] = float(df.isna().mean().mean()*100)\n",
    "\n",
    "    # Continuous summaries\n",
    "    cont_summary = {}\n",
    "    for col in key_cont_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        s = pd.to_numeric(df[col], errors='coerce')\n",
    "        cont_summary[col] = {\n",
    "            'mean': float(np.nanmean(s)),\n",
    "            'median': float(np.nanmedian(s)),\n",
    "            'missing_pct': float(s.isna().mean()*100)\n",
    "        }\n",
    "    out['continuous'] = cont_summary\n",
    "\n",
    "    # Target prevalence\n",
    "    tgt = _coerce_binary(df[target_col])\n",
    "    out['mafld_prevalence_pct'] = float(np.nanmean(tgt)*100)\n",
    "\n",
    "    # Sex proportion (optional)\n",
    "    if sex_col and sex_col in df.columns:\n",
    "        sex = _coerce_binary(df[sex_col])\n",
    "        out['sex_prop_male_pct'] = float(np.nanmean(sex)*100)\n",
    "\n",
    "    return out\n",
    "\n",
    "def _smd(x: pd.Series, y: pd.Series) -> float:\n",
    "    \"\"\"Standardized mean difference for continuous variables (Hedges not applied).\"\"\"\n",
    "    x = pd.to_numeric(x, errors='coerce')\n",
    "    y = pd.to_numeric(y, errors='coerce')\n",
    "    mx, my = np.nanmean(x), np.nanmean(y)\n",
    "    sx, sy = np.nanstd(x, ddof=1), np.nanstd(y, ddof=1)\n",
    "    sp = np.sqrt(((sx**2) + (sy**2))/2.0)\n",
    "    if sp == 0 or np.isnan(sp):\n",
    "        return np.nan\n",
    "    return float((mx - my)/sp)\n",
    "\n",
    "# --------------------------\n",
    "# Main cleaning & compare (supports both orders)\n",
    "# --------------------------\n",
    "def clean_with_thresholds(df: pd.DataFrame,\n",
    "                          feature_missing_thresh: float,\n",
    "                          row_missing_thresh: float,\n",
    "                          order: str = \"cols_then_rows\") -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Clean by thresholds with controllable order + remove duplicates.\n",
    "    order: \"cols_then_rows\" (default) or \"rows_then_cols\"\n",
    "    Thresholds are fractions (e.g., 0.45 means 45%).\n",
    "    \"\"\"\n",
    "    if order not in (\"cols_then_rows\", \"rows_then_cols\"):\n",
    "        raise ValueError(\"order must be 'cols_then_rows' or 'rows_then_cols'\")\n",
    "\n",
    "    log = {}\n",
    "\n",
    "    if order == \"cols_then_rows\":\n",
    "        # 1) drop columns by feature threshold\n",
    "        col_miss0 = _col_missing_pct(df)\n",
    "        keep_cols = col_miss0[col_miss0 <= feature_missing_thresh].index.tolist()\n",
    "        removed_cols = col_miss0[col_miss0 > feature_missing_thresh].sort_values(ascending=False)\n",
    "        df2 = df[keep_cols].copy()\n",
    "\n",
    "        # 2) drop rows by row threshold\n",
    "        row_miss = _row_missing_pct(df2)\n",
    "        keep_rows = row_miss[row_miss <= row_missing_thresh].index\n",
    "        removed_rows = row_miss[row_miss > row_missing_thresh].sort_values(ascending=False)\n",
    "        df_clean = df2.loc[keep_rows].copy()\n",
    "\n",
    "    else:  # rows_then_cols\n",
    "        # 1) drop rows by row threshold (on original df)\n",
    "        row_miss0 = _row_missing_pct(df)\n",
    "        keep_rows = row_miss0[row_miss0 <= row_missing_thresh].index\n",
    "        removed_rows = row_miss0[row_miss0 > row_missing_thresh].sort_values(ascending=False)\n",
    "        df2 = df.loc[keep_rows].copy()\n",
    "\n",
    "        # 2) drop columns by feature threshold (on filtered rows)\n",
    "        col_miss = _col_missing_pct(df2)\n",
    "        keep_cols = col_miss[col_miss <= feature_missing_thresh].index.tolist()\n",
    "        removed_cols = col_miss[col_miss > feature_missing_thresh].sort_values(ascending=False)\n",
    "        df_clean = df2[keep_cols].copy()\n",
    "\n",
    "    # 3) حذف رکوردهای duplicate\n",
    "    before_dups = df_clean.shape[0]\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    after_dups = df_clean.shape[0]\n",
    "    removed_dups_count = before_dups - after_dups\n",
    "\n",
    "    log['removed_cols'] = removed_cols.to_dict()\n",
    "    log['removed_rows_count'] = int(removed_rows.shape[0])\n",
    "    log['removed_duplicates_count'] = int(removed_dups_count)\n",
    "    log['kept_rows_count'] = int(df_clean.shape[0])\n",
    "    log['kept_cols_count'] = int(df_clean.shape[1])\n",
    "\n",
    "    return df_clean, log\n",
    "\n",
    "\n",
    "def compare_thresholds(df: pd.DataFrame,\n",
    "                       thresholds: List[Tuple[float, float]] = [(0.30, 0.50),\n",
    "                                                                (0.45, 0.50),\n",
    "                                                                (0.60, 0.50)],\n",
    "                       key_cont_candidates: List[str] = ['age','BMI','ALT','AST','PLT','CRP'],\n",
    "                       target_name: str = 'fattyliver',\n",
    "                       sex_name: str = 'sex',\n",
    "                       order: str = \"cols_then_rows\") -> Dict:\n",
    "    \"\"\"\n",
    "    Run cleaning for multiple thresholds and summarize cohorts.\n",
    "    order: \"cols_then_rows\" or \"rows_then_cols\"\n",
    "    Returns:\n",
    "      - 'scenarios': dict of per-threshold summaries\n",
    "      - 'smd_vs_045': SMDs vs the 0.45/0.50 scenario (same order)\n",
    "    \"\"\"\n",
    "    # Resolve essential columns case-insensitively\n",
    "    target_col = _find_col(df, target_name)\n",
    "    sex_col = None\n",
    "    try:\n",
    "        sex_col = _find_col(df, sex_name)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Resolve key continuous columns that exist\n",
    "    resolved_keys = []\n",
    "    for nm in key_cont_candidates:\n",
    "        try:\n",
    "            resolved_keys.append(_find_col(df, nm))\n",
    "        except KeyError:\n",
    "            pass\n",
    "    key_cont_cols = list(dict.fromkeys(resolved_keys))\n",
    "\n",
    "    order_tag = \"CthenR\" if order == \"cols_then_rows\" else \"RthenC\"\n",
    "    scenarios, cleaned_dfs = {}, {}\n",
    "\n",
    "    for f_thr, r_thr in thresholds:\n",
    "        label = f\"{order_tag}_feat_{int(f_thr*100)}_row_{int(r_thr*100)}\"\n",
    "        df_clean, log = clean_with_thresholds(df, f_thr, r_thr, order=order)\n",
    "        summ = _summarize_cohort(df_clean, key_cont_cols, target_col, sex_col)\n",
    "        summ['thresholds'] = {'feature_missing': f_thr, 'row_missing': r_thr, 'order': order}\n",
    "        summ['removed_cols_top5'] = dict(sorted(log['removed_cols'].items(),\n",
    "                                                key=lambda x: x[1],\n",
    "                                                reverse=True)[:5])\n",
    "        summ['removed_rows_count'] = log['removed_rows_count']\n",
    "        summ['removed_duplicates_count'] = log['removed_duplicates_count']\n",
    "        scenarios[label] = summ\n",
    "        cleaned_dfs[label] = df_clean\n",
    "\n",
    "    # SMDs vs 0.45/0.50 baseline under the same order\n",
    "    base_key = f\"{order_tag}_feat_45_row_50\"\n",
    "    smd_table = {}\n",
    "    if base_key in cleaned_dfs:\n",
    "        base = cleaned_dfs[base_key]\n",
    "        for label, dfi in cleaned_dfs.items():\n",
    "            if label == base_key:\n",
    "                continue\n",
    "            smds = {}\n",
    "            for col in key_cont_cols:\n",
    "                if col in base.columns and col in dfi.columns:\n",
    "                    smds[col] = _smd(base[col], dfi[col])\n",
    "            smd_table[label] = smds\n",
    "\n",
    "    return {'scenarios': scenarios, 'smd_vs_045': smd_table}\n",
    "\n",
    "# --------------------------\n",
    "# Example usage (both orders)\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) دیتافریم رو لود کن (مسیر فایل خودت رو بگذار)\n",
    "    df = pd.read_csv(r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\datasets\\Data\\0-Original Data.csv\")\n",
    "\n",
    "    # 2) اول ستون‌ها بعد ردیف‌ها\n",
    "    res_CthenR = compare_thresholds(df, order=\"cols_then_rows\")\n",
    "    rows1 = []\n",
    "    for label, summ in res_CthenR['scenarios'].items():\n",
    "        rows1.append({\n",
    "            'scenario': label,\n",
    "            'n_rows': summ['n_rows'],\n",
    "            'n_cols': summ['n_cols'],\n",
    "            'overall_missing_%': round(summ['overall_missing_pct'], 2),\n",
    "            'MAFLD_prev_%': round(summ['mafld_prevalence_pct'], 2),\n",
    "            'Age_mean': round(summ['continuous'].get('age',{}).get('mean', np.nan), 2),\n",
    "            'BMI_mean': round(summ['continuous'].get('BMI',{}).get('mean', np.nan), 2),\n",
    "            'ALT_mean': round(summ['continuous'].get('ALT', summ['continuous'].get('alt', {})).get('mean', np.nan), 2),\n",
    "            'AST_mean': round(summ['continuous'].get('AST', summ['continuous'].get('ast', {})).get('mean', np.nan), 2),\n",
    "            'PLT_mean': round(summ['continuous'].get('PLT',{}).get('mean', np.nan), 2),\n",
    "            'CRP_mean': round(summ['continuous'].get('CRP',{}).get('mean', np.nan), 2),\n",
    "            'Removed_dups': summ['removed_duplicates_count'],\n",
    "        })\n",
    "    summary_CthenR = pd.DataFrame(rows1)\n",
    "    print(\"=== Columns-then-Rows ===\")\n",
    "    print(summary_CthenR.to_string(index=False))\n",
    "\n",
    "    print(\"\\nSMDs vs 45%/50% (CthenR):\")\n",
    "    print(pd.DataFrame(res_CthenR['smd_vs_045']).T.round(3).to_string())\n",
    "\n",
    "    # 3) اول ردیف‌ها بعد ستون‌ها\n",
    "    res_RthenC = compare_thresholds(df, order=\"rows_then_cols\")\n",
    "    rows2 = []\n",
    "    for label, summ in res_RthenC['scenarios'].items():\n",
    "        rows2.append({\n",
    "            'scenario': label,\n",
    "            'n_rows': summ['n_rows'],\n",
    "            'n_cols': summ['n_cols'],\n",
    "            'overall_missing_%': round(summ['overall_missing_pct'], 2),\n",
    "            'MAFLD_prev_%': round(summ['mafld_prevalence_pct'], 2),\n",
    "            'Age_mean': round(summ['continuous'].get('age',{}).get('mean', np.nan), 2),\n",
    "            'BMI_mean': round(summ['continuous'].get('BMI',{}).get('mean', np.nan), 2),\n",
    "            'ALT_mean': round(summ['continuous'].get('ALT', summ['continuous'].get('alt', {})).get('mean', np.nan), 2),\n",
    "            'AST_mean': round(summ['continuous'].get('AST', summ['continuous'].get('ast', {})).get('mean', np.nan), 2),\n",
    "            'PLT_mean': round(summ['continuous'].get('PLT',{}).get('mean', np.nan), 2),\n",
    "            'CRP_mean': round(summ['continuous'].get('CRP',{}).get('mean', np.nan), 2),\n",
    "            'Removed_dups': summ['removed_duplicates_count'],\n",
    "        })\n",
    "    summary_RthenC = pd.DataFrame(rows2)\n",
    "    print(\"\\n=== Rows-then-Columns ===\")\n",
    "    print(summary_RthenC.to_string(index=False))\n",
    "\n",
    "    print(\"\\nSMDs vs 45%/50% (RthenC):\")\n",
    "    print(pd.DataFrame(res_RthenC['smd_vs_045']).T.round(3).to_string())\n",
    "\n",
    "    # (اختیاری) ذخیره خروجی‌ها برای پیوست به پاسخ داور\n",
    "    summary_CthenR.to_csv(\"summary_columns_then_rows.csv\", index=False)\n",
    "    summary_RthenC.to_csv(\"summary_rows_then_columns.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8139dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- تنظیمات کاربر ----------\n",
    "CSV_PATH = r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\datasets\\Data\\0-Original Data.csv\"\n",
    "\n",
    "# ستون/پرچمی که نشان می‌دهد بیمار سونوگرافی دارد (مثال: 'has_ultrasound' یا 'ultra' یا ...)\n",
    "# اگر چنین ستونی نداری و محدودسازی به سونوگرافی را قبلاً در فایل اعمال کرده‌ای، این فیلتر را غیرفعال کن.\n",
    "ULTRA_FLAG_COL = None  # مثلا \"has_ultrasound\" یا None اگر ندارید\n",
    "ULTRA_POS_VALUES = {1, True, 'yes', 'Yes', 'Y'}  # مقادیر مثبت\n",
    "\n",
    "# لیست ستون‌های نامرتبط که در مقاله حذف کردی (این را طبق پروژه خودت پر کن)\n",
    "IRRELEVANT_COLS = [\n",
    "    # مثال: 'prescription_date', 'visit_date', 'some_id', ...\n",
    "]\n",
    "\n",
    "# بازه‌های مجاز برای تبدیل out-of-range به NaN (فقط مثال؛ با Numbers واقعی خودت جایگزین کن)\n",
    "RANGES = {\n",
    "    'age': (0, 120),\n",
    "    'BMI': (10, 80),\n",
    "    'ALT': (0, 500),\n",
    "    'AST': (0, 500),\n",
    "    'PLT': (10, 1000),\n",
    "    'CRP': (0, 300),\n",
    "    # ... هر چیزی که داری\n",
    "}\n",
    "\n",
    "# آستانه‌ها\n",
    "FEATURE_MISSING_THR = 0.45  # 45%\n",
    "ROW_MISSING_THR = 0.50      # 50%\n",
    "\n",
    "# ترتیب حذف‌ها: اول ستون‌ها بعد ردیف‌ها (مطابق متن مقاله)\n",
    "ORDER = \"cols_then_rows\"  # یا \"rows_then_cols\" اگر واقعاً این بوده\n",
    "\n",
    "# کلیدهای یکتا برای حذف رکوردهای تکراری (اگر معیاری داری؛ در غیر این صورت بر اساس کل سطر dedup می‌کند)\n",
    "DEDUP_KEYS = None  # مثال: ['patient_id'] یا None\n",
    "\n",
    "\n",
    "# ---------- توابع کمکی ----------\n",
    "def pct_missing_by_col(df):\n",
    "    return df.isna().mean().sort_values(ascending=False)\n",
    "\n",
    "def clamp_ranges_to_nan(df, ranges_dict):\n",
    "    df2 = df.copy()\n",
    "    for col, (lo, hi) in ranges_dict.items():\n",
    "        if col in df2.columns:\n",
    "            s = pd.to_numeric(df2[col], errors='coerce')\n",
    "            s = s.mask((s < lo) | (s > hi), np.nan)\n",
    "            df2[col] = s\n",
    "    return df2\n",
    "\n",
    "def apply_ultrasound_filter(df, flag_col, pos_values):\n",
    "    if flag_col is None or flag_col not in df.columns:\n",
    "        return df, \"no_ultra_filter\"\n",
    "    mask = df[flag_col].isin(pos_values)\n",
    "    return df[mask].copy(), f\"ultra_filtered({mask.sum()} kept)\"\n",
    "\n",
    "def drop_irrelevant(df, cols):\n",
    "    cols_present = [c for c in cols if c in df.columns]\n",
    "    return df.drop(columns=cols_present), cols_present\n",
    "\n",
    "def drop_by_thresholds(df, f_thr, r_thr, order=\"cols_then_rows\"):\n",
    "    df2 = df.copy()\n",
    "    if order == \"cols_then_rows\":\n",
    "        col_miss = df2.isna().mean()\n",
    "        keep_cols = col_miss[col_miss <= f_thr].index\n",
    "        rem_cols = col_miss[col_miss > f_thr].sort_values(ascending=False)\n",
    "        df2 = df2[keep_cols]\n",
    "\n",
    "        row_miss = df2.isna().mean(axis=1)\n",
    "        keep_rows = row_miss[row_miss <= r_thr].index\n",
    "        rem_rows_count = (row_miss > r_thr).sum()\n",
    "        df2 = df2.loc[keep_rows].copy()\n",
    "    else:  # rows_then_cols\n",
    "        row_miss = df2.isna().mean(axis=1)\n",
    "        keep_rows = row_miss[row_miss <= r_thr].index\n",
    "        rem_rows_count = (row_miss > r_thr).sum()\n",
    "        df2 = df2.loc[keep_rows].copy()\n",
    "\n",
    "        col_miss = df2.isna().mean()\n",
    "        keep_cols = col_miss[col_miss <= f_thr].index\n",
    "        rem_cols = col_miss[col_miss > f_thr].sort_values(ascending=False)\n",
    "        df2 = df2[keep_cols]\n",
    "\n",
    "    return df2\n",
    "\n",
    "def deduplicate(df, keys=None):\n",
    "    if keys is None:\n",
    "        before = len(df)\n",
    "        out = df.drop_duplicates()\n",
    "        return out, before - len(out), \"full-row\"\n",
    "    else:\n",
    "        before = len(df)\n",
    "        out = df.sort_index().drop_duplicates(subset=keys, keep='first')\n",
    "        return out, before - len(out), f\"subset({','.join(keys)})\"\n",
    "\n",
    "\n",
    "# ---------- Pipeline با لاگ کامل ----------\n",
    "df0 = pd.read_csv(CSV_PATH)\n",
    "print(f\"[0] loaded: n={len(df0)}, p={df0.shape[1]}\")\n",
    "\n",
    "# (۱) فیلتر سونوگرافی\n",
    "df1, ultra_info = apply_ultrasound_filter(df0, ULTRA_FLAG_COL, ULTRA_POS_VALUES)\n",
    "print(f\"[1] ultrasound filter: {ultra_info} -> n={len(df1)}, p={df1.shape[1]}\")\n",
    "\n",
    "# (۲) حذف ستون‌های نامرتبط\n",
    "df2, removed_list = drop_irrelevant(df1, IRRELEVANT_COLS)\n",
    "print(f\"[2] drop irrelevant cols: removed={len(removed_list)} -> n={len(df2)}, p={df2.shape[1]}\")\n",
    "\n",
    "# (۳) out-of-range -> NaN\n",
    "df3 = clamp_ranges_to_nan(df2, RANGES)\n",
    "print(f\"[3] clamp to ranges -> n={len(df3)}, p={df3.shape[1]} (no row change expected)\")\n",
    "\n",
    "# (۴) آستانه‌ها (ستون‌ها سپس ردیف‌ها، یا برعکس)\n",
    "df4 = drop_by_thresholds(df3, FEATURE_MISSING_THR, ROW_MISSING_THR, order=ORDER)\n",
    "print(f\"[4] thresholds ({ORDER}) -> n={len(df4)}, p={df4.shape[1]}\")\n",
    "\n",
    "# (۵) حذف رکوردهای تکراری\n",
    "df5, dup_count, dedup_mode = deduplicate(df4, keys=DEDUP_KEYS)\n",
    "print(f\"[5] deduplicate {dedup_mode}: removed {dup_count} -> n={len(df5)}, p={df5.shape[1]}\")\n",
    "\n",
    "# گزارش میسینگ ستون‌ها پس از مرحله 5 (باید همان حوالی 31 فیچر + تارگت باشد)\n",
    "col_miss_final = pct_missing_by_col(df5)\n",
    "print(\"\\nTop missing columns after cleaning:\")\n",
    "print(col_miss_final.head(10).apply(lambda x: round(x*100,2)))\n",
    "\n",
    "print(f\"\\n>>> FINAL n (should ~ 3769): {len(df5)}\")\n",
    "\n",
    "# (اختیاری) سیو برای مراحل بعدی\n",
    "# df5.to_csv(\"cleaned_for_modeling.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e6bab",
   "metadata": {},
   "source": [
    "## 2) Data Cleaning\n",
    "- Remove irrelevant columns (per clinical expert judgement).\n",
    "- Enforce valid ranges; cast dtypes.\n",
    "- Drop duplicates.\n",
    "- Apply missingness thresholds (45% per-column, 50% per-row) as justified in the revision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# فرض: این سه تا دیتافریم قبلا ساخته شدن\n",
    "# Kbest_df, pca_results_df, rfecv_results_df\n",
    "\n",
    "# انتخاب فقط ستون‌های مهم برای مقایسه\n",
    "kbest_sel = Kbest_df[[\"Classifier\",\"Best_K\",\"Accuracy\"]].rename(\n",
    "    columns={\"Best_K\":\"KBest_n\",\"Accuracy\":\"KBest_Accuracy\"}\n",
    ")\n",
    "pca_sel = pca_results_df[[\"Classifier\",\"Best_n_components\",\"Accuracy\"]].rename(\n",
    "    columns={\"Best_n_components\":\"PCA_n\",\"Accuracy\":\"PCA_Accuracy\"}\n",
    ")\n",
    "rfecv_sel = results_df[[\"Classifier\",\"Selected_Count\",\"Accuracy\"]].rename(\n",
    "    columns={\"Selected_Count\":\"RFECV_n\",\"Accuracy\":\"RFECV_Accuracy\"}\n",
    ")\n",
    "\n",
    "# فرضاً plain_results_rows هم داری\n",
    "plain_df = pd.DataFrame(plain_results_rows)[[\"Classifier\",\"Accuracy\"]].rename(\n",
    "    columns={\"Accuracy\":\"Accuracy_Without\"}\n",
    ")\n",
    "\n",
    "# حالا merge چهار جدول\n",
    "final_table = plain_df.merge(kbest_sel, on=\"Classifier\")\\\n",
    "                      .merge(pca_sel, on=\"Classifier\")\\\n",
    "                      .merge(rfecv_sel, on=\"Classifier\")\n",
    "\n",
    "# ذخیره و نمایش\n",
    "final_table.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\FeatureSelection_Comparison.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(final_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85977d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_imputation(df_imputed, df_true, missings):\n",
    "    evaluations = {}\n",
    "    for col in missings:\n",
    "        missing_indices = dff[col].isna()\n",
    "        miss_idx = dff.index[missing_indices]\n",
    "        common = df_true.index.intersection(miss_idx)\n",
    "        y_true = df_true.loc[common, col].values\n",
    "        y_pred = df_imputed.loc[common, col].values\n",
    "\n",
    "\n",
    "        if col in continues:\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            MABR = mean_absolute_error (y_true, y_pred)\n",
    "            evaluations[col] = {'MSE': mse, 'R2': r2, 'MABR':MABR}\n",
    "        else:\n",
    "            y_pred = np.round(y_pred).astype(int)\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "            evaluations[col] = {'Accuracy': acc}\n",
    "    return evaluations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b36b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data_path = r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\datasets\\Data\\3-my_null_data_40_del.csv\"\n",
    "save_path_first_classification = r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\sixth-121\\LogRes.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c38017",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(og_data_path).iloc[:, :-1]\n",
    "missing_dataframe = pd.DataFrame(columns=[\"Variable\", \"Missing_number\", \"Percentage\"])\n",
    "total_records = data.shape[0]\n",
    "\n",
    "for i, col in enumerate(data.columns):\n",
    "    missing = data[col].isnull().sum()\n",
    "    missing_dataframe.loc[i] = [col, missing, round((missing / total_records) * 100, 2)]\n",
    "\n",
    "missing_dataframe = missing_dataframe.sort_values(by=\"Percentage\", ascending=False).reset_index(drop=True)\n",
    "missing_columns = missing_dataframe[\"Variable\"][missing_dataframe[\"Missing_number\"] > 100]\n",
    "\n",
    "print(missing_dataframe.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4594d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spilliting two halfs\n",
    "df_train, df_test = train_test_split(data, train_size=0.5)\n",
    "\n",
    "#Second Half to train and val for evaluating prediction of classifiers\n",
    "df_train_train, df_train_val = train_test_split(df_train, train_size=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ada63",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = [\"Classifier\", \"Parameters\", \"Column\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "Log_res = pd.DataFrame(columns=new_col)\n",
    "logreg_param_grid = {'C': [0.1, 1, 10], 'max_iter': [100, 200, 300], 'solver': ['lbfgs', 'liblinear']}\n",
    "knn_param_grid = {'n_neighbors': [3, 5, 7]}\n",
    "dtree_param_grid = {'max_depth': [3, 5, 8], 'min_samples_split': [2, 5, 10]}\n",
    "svm_param_grid = {'C': [0.1, 1, 10]}\n",
    "rf_param_grid = {'n_estimators': [100, 200, 300], 'max_depth': [3, 5, 8]}\n",
    "et_param_grid = {'n_estimators': [100, 200, 300], 'max_depth': [3, 5, 8]}\n",
    "xgb_param_grid = {'max_depth': [3, 5, 8], 'learning_rate': [0.1, 0.05, 0.01]}\n",
    "ada_param_grid = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1]}\n",
    "results = []\n",
    "\n",
    "Grids = [\n",
    "    logreg_param_grid,\n",
    "    knn_param_grid, dtree_param_grid\n",
    "    , svm_param_grid, rf_param_grid, et_param_grid, xgb_param_grid, ada_param_grid\n",
    "      ]\n",
    "classifiers_name = [\n",
    "    \"LogReg\",\n",
    "    \"KNN\", \"DT\"\n",
    "    , \"SVM\", \"RF\", \"ET\", \"XGB\", \"AdaBoost\"\n",
    "    ]\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier()\n",
    "    ,\n",
    "    SVC(),\n",
    "    RandomForestClassifier(),\n",
    "    ExtraTreesClassifier(),\n",
    "    xgb.XGBClassifier(objective='binary:logistic'),\n",
    "    AdaBoostClassifier()\n",
    "]\n",
    "\n",
    "\n",
    "row = 0\n",
    "for i in range(len(classifiers)):\n",
    "    print(f\"-----{classifiers_name[i]}-----\")\n",
    "    clf = classifiers[i]\n",
    "    param_grid = Grids[i]\n",
    "    for colmn in missing_columns:\n",
    "        print(f\"-{colmn}--\")\n",
    "        selected = [c for c in df_train.columns if c != colmn]\n",
    "\n",
    "        y_train = np.where(df_train_train[colmn].isna(), 1, 0)\n",
    "        y_test = np.where(df_train_val[colmn].isna(), 1, 0)\n",
    "\n",
    "        x_train = df_train_train[selected].fillna(df_train_train[selected].median())\n",
    "        x_test = df_train_val[selected].fillna(df_train_val[selected].median())\n",
    "\n",
    "        grid_search = GridSearchCV(clf, param_grid=param_grid, cv=3, scoring=\"f1\")\n",
    "        grid_search.fit(x_train, y_train)\n",
    "        y_pred = grid_search.predict(x_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "        Log_res.loc[row, \"Classifier\"] = classifiers_name[i]\n",
    "        Log_res.loc[row, \"Parameters\"] = str(grid_search.best_params_)\n",
    "        Log_res.loc[row, \"Column\"] = colmn\n",
    "        Log_res.loc[row, \"Accuracy\"] = accuracy\n",
    "        Log_res.loc[row, \"Precision\"] = precision\n",
    "        Log_res.loc[row, \"Recall\"] = recall\n",
    "        Log_res.loc[row, \"F1\"] = f1\n",
    "        row += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gridi = Log_res\n",
    "columns = Gridi[\"Column\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = df_test.copy()\n",
    "df_test = df_test_original.copy()\n",
    "\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74028e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84edd509",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total_NaN_in_dff:\", dff.isna().sum().sum())\n",
    "print(\"total_NaN_in_df:\", df.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results\n",
    "results = {}\n",
    "# Function to evaluate the imputation results\n",
    "def evaluate_imputation(df_imputed, df_true, missings):\n",
    "    evaluations = {}\n",
    "    for col in missings:\n",
    "        missing_indices = dff[col].isna()\n",
    "        miss_idx = dff.index[missing_indices]\n",
    "        common = df_true.index.intersection(miss_idx)\n",
    "        y_true = df_true.loc[common, col].values\n",
    "        y_pred = df_imputed.loc[common, col].values\n",
    "\n",
    "\n",
    "        if col in continues:\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            MABR = mean_absolute_error (y_true, y_pred)\n",
    "            evaluations[col] = {'MSE': mse, 'R2': r2, 'MABR':MABR}\n",
    "        else:\n",
    "            y_pred = np.round(y_pred).astype(int)\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "            evaluations[col] = {'Accuracy': acc}\n",
    "    return evaluations\n",
    "\n",
    "combos={}\n",
    "\n",
    "for j, (name, estimator) in enumerate(estimators.items()):\n",
    "    combinations = product(*params[j].values())\n",
    "\n",
    "    for i ,comb in enumerate(combinations):\n",
    "        print(f\"Imputing with {name} _ {i}...\")\n",
    "        param_combo = dict(zip(params[j].keys(), comb))\n",
    "\n",
    "        if name == \"KNN\":\n",
    "            estimator.set_params(**param_combo)\n",
    "            combos[f\"{name}_{i}\"] = param_combo\n",
    "            df_imputed = estimator.fit_transform(dfff.copy())\n",
    "        else:\n",
    "\n",
    "            estimator.estimator.set_params(**param_combo)\n",
    "            combos[f\"{name}_{i}\"] = param_combo\n",
    "\n",
    "            if callable(estimator):\n",
    "                df_imputed = estimator(dfff.copy())\n",
    "\n",
    "            else:\n",
    "                df_imputed = estimator.fit_transform(dfff.copy())\n",
    "\n",
    "\n",
    "\n",
    "        df_imputed = pd.DataFrame(df_imputed, columns=dfff.columns)\n",
    "        df_imputed.index = dff.index\n",
    "\n",
    "        results[f\"{name}_{i}\"] = evaluate_imputation(df_imputed, df_test, missings)\n",
    "\n",
    "\n",
    "print(results)\n",
    "\n",
    "scale_weight = {}\n",
    "sum_miss = np.sum(dff.isna().sum(), axis=0)\n",
    "\n",
    "if sum_miss == 0:\n",
    "    print(\"Warning: no missing cells in df (sum_miss == 0) → وزن‌دهی رو رد می‌کنم.\")\n",
    "\n",
    "for cls in df_test.columns:\n",
    "    we = dfff[cls].isna().sum()\n",
    "    scale_weight[cls] = we / sum_miss\n",
    "\n",
    "\n",
    "labels =[]\n",
    "con_values = []\n",
    "binary_values = []\n",
    "\n",
    "continues = ['PLT','hip', 'CRP', 'VitD', 'insulin', 'UA', 'ast', 'alt', 'alkp', 'homa']\n",
    "binary = ['Retino','CAD', 'CVA', 'Smoking']\n",
    "\n",
    "continues_eval = [c for c in ['PLT','hip','CRP','VitD','insulin','UA','ast','alt','alkp','homa'] if c in missings]\n",
    "binary_eval    = [c for c in ['Retino','CAD','CVA','Smoking']                                    if c in missings]\n",
    "\n",
    "\n",
    "for est in results.keys():\n",
    "    labels.append(est)\n",
    "    continues_score = 0\n",
    "    binary_score = 0\n",
    "\n",
    "    scale_weight_con = 0\n",
    "    scale_weight_con_list = []\n",
    "    for con in continues:\n",
    "        continues_score = continues_score + results[est][con][\"R2\"] * scale_weight[con]\n",
    "        scale_weight_con = scale_weight_con + scale_weight[con]\n",
    "        scale_weight_con_list.append(scale_weight[con])\n",
    "    con_values.append(continues_score /scale_weight_con )\n",
    "\n",
    "    scale_weight_bin = 0\n",
    "    scale_weight_bin_list = []\n",
    "    for bin in binary:\n",
    "        binary_score = binary_score+ results[est][bin][\"Accuracy\"] * scale_weight[bin]\n",
    "        scale_weight_bin = scale_weight_bin + scale_weight[bin]\n",
    "        scale_weight_bin_list.append(scale_weight[bin])\n",
    "    binary_values.append(binary_score /scale_weight_bin )\n",
    "\n",
    "\n",
    "#\n",
    "# plt.bar(labels,con_values)\n",
    "# plt.title(\"continues\")\n",
    "# plt.show()\n",
    "#\n",
    "# plt.bar(labels,binary_values)\n",
    "# plt.title(\"binary\")\n",
    "# plt.show()\n",
    "\n",
    "Res = pd.DataFrame(columns=[\"Labels\" ,\"Labels2\", \"Parameters\" , \"Continues\" , \"Binary\",\"Details\"])\n",
    "Res[\"Labels\"] = labels\n",
    "Res[\"Labels2\"] = combos.keys()\n",
    "Res[\"Parameters\"] = combos.values()\n",
    "Res[\"Continues\"] = con_values\n",
    "Res[\"Binary\"] = binary_values\n",
    "Res[\"Details\"] = results.values()\n",
    "\n",
    "\n",
    "Wi_con = pd.DataFrame(columns = [\"Con\",\"Con_weight\",\"Bin\",\"Bin_weight\"])\n",
    "Wi_bin = pd.DataFrame(columns = [\"Bin\",\"Bin_weight\"])\n",
    "Wi_con[\"Con\"] = continues\n",
    "Wi_con[\"Con_weight\"] = scale_weight_con_list\n",
    "\n",
    "Wi_bin[\"Bin\"] = binary\n",
    "Wi_bin[\"Bin_weight\"] = scale_weight_bin_list\n",
    "\n",
    "\n",
    "\n",
    "Res.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\sixth-121\\ResultsOfGrid.csv\")\n",
    "Wi_con.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\sixth-121\\WeightsCon.csv\")\n",
    "Wi_bin.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\sixth-121\\WeightsBin.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb0d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Split Halves & Complete Subset =====\n",
    "idx = np.arange(len(df))\n",
    "first_half  = df.iloc[idx % 2 == 0].reset_index(drop=True)\n",
    "second_half = df.iloc[idx % 2 == 1].reset_index(drop=True)\n",
    "print(\"First half:\", first_half.shape, \"Second half:\", second_half.shape)\n",
    "\n",
    "# subset کاملاً کامل برای شبیه‌سازی\n",
    "complete_subset = first_half.dropna(axis=0).reset_index(drop=True)\n",
    "print(\"Complete subset:\", complete_subset.shape)\n",
    "\n",
    "# ستون‌هایی که مفقودی دارند (روی کل داده تمیزشده)\n",
    "cols_with_missing = [c for c in df.columns if df[c].isna().any()]\n",
    "target_missing_rates = df[cols_with_missing].isna().mean().to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2660570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Learn Missingness Models =====\n",
    "CLASSIFIER_GRID = {\n",
    "    # \"logreg\": (LogisticRegression(max_iter=500), {\"C\":[0.1,1,10], \"solver\":[\"liblinear\",\"lbfgs\"]}),\n",
    "    \"knn\":    (KNeighborsClassifier(), {\"n_neighbors\":[3,5,7], \"weights\":[\"uniform\",\"distance\"]}),\n",
    "    # \"svc\":    (SVC(), {\"C\":[0.5,1,5], \"kernel\":[\"rbf\",\"linear\"], \"gamma\":[\"scale\"]}),\n",
    "    \"dt\":     (DecisionTreeClassifier(), {\"max_depth\":[None,5,10]})\n",
    "    # ,\n",
    "    # \"rf\":     (RandomForestClassifier(), {\"n_estimators\":[200,400], \"max_depth\":[None,10]}),\n",
    "    # \"et\":     (ExtraTreesClassifier(), {\"n_estimators\":[200,400], \"max_depth\":[None,10]}),\n",
    "    # \"ada\":    (AdaBoostClassifier(), {\"n_estimators\":[200,400], \"learning_rate\":[0.5,1.0]}),\n",
    "\n",
    "\n",
    "    #     (\"KNN\", KNeighborsClassifier(), {'n_neighbors': [3, 5, 7]}),\n",
    "    # (\"DT\", DecisionTreeClassifier(), {'max_depth': [3, 5, 8], 'min_samples_split': [2, 5, 10]})\n",
    "    # ,\n",
    "    # (\"SVM\", SVC(), {'C': [0.1, 1, 10]}),\n",
    "    # (\"RF\", RandomForestClassifier(), {'n_estimators': [100, 200, 300], 'max_depth': [3, 5, 8]}),\n",
    "    # (\"GB\", GradientBoostingClassifier(), {'n_estimators': [100, 200, 300], 'learning_rate': [0.1, 0.05, 0.01]}),\n",
    "    # (\"XGB\", xgb.XGBClassifier(objective='binary:logistic', eval_metric=\"logloss\"),\n",
    "    #     {'max_depth': [3, 5, 8], 'learning_rate': [0.1, 0.05, 0.01]})\n",
    "}\n",
    "\n",
    "def learn_missingness_models(df_with_missing, cols):\n",
    "    models = {}\n",
    "    scorer = make_scorer(f1_score)\n",
    "    for col in cols:\n",
    "        y = df_with_missing[col].isna().astype(int)\n",
    "        X = df_with_missing.drop(columns=[col]).copy()\n",
    "        X = X.fillna(-777)  # جایگزینی امن برای NaN\n",
    "        best = (-1, None, None)\n",
    "        for name, (est, grid) in CLASSIFIER_GRID.items():\n",
    "            cv = GridSearchCV(est, grid, cv=3, scoring=scorer, n_jobs=-1)\n",
    "            cv.fit(X, y)\n",
    "            if cv.best_score_ > best[0]:\n",
    "                best = (cv.best_score_, name, cv.best_estimator_)\n",
    "        models[col] = {\"best_name\": best[1], \"estimator\": best[2], \"best_f1\": float(best[0])}\n",
    "    return models\n",
    "\n",
    "miss_models = learn_missingness_models(second_half, cols_with_missing)\n",
    "list(miss_models.items())[:3]  # پیش‌نمایش\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7bbbe",
   "metadata": {},
   "source": [
    "## 3) Missingness Analysis & MAR Simulation\n",
    "- Little’s test to reject MCAR; adopt MAR assumption.\n",
    "- Train per-column classifiers to model missingness probability using observed features.\n",
    "- Simulate missingness on a **complete subset** to benchmark imputers against ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kbest_df = pd.DataFrame(results_rows)\n",
    "Kbest_df.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\With AdaBoost\\Kbest_Summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c323fe",
   "metadata": {},
   "source": [
    "## 4) Imputation Benchmarking\n",
    "- Evaluate MICE (with multiple estimators), KNN, MissForest, ExtraTrees/AdaBoost-based iterative imputers.\n",
    "- Metrics: R² for continuous, Accuracy for binary; weighted by each variable’s missing fraction.\n",
    "- Select the most robust imputer across seeds (per revision: AdaBoost).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c82c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running without feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a918dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5606ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_imputer_index = np.argmax(best_per_model[\"Continues\"])\n",
    "print(best_per_model.iloc[best_imputer_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881bdfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data_path = r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\datasets\\Data\\3-my_null_data_40_del.csv\"\n",
    "og_data = pd.read_csv(og_data_path)\n",
    "\n",
    "\n",
    "imputed_data = best_imputer.fit_transform(og_data)\n",
    "imputed_data = pd.DataFrame(imputed_data , columns=og_data.columns)\n",
    "binary = ['Retino', 'htn', 'sex', 'CAD', 'CVA', 'Smoking']\n",
    "for col in binary:\n",
    "    imputed_data[col] = np.round(imputed_data[col])\n",
    "imputed_data.to_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\4-imputed_data.csv\" , index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392cd1ae",
   "metadata": {},
   "source": [
    "## 5) Outlier Detection\n",
    "- Apply density-based LOF after imputation to avoid bias from missingness patterns.\n",
    "- Remove only clearly erroneous records.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1270baf7",
   "metadata": {},
   "source": [
    "## 6) Dataset Balancing\n",
    "- Mild class imbalance. Show results for random undersampling and SMOTE; report negligible differences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b05983",
   "metadata": {},
   "source": [
    "## 7) Feature Selection\n",
    "- KBest, RFECV, PCA, and **Genetic Algorithm** (Taguchi-tuned hyperparameters).\n",
    "- Perform **within-fold** selection to avoid leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac3650",
   "metadata": {},
   "source": [
    "## 8) Modeling & Cross-Validation\n",
    "- Models: LR, KNN, SVM, DT, ET, GB, XGBoost, LightGBM.\n",
    "- Stratified 5-fold CV; grid/random search **within each training fold**.\n",
    "- Primary metric: AUC; tiebreaker: F1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a620c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\sixth-121\\test2.csv\")\n",
    "df_test_original = pd.read_csv(r\"C:\\Users\\z_kho\\OneDrive\\Desktop\\sixth-121\\test_original2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e0afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = df_test.copy()\n",
    "df_test = df_test_original.copy()\n",
    "\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "dff = df.copy()\n",
    "dfff = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e228bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "missings = [c for c in df.columns if (c in df_test.columns) and (df[c].isna().sum() > 0)]\n",
    "\n",
    "continues = ['PLT','hip', 'CRP', 'VitD', 'insulin', 'UA', 'ast', 'alt', 'alkp', 'homa']\n",
    "binary = ['Retino','CAD', 'CVA', 'Smoking']\n",
    "\n",
    "continues_eval = [c for c in ['PLT','hip','CRP','VitD','insulin','UA','ast','alt','alkp','homa'] if c in missings]\n",
    "binary_eval    = [c for c in ['Retino','CAD','CVA','Smoking']                                    if c in missings]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd3a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Load Data =====\n",
    "df = pd.read_csv(r\"C:\\zaza\\documents\\University\\my subjects\\arshad\\And beyond\\Missing Data\\datasets\\Data\\3-my_null_data_40_del.csv\")\n",
    "print(\"Raw shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09078ebe",
   "metadata": {},
   "source": [
    "## 9) Evaluation & Metrics (with variance)\n",
    "- Report mean ± SD and 95% CI across repeated CV.\n",
    "- Include confusion matrices, ROC/PR curves if applicable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1346d497",
   "metadata": {},
   "source": [
    "## 10) Feature Importance & Explainability\n",
    "- Compare importance across XGBoost (no FS), GB (with KBest), LightGBM (with reduced features).\n",
    "- Add SHAP summary for the final model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795fae31",
   "metadata": {},
   "source": [
    "## 11) Save Artifacts\n",
    "- Save fitted models, feature lists, and result tables to `outputs/` for supplement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85ed052",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Notes\n",
    "- All code cells have been cleaned of transient debugging prints and cleared of outputs.\n",
    "- Please place any private data files in a `data/` folder before running.\n",
    "- If certain figures/tables are required for the supplement, run corresponding cells in Sections 9–11.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
